---
title: "Korrelation"
lang: "de-CH"
---

```{r}
#| label: correlation-grid
#| echo: true
#| code-fold: true
#| fig-cap: "Korrelation zwischen zwei Variablen"
# Funktion zur Generierung von korrelierten Daten
generate_data <- function(n, rho) {
  library(MASS)
  mu <- c(0, 0)
  sigma <- matrix(c(1, rho, rho, 1), ncol = 2)
  mvrnorm(n, mu, sigma)
}

# Layout für 2x2 Plots
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

# Verschiedene Korrelationswerte
cor_values <- c(0.9, 0.5, 0, -0.999)

# Plots erstellen
for (rho in cor_values) {
  data <- generate_data(500, rho)
  plot(data, 
       main = paste("r =", rho),
       xlab = "x", 
       ylab = "y", 
       pch = 19, 
       col = rgb(0, 0, 1, 0.5))
}
```

-   Die Korrelation beschreibt den statistischen Zusammenhang zwischen zwei Variablen. Sie misst, ob und wie stark zwei Variablen gemeinsam variieren.
    -   **Positive Korrelation:** Beide Variablen nehmen gleichzeitig zu oder ab (z.B. Körpergrösse und Schuhgrösse)
    -   **Negative Korrelation:** Eine Variable nimmt zu, während die andere abnimmt (z.B. Anzahl Lernstunden und Fehleranzahl in einem Test).

1.  **Unterschied zwischen Korrelation und Kausalität:**
    -   **Korrelation ≠ Kausalität:** Nur weil zwei Variablen korrelieren, bedeutet das nicht, dass die eine Variable die andere verursacht.
    -   **Die Korrelation sagt nichts über die Kausalität, oder die Richtung der Wirkung.**
    -   **Beispiele für Scheinzusammenhänge:**
        -   Der Konsum von Speiseeis korreliert mit der Anzahl von Sonnenbrandfällen. Ursache ist die höhere Sonneneinstrahlung im Sommer, nicht das Eis selbst.

## Von der Kovarianz zur Korrelation

1. **Das Produkt der Abweichungen:**
    -   Zentrale Frage: **"Variieren zwei Variablen gemeinsam?"**
    -   Formel:
        $$
        (X_i - \bar{X}) \cdot (Y_i - \bar{Y})
        $$
        -   $X_i$ und $Y_i$ sind die beobachteten Werte der Variablen $X$ und $Y$ für die $i$-te Beobachtung.
        -   $\bar{X}$ und $\bar{Y}$ sind die Mittelwerte der Variablen $X$ und $Y$.
        -   **Positives Produkt:** Beide Abweichungen haben das gleiche Vorzeichen (gleichsinnige Variation).
        -   **Negatives Produkt:** Abweichungen haben unterschiedliche Vorzeichen (gegensätzliche Variation).

2. **Kovarianz: Bedeutung und Berechnung:**
    -   Die **Kovarianz** misst die durchschnittliche gemeinsame Abweichung zweier Variablen von ihren Mittelwerten.
    -   Formel der Stichprobenkovarianz:
        $$
        \text{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})
        $$
        -   $n$ ist die Anzahl der Beobachtungen.
        -   $X_i$ und $Y_i$ sind die beobachteten Werte der Variablen $X$ und $Y$ für die $i$-te Beobachtung.
        -   $\bar{X}$ und $\bar{Y}$ sind die Mittelwerte der Variablen $X$ und $Y$.
  
    -   **Interpretation:**
        -   **Positive Kovarianz:** Tendenz zu gleichsinniger Variation.
        -   **Negative Kovarianz:** Tendenz zu gegensätzlicher Variation.
        -   **Nahe 0:** Kein linearer Zusammenhang.

3. **Normierung zur Berechnung des Korrelationskoeffizienten:**
    -   Problem der Kovarianz: **Abhängig von den Einheiten der Variablen.**
    -   Lösung: **Normierung** durch die Standardabweichungen von $X$ und $Y$:
        $$
        r = \frac{\text{Cov}(X, Y)}{\sigma_X \cdot \sigma_Y}
        $$
        -   $\sigma_X$ und $\sigma_Y$ sind die Standardabweichungen der Variablen $X$ und $Y$.
        -   **Ergebnis:** Der **Korrelationskoeffizient** ($r$), der immer zwischen **-1 und +1** liegt.

## Der Pearson-Korrelationskoeffizient

$$
\rho_{X,Y} = \frac{\sum_{i=1}^N (X_i - \mu_x)(Y_i - \mu_y)}{\sqrt{\sum_{i=1}^N (X_i - \mu_x)^2 \sum_{i=1}^N (Y_i - \mu_y)^2}}
$$

-   $\rho$ ist der Standardbuchstabe für den Korrelationskoeffizienten der Grundgesamtheit.
-   $r$ ist der Standardbuchstabe für den Korrelationskoeffizienten der Stichprobe.
-   $\mu_x$ und $\mu_y$ sind die Mittelwerte der Variablen $X$ und $Y$.
-   $N$ ist die Anzahl der Beobachtungen (wird hier gross geschrieben da es sich um die Grundgesamtheit handelt)
-   $X_i$ und $Y_i$ sind die beobachteten Werte der Variablen $X$ und $Y$ für die $i$-te Beobachtung.

::: {.callout-note title="Beispiel"}

Wir ergänzen unsere Daten aus @tbl-noten-beispiel um eine zweite Variable, die **Lernzeit pro Woche (h)**, um zu untersuchen, ob ein Zusammenhang zwischen Lernzeit und Prüfungsnoten besteht.

| Student | Prüfungsnote ($X$) | Lernzeit ($Y$) |
|---------|--------------------|----------------|
| A       | 70                 | 10             |
| B       | 80                 | 12             |
| C       | 50                 | 5              |
| D       | 90                 | 15             |
| E       | 60                 | 8              |

: {#tbl-noten-lernzeit-beispiel}

**1. Berechnung der Mittelwerte**

```{r}
#| echo: true
#| label: noten-lernzeit-mittelwerte-x
#| code-fold: false

library(tibble)

noten_lernzeit_tabelle <- tibble(
    student = c("A", "B", "C", "D", "E"),
    note = c(70, 80, 50, 90, 60),
    lernzeit = c(10, 12, 5, 15, 8)
)


mean(noten_lernzeit_tabelle$note)
```

$$
\begin{aligned}
\bar{X} &= \frac{70 + 80 + 50 + 90 + 60}{5} \\ 
&= 70 \\
\end{aligned}
$$

```{r}
#| echo: true
#| label: noten-lernzeit-mittelwerte-y
#| code-fold: false

mean(noten_lernzeit_tabelle$lernzeit)
```

$$
\begin{aligned}
\bar{Y} &= \frac{10 + 12 + 5 + 15 + 8}{5} \\
&= 10
\end{aligned}
$$

**2. Kovarianz berechnen**

```{r}
#| echo: true
#| label: noten-lernzeit-kovarianz-x-y
#| code-fold: false

cov(noten_lernzeit_tabelle$note, noten_lernzeit_tabelle$lernzeit)
```


$$
\begin{aligned}
\text{Cov}(X, Y) &= \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y}) \\
&= \frac{1}{4} 
\left[
\begin{array}{l}
(70-70) \cdot (10-10) + (80-70) \cdot (12-10) +  (50-70) \cdot (5-10) +  \\
(90-70) \cdot (15-10) +  (60-70) \cdot (8-10)
\end{array}
\right] \\
&= \frac{1}{4} \left[0 \cdot 0 + 10 \cdot 2 + (-20) \cdot (-5) + 20 \cdot 5 + (-10) \cdot (-2)\right] \\
&= \frac{1}{4} \left[0 + 20 + 100 + 100 + 20\right] \\
&= \frac{1}{4} 240 \\
&= 60
\end{aligned}
$$


**3. Standardabweichungen berechnen**

```{r}
#| echo: true
#| label: noten-lernzeit-standardabweichung-x
#| code-fold: false

sd(noten_lernzeit_tabelle$note)
```


$$
\begin{aligned}
\sigma_X &= \sqrt{\frac{1}{n-1} \sum (X_i - \bar{X})^2} \\
&= \sqrt{\frac{1}{4} \left[(70-70)^2 + (80-70)^2 + (50-70)^2 + (90-70)^2 + (60-70)^2\right]} \\
&= \sqrt{\frac{1}{4} \left[0 + 100 + 400 + 400 + 100\right]} \\
&= \sqrt{\frac{1}{4} 1000} \\
&= \sqrt{250} \\
&\approx 15.81
\end{aligned}
$$

```{r}
#| echo: true
#| label: noten-lernzeit-standardabweichung-y
#| code-fold: false

sd(noten_lernzeit_tabelle$lernzeit)
```

$$
\begin{aligned}
\sigma_Y &= \sqrt{\frac{1}{n-1} \sum (Y_i - \bar{Y})^2} \\
&= \sqrt{\frac{1}{4} \left[(10-10)^2 + (12-10)^2 + (5-10)^2 + (15-10)^2 + (8-10)^2\right]} \\
&= \sqrt{\frac{1}{4} \left[0 + 4 + 25 + 25 + 4\right]} \\
&= \sqrt{\frac{1}{4} 58} \\
&= \sqrt{14.5} \\
&\approx 3.81
\end{aligned}
$$

**4. Pearson-Korrelationskoeffizient berechnen**

```{r}
#| echo: true
#| label: noten-lernzeit-pearson
#| code-fold: false

cor(noten_lernzeit_tabelle$note, noten_lernzeit_tabelle$lernzeit, method = "pearson")
```

$$
\begin{aligned}
r &= \frac{\text{Cov}(X, Y)}{\sigma_X \cdot \sigma_Y} \\
&= \frac{60}{15.81 \cdot 3.81} \\
&= \frac{60}{60.21} \\
&\approx 0.997
\end{aligned}
$$


**Ergebnis:** $\rho_{X,Y} \approx 0.997$, was auf eine **sehr starke positive Korrelation** hindeutet.

**5. Interpretation**

- Da $\rho_{X,Y} \approx 0.997$ nahe an 1 liegt, bedeutet das, dass Studierende, die mehr Lernzeit investiert haben, **tendenziell bessere Noten erzielt haben**. (Aber: wir können nicht sagen, dass die Lernzeit die Note beeinflusst hat, sondern nur, dass beide Variablen tendenziell gemeinsam variieren.)
- Das zeigt eine **fast perfekte positive Korrelation** zwischen den beiden Variablen.

:::

::: {.callout-note collapse="true"}
### Herleitung

1.  Idee: Produkt der Anomalien

$$
\begin{aligned}
X_i ^d &= X_i - \mu_x \\
Y_i ^d &= Y_i - \mu_y \\
\sum_{i=1}^N (X_i - \mu_x)(Y_i - \mu_y) &= \sum_{i=1}^N X_i ^d Y_i ^d
\end{aligned}
$$

Wo $X_i ^d$ die Abweichung von $X_i$ vom Mittelwert $\mu_x$ der Variable $X$ ist und $Y_i ^d$ die Abweichung von $Y_i$ vom Mittelwert $\mu_y$ der Variable $Y$ ist.

**Problem:** Die Summe der Produkte der Abweichungen ist abhängig von der Stichprobengrösse $N$.

2.  Division durch Stichprobengrösse Kovarianz zwischen $X$ und $Y$

$$
\sigma_{x,y} = \frac{i=1}{N-1} (X_i - \mu_x)(Y_i - \mu_y)
$$

**Problem:** Die Kovarianz ist abhängig von den Einheiten der Variablen.

3.  Standardisierung durch die Standardabweichungen der Variablen $X$ und $Y$

$$
\rho_{X,Y} = \frac{\sum_{i=1}^N \frac{(X_i - \mu_x)}{\sigma_x} \frac{(Y_i - \mu_y)}{\sigma_y}}{N}
$$

wo:

$$
\begin{aligned}
\sigma_x &= \sqrt{\frac{\sum_{i=1}^N (X_i - \mu_x)^2}{N}} \text{ ,und} \\
\sigma_y &= \sqrt{\frac{\sum_{i=1}^N (Y_i - \mu_y)^2}{N}}
\end{aligned}
$$

4.  Ergebnis ist der Pearson-Korrelationskoeffizient $\rho_{x,y}$

$$
\rho_{X,Y} = \frac{\sum_{i=1}^N (X_i - \mu_x)(Y_i - \mu_y)}{\sqrt{\sum_{i=1}^N (X_i - \mu_x)^2 \sum_{i=1}^N (Y_i - \mu_y)^2}}
$$
:::

Wenn wir Daten plotten, können wir häufig Korrelationen auch visuell schon relativ gut schätzen.

```{r}
#| label: simple-correlation-examples
#| echo: true
#| code-fold: true
#| fig-cap: "Beispiele für Pearson-Korrelationen"

# Set seed for reproducibility
set.seed(123)

# Hohe Korrelation (r nahe bei 1)
x_high <- rnorm(30, mean = 5, sd = 1)
y_high <- 2 * x_high + rnorm(30, mean = 0, sd = 0.5)
r_high <- cor(x_high, y_high)

# Niedrige Korrelation (r nahe bei 0)
x_low <- rnorm(30, mean = 5, sd = 1)
y_low <- rnorm(30, mean = 5, sd = 1)
r_low <- cor(x_low, y_low)

# Plots nebeneinander
par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))

# Plot mit hoher Korrelation
plot(x_high, y_high, 
     main = paste("Hohe Korrelation\nr =", round(r_high, 3)), 
     xlab = "x", ylab = "y", 
     pch = 19, col = rgb(0, 0, 1, 0.5))

# Plot mit niedriger Korrelation
plot(x_low, y_low, 
     main = paste("Niedrige Korrelation\nr =", round(r_low, 3)), 
     xlab = "x", ylab = "y", 
     pch = 19, col = rgb(0, 0, 1, 0.5))
```

Die visuelle Darstellung erlaubt es uns, die Sensitivität des Pearson-Korrelationskoeffizienten gegenüber Ausreissern zu betrachten. Dafür fügen wir den Daten einen Ausreisser hinzu und sehen, wie sich der Korrelationskoeffizient drastisch verändert.
```{r}
#| label: outlier-influence
#| echo: true
#| code-fold: true
#| fig-cap: "Einfluss von Ausreissern auf den Pearson-Korrelationskoeffizienten"

# Berechnung des Pearson-Korrelationskoeffizienten

# Daten ohne Ausreisser generieren
x <- runif(30, 1, 10)
y <- runif(30, 1, 10)

# Korrelationskoeffizient ohne Ausreisser
r_no_outlier <- cor(x, y)

# Daten mit Ausreisser hinzufügen
x_outlier <- c(x, 20)
y_outlier <- c(y, 25)

# Pearson-Korrelationskoeffizient mit Ausreisser
r_with_outlier <- cor(x_outlier, y_outlier)

# Plots nebeneinander
par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))

# Plot ohne Ausreisser
plot(x, y, 
     main = paste("Ohne Ausreisser\nr =", round(r_no_outlier, 3)), 
     xlab = "x", ylab = "y", 
     pch = 19, col = rgb(0, 0, 1, 0.5), xlim = c(0, 22), ylim = c(0, 27))

# Plot mit Ausreisser
plot(x_outlier, y_outlier, 
     main = paste("Mit Ausreisser\nr =", round(r_with_outlier, 3)), 
     xlab = "x", ylab = "y", 
     pch = 19, col = c(rep(rgb(0, 0, 1, 0.5), 30), rgb(1, 0, 0, 0.5)), xlim = c(0, 22), ylim = c(0, 27))
```

## Der Spearman-Rangkorrelationskoeffizient

Der Spearman-Korrelationskoeffizient misst den monotonen Zusammenhang zwischen zwei Variablen anhand ihrer Ränge. Er ist robuster gegenüber Ausreissern als die Pearson-Korrelation und eignet sich auch für nicht-lineare Beziehungen.

- Wird verwendet, wenn der Zusammenhang nicht-linear, aber monoton ist.
- Robuster gegen Ausreißer als Pearson.
- Funktioniert für metrische und ordinale Daten.

$$
r_{(X,Y)} = 1 - \frac{6 \sum_{i=1}^n (r_{X_{i}} - r_{Y_{i}})^2}{n(n^2 - 1)}
$$

-   $r_{X_{i}}$ ist der Rang der $i$-ten Beobachtung von $X$
-   $r_{Y_{i}}$ ist der Rang der $i$-ten Beobachtung von $Y$
-   $n$ ist die Anzahl der Beobachtungen.
-   **ACHTUNG:** Vereinfachte Formel, wenn jeder Rang nur einmal vorkommt. Diese Formel darf bei unentschiedenen Rängen nicht verwendet werden.

::: {.callout-note title="Beispiel"}

Wir können mit unseren Daten aus @tbl-noten-lernzeit-beispiel auch den Spearman-Rangkorrelationskoeffizienten berechnen.

**1. Ränge berechnen**

```{r}
#| echo: true
#| label: noten-lernzeit-rankings
#| code-fold: false

noten_lernzeit_tabelle$note_rank <- rank(noten_lernzeit_tabelle$note)
noten_lernzeit_tabelle$lernzeit_rank <- rank(noten_lernzeit_tabelle$lernzeit)

noten_lernzeit_tabelle
```

| Student | Prüfungsnote ($X$) | $r_{X_{i}}$ | Lernzeit ($Y$) | $r_{Y_{i}}$ |
|---------|--------------------|-------------|----------------|-------------|
| A       | 70                 | 3           | 10             | 3           |
| B       | 80                 | 4           | 12             | 4           |
| C       | 50                 | 1           | 5              | 1           |
| D       | 90                 | 5           | 15             | 5           |
| E       | 60                 | 2           | 8              | 2           |

**2. Berechnung der Rangdifferenzen und quadrierten Rangdifferenzen**

```{r}
#| echo: true
#| label: noten-lernzeit-rangdifferenzen
#| code-fold: false

noten_lernzeit_tabelle$d_i <- noten_lernzeit_tabelle$note_rank - noten_lernzeit_tabelle$lernzeit_rank
noten_lernzeit_tabelle$d_i_sq <- noten_lernzeit_tabelle$d_i^2

sum(noten_lernzeit_tabelle$d_i_sq)
```

$$
\begin{aligned}
\sum d_i^2 &= \sum_{i=1}^N (r_{X_{i}} - r_{Y_{i}})^2 \\
&= (3-3)^2 + (4-4)^2 + (1-1)^2 + (5-5)^2 + (2-2)^2 \\
&= 0 + 0 + 0 + 0 + 0 \\
&= 0
\end{aligned}
$$

**3. Spearman-Korrelation berechnen**

```{r}
#| echo: true
#| label: noten-lernzeit-spearman
#| code-fold: false

cor(noten_lernzeit_tabelle$note, noten_lernzeit_tabelle$lernzeit, method = "spearman")
```

$$
\begin{aligned}
r_{(X,Y)} &= 1 - \frac{6 \sum d_i^2}{n(n^2 - 1)} \\
&= 1 - \frac{6 \times 0}{5(5^2 - 1)} \\
&= 1 - 0 \\
&= 1
\end{aligned}
$$

**Ergebnis:** $r_{X,Y} = 1$, was auf eine **perfekte positive Korrelation** hindeutet.

**4. Interpretation**

- Da $r_{(X,Y)} = 1$, bedeutet das, dass die Ränge von Prüfungsnoten und Lernzeit **perfekt übereinstimmen**.
- Das zeigt, dass die Variablen **monoton steigend zusammenhängen** – also mehr Lernzeit immer mit einer besseren Note einhergeht.

::: 

## Vergleich der Korrelationskoeffizienten

### Visualisierung der Korrelationen

```{r}
#| label: spearman-outlier-synthetic
#| echo: true
#| code-fold: true
#| fig-cap: "Einfluss von Ausreissern auf beide Korrelationskoeffizienten"

# Synthetische Daten generieren
x <- rnorm(100, mean = 10, sd = 1)
y <- 0.8 * x + rnorm(100, mean = 0, sd = 0.5)

# Funktion zur Berechnung von Pearson- und Spearman-Korrelation
correlations <- function(x, y) {
  list(
    pearson = round(cor(x, y, method = "pearson"), 2),
    spearman = round(cor(x, y, method = "spearman"), 2)
  )
}

# Layout für 2x2 Plots
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

# 1. Plot (ohne Ausreisser)
cor_vals <- correlations(x, y)
plot(x, y, pch = 19, col = rgb(0, 0, 1, 0.5),
     main = paste0("r(Spearman) = ", cor_vals$spearman, 
                   "\nr(Pearson) = ", cor_vals$pearson),
     xlab = "x", ylab = "y")

# 2. Plot (Ausreisser unten rechts)
x2 <- c(x, 12)
y2 <- c(y, -3)
cor_vals2 <- correlations(x2, y2)
plot(x2, y2, pch = c(rep(19, 100), 19), col = c(rep(rgb(0, 0, 1, 0.5), 100), rgb(1, 0, 0, 0.5)),
     main = paste0("r(Spearman) = ", cor_vals2$spearman, 
                   "\nr(Pearson) = ", cor_vals2$pearson),
     xlab = "x", ylab = "y")

# 3. Plot (Ausreisser oben links)
x3 <- c(x, 7.5)
y3 <- c(y, 1.5)
cor_vals3 <- correlations(x3, y3)
plot(x3, y3, pch = c(rep(19, 100), 19), col = c(rep(rgb(0, 0, 1, 0.5), 100), rgb(1, 0, 0, 0.5)),
     main = paste0("r(Spearman) = ", cor_vals3$spearman, 
                   "\nr(Pearson) = ", cor_vals3$pearson),
     xlab = "x", ylab = "y")

# 4. Plot (zwei Ausreisser oben links und unten rechts)
x4 <- c(x, 7.5, 12)
y4 <- c(y, 1.5, -3)
cor_vals4 <- correlations(x4, y4)
plot(x4, y4, pch = c(rep(19, 100), 19, 19), col = c(rep(rgb(0, 0, 1, 0.5), 100), rgb(1, 0, 0, 0.5), rgb(1, 0, 0, 0.5)),
     main = paste0("r(Spearman) = ", cor_vals4$spearman, 
                   "\nr(Pearson) = ", cor_vals4$pearson),
     xlab = "x", ylab = "y")
```

```{r}
#| label: spearman-nonlinearity
#| echo: true
#| code-fold: true
#| fig-cap: "Einfluss von Nicht-Linearitäten auf beide Korrelationskoeffizienten"

# Set seed for reproducibility
set.seed(42)

# Basisdaten generieren
x <- runif(100, 7, 12)  # x-Werte im Bereich 7 bis 12

# 1. Lineare Beziehung
y_linear <- 0.8 * x + rnorm(100, mean = 0, sd = 0.5)

# 2. U-förmige (quadratische) Beziehung
y_quadratic <- -1 * (x - 9.5)^2 + 8 + rnorm(100, mean = 0, sd = 0.5)

# 3. Logarithmische Beziehung
y_logarithmic <- log(x - 6.5) + rnorm(100, mean = 0, sd = 0.3)

# 4. Exponentielle Beziehung
y_exponential <- exp((x - 10) / 3) + rnorm(100, mean = 0, sd = 0.5)

# Funktion zur Berechnung von Pearson- und Spearman-Korrelation
correlations <- function(x, y) {
  list(
    pearson = round(cor(x, y, method = "pearson"), 2),
    spearman = round(cor(x, y, method = "spearman"), 2)
  )
}

# Layout für 2x2 Plots
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))

# Farben definieren
colors <- rgb(0, 0, 1, 0.5)

# 1. Plot (lineare Beziehung)
cor_vals1 <- correlations(x, y_linear)
plot(x, y_linear, pch = 19, col = colors,
     main = paste0("r(Spearman) = ", cor_vals1$spearman, 
                   "\nr(Pearson) = ", cor_vals1$pearson),
     xlab = "x", ylab = "y")

# 2. Plot (quadratische Beziehung)
cor_vals2 <- correlations(x, y_quadratic)
plot(x, y_quadratic, pch = 19, col = colors,
     main = paste0("r(Spearman) = ", cor_vals2$spearman, 
                   "\nr(Pearson) = ", cor_vals2$pearson),
     xlab = "x", ylab = "y")

# 3. Plot (logarithmische Beziehung)
cor_vals3 <- correlations(x, y_logarithmic)
plot(x, y_logarithmic, pch = 19, col = colors,
     main = paste0("r(Spearman) = ", cor_vals3$spearman, 
                   "\nr(Pearson) = ", cor_vals3$pearson),
     xlab = "x", ylab = "y")

# 4. Plot (exponentielle Beziehung)
cor_vals4 <- correlations(x, y_exponential)
plot(x, y_exponential, pch = 19, col = colors,
     main = paste0("r(Spearman) = ", cor_vals4$spearman, 
                   "\nr(Pearson) = ", cor_vals4$pearson),
     xlab = "x", ylab = "y")
```

### Vergleichstabelle

+--------------------------------------------------+---------------------------------------------------------+------------------------------------------------------------+
| Kriterium                                        | Pearson                                                 | Spearman                                                   |
+==================================================+=========================================================+============================================================+
| **Art des Zusammenhangs**                        | - Misst lineare Zusammenhänge                           | - Misst monotone Zusammenhänge (linear oder nicht-linear)  |
+--------------------------------------------------+---------------------------------------------------------+------------------------------------------------------------+
| **Anwendung**                                    | - Häufig in der Statistik für metrische Variablen       | - Ideal für Rangdaten oder nicht normalverteilte Daten     |
+--------------------------------------------------+---------------------------------------------------------+------------------------------------------------------------+
| **Voraussetzungen**                              | - Normalverteilung der Variablen                        | - Keine Normalverteilung erforderlich                      |
|                                                  | - Linearität                                            | - Monotone Beziehung erforderlich                          |
+--------------------------------------------------+---------------------------------------------------------+------------------------------------------------------------+
| **Datentypen**                                   | - Metrische (intervall- oder verhältnisskalierte) Daten | - Ordinal-, Intervall- und Verhältnisskalen                |
+--------------------------------------------------+---------------------------------------------------------+------------------------------------------------------------+
| **Sensitivität gegenüber Ausreissern**           | - Sehr empfindlich gegenüber Ausreissern                | - Robust gegenüber Ausreissern                             |
+--------------------------------------------------+---------------------------------------------------------+------------------------------------------------------------+
| **Robustheit bei nicht-linearen Zusammenhängen** | - Nicht robust bei nicht-linearen Zusammenhängen        | - Robust bei nicht-linearen, aber monotonen Zusammenhängen |
+--------------------------------------------------+---------------------------------------------------------+------------------------------------------------------------+
| **Skalenniveau**                                 | - Intervall- oder verhältnisskaliert                    | - Mindestens ordinalskaliert                               |
+--------------------------------------------------+---------------------------------------------------------+------------------------------------------------------------+
| **Berechnungsgrundlage**                         | - Kovarianz, normiert durch Standardabweichung          | - Berechnet auf Basis von Rangdifferenzen                  |
+--------------------------------------------------+---------------------------------------------------------+------------------------------------------------------------+
| **Vorteile**                                     | - Einfach zu interpretieren                             | - Robust gegenüber Ausreissern                             |
|                                                  | - Weit verbreitet                                       | - Geeignet für nicht-lineare monotone Beziehungen          |
+--------------------------------------------------+---------------------------------------------------------+------------------------------------------------------------+
| **Nachteile**                                    | - Nicht robust gegenüber Ausreissern                    | - Weniger empfindlich bei linearen Zusammenhängen          |
|                                                  | - Nicht geeignet für nicht-lineare Zusammenhänge        | - Informationsverlust durch Rangkodierung                  |
+--------------------------------------------------+---------------------------------------------------------+------------------------------------------------------------+
