[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quantitative Methoden",
    "section": "",
    "text": "Diese Website dient als Sammlung meiner Vorlesungsnotizen und Übungen für die Vorlesung “Quantitative Methoden” im Herbstsemester 2024.\nDiese Sammlung ist aktuell noch am Entstehen.\n\n\n\n\n\n\nHinweis\n\n\n\nDie Inhalte dieser Website können Lücken und Fehler enthalten. Die Inhalte werden laufend ergänzt und verbessert.\nFehler und Vorschläge können auf jeder Seite direkt über Problem melden auf Github gemeldet werden. Bitte das entsprechende Issue-Template verwenden.\nFalls bereits Erfahrung mit Github und quarto besteht, kann auch jeweils direkt die Seite bearbeitet werden. Anschliessend kann ein Pull-Request erstellt werden.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Übersicht</span>"
    ]
  },
  {
    "objectID": "grundlagen_R.html",
    "href": "grundlagen_R.html",
    "title": "2  Grundlagen R",
    "section": "",
    "text": "3 Grundlagen R",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen R</span>"
    ]
  },
  {
    "objectID": "grundlagen_R.html#grundsätzliches-zu-r",
    "href": "grundlagen_R.html#grundsätzliches-zu-r",
    "title": "2  Grundlagen R",
    "section": "3.1 Grundsätzliches zu R",
    "text": "3.1 Grundsätzliches zu R\n\n3.1.1 Zuweisungsoperator\nIn R werden Werte Variablen mit dem &lt;- Operator zugewiesen.\n\n# Variablen erstellen und Wert zuweisen\na &lt;- 5\n\n# Die Zuweisung kann auch umgekehrt erfolgen\n5 -&gt; b\n\n# Das gleiche funktioniert grundsätzlich aber auch mit dem = Operator\n# Allerdings wird der &lt;- Operator bevorzugt\nc = 10\n\n\n\n3.1.2 Kommentare\nKommentare in R werden mit einem # eingeleitet. Sie können entweder in einer eigenen Zeile stehen oder am Ende einer Codezeile.\n\n# Das ist ein Kommentar\na &lt;- 5 # Das ist auch ein Kommentar\n\n\n\n3.1.3 Ausgabe\nIn R können Werte entweder mit der print() Funktion oder einfach durch Eingabe des Variablennamens ausgegeben werden.\n\n# Ausgabe von Variablen\nprint(a)\n\n[1] 5\n\nb\n\n[1] 5",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen R</span>"
    ]
  },
  {
    "objectID": "grundlagen_R.html#datentypen",
    "href": "grundlagen_R.html#datentypen",
    "title": "2  Grundlagen R",
    "section": "3.2 Datentypen",
    "text": "3.2 Datentypen\nIn R gibt es verschiedene Datentypen. Die wichtigsten sind:\n\nNumerische Werte (z.B. 5, 3.14)\nZeichenketten (Strings) (z.B. \"Hallo Welt\")\nLogische Werte (TRUE, FALSE)\nVektoren (in Python Listen) (z.B. c(1, 2, 3))1\nData Frames (ähnlich wie Tabellen, Mischen von Datentypen möglich)\nMatrizen (ähnlich wie in der Mathematik, keine Mischung von Datentypen möglich)\n\n\n3.2.1 Vektoren\nDa Vektoren eine der grundlegenden Datenstrukturen in R sind, werden wir uns diese genauer ansehen.\nSie können mit der c() Funktion erstellt werden. Vektoren sind grundsätzlich ähnlich wie Objekte vom Typ list in Python. Vektoren können sämtliche Datentypen enthalten, jedoch nur einen Datentyp pro Vektor.\nAuf Vektoren können verschiedene Operationen durchgeführt werden, wie z.B. Addition, Subtraktion, Multiplikation, Division, etc.\n\n# Vektor erstellen\nvectorA &lt;- c(1, 2, 3, 4, 5)\n\n# Länge des Vektors\nlength(vectorA)\n\n[1] 5\n\n# Logischer Vergleich\nvectorA &gt;= 3\n\n[1] FALSE FALSE  TRUE  TRUE  TRUE\n\n# Arithmetische Operationen\nvectorA + 5\n\n[1]  6  7  8  9 10\n\n\nWenn wir nun den Vektor vectorA erneut ausgeben, beobachten wir, dass die Operationen nicht den ursprünglichen Vektor verändert haben.\n\nvectorA\n\n[1] 1 2 3 4 5\n\n\nUm den Vektor zu verändern, müssen wir den veränderten Vektor entweder einer neuen Variablen zuweisen oder den Vektor direkt überschreiben.\n\n# Vektor einer neuen Variablen zuweisen\nvectorB &lt;- vectorA + 5\n\n# Vektor ausgeben\nvectorB\n\n[1]  6  7  8  9 10\n\n# Wir können den Vektor auch direkt überschreiben\nvectorA &lt;- vectorA + 5\n\n# Vektor ausgeben\nvectorA\n\n[1]  6  7  8  9 10\n\n\n\n3.2.1.1 Indizierung\nVektoren können indiziert werden, um auf bestimmte Elemente zuzugreifen.\nDie Indizierung beginnt in R bei 1.\n\n# Erstes Element des Vektors\nvectorA[1]\n\n[1] 6\n\n\nWir können auch auf mehrere Elemente gleichzeitig zugreifen. Wenn wir z.B. auf das zweite bis vierte Element des Vektors zugreifen wollen, können wir dies mit dem : Operator tun.\n\n# Zweites bis viertes Element des Vektors\nvectorA[2:4]\n\n[1] 7 8 9\n\n# Alternativ können wir auch einzelne Elemente überspringen\nvectorA[c(1, 3, 5)]\n\n[1]  6  8 10\n\n\n\n\n3.2.1.2 Vektoren konkatenieren\nVektoren können auch konkateniert werden.\n\n# Vektoren erstellen\nvectorA &lt;- c(1, 2, 3, 4, 5)\n\n# Vektoren konkatenieren\nvectorC &lt;- c(vectorA, vectorB)\n\n# Vektor ausgeben\nvectorC\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n3.2.1.3 Vektoren addieren\nVektoren können auch addiert werden. Hierbei werden die Vektoren elementweise addiert.2\n\n# Vektoren addieren\nvectorA + vectorB\n\n[1]  7  9 11 13 15\n\n\n\n\n3.2.1.4 Wichtige Funktionen für Vektoren\n\n\n\n\n\n\n\n\n\nFunktion\nBeschreibung\nBeispiel\nAusgabe\n\n\n\n\nlength()\nLänge des Vektors\nlength(vectorA)\n5\n\n\nsum()\nSumme der Elemente des Vektors\nsum(vectorA)\n15\n\n\nmean()\nDurchschnitt der Elemente des Vektors\nmean(vectorA)\n3\n\n\nvar()\nVarianz der Elemente des Vektors\nvar(vectorA)\n2.5\n\n\nsd()\nStandardabweichung der Elemente des Vektors\nsd(vectorA)\n1.5811388\n\n\nmin()\nMinimum des Vektors\nmin(vectorA)\n1\n\n\nmax()\nMaximum des Vektors\nmax(vectorA)\n5\n\n\nrange()\nBereich des Vektors\nrange(vectorA)\n1, 5\n\n\n\n\n\n\n3.2.2 Data Frames\nIn Data Frames können Vektoren unterschiedlicher Datentypen kombiniert werden. Sie sind ähnlich wie Tabellen in relationalen Datenbanken.\nData Frames können mit der data.frame() Funktion direkt erstellt werden.\n\n# Data Frame erstellen\ndataFrameA &lt;- data.frame(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\"),\n  age = c(25, 30, 35, 40),\n  married = c(TRUE, FALSE, TRUE, TRUE)\n)\n\n# Data Frame ausgeben\ndataFrameA\n\n     name age married\n1   Alice  25    TRUE\n2     Bob  30   FALSE\n3 Charlie  35    TRUE\n4   David  40    TRUE\n\n\nWichtig ist, dass die Vektoren, die im Data Frame kombiniert werden sollen, die gleiche Länge haben müssen und dass die Vektoren nur einen Datentyp pro Vektor enthalten dürfen.\nWir beobachten auch hier, dass die Vektoren in der Initialisierung des Data Frames wieder mit der c() Funktion erstellt werden.\n\n3.2.2.1 Indizierung\nData Frames können indiziert werden, um auf bestimmte Elemente zuzugreifen.\nDie Indizierung erfolgt ähnlich wie bei Vektoren, jedoch mit dem Unterschied, dass wir zusätzlich auch die gewünschte Spalte angeben müssen. Dies erfolgt durch die Angabe der Zeilen- und Spaltennummer in eckigen Klammern.\nWenn wir eine ganze Zeile ausgeben wollen, geben wir nur die Zeilennummer an und lassen die Spaltennummer weg.\n\n# Erste Zeile des Data Frames\ndataFrameA[1, ]\n\n   name age married\n1 Alice  25    TRUE\n\n# Zweite Zeile und dritte Spalte des Data Frames\ndataFrameA[2, 3]\n\n[1] FALSE\n\n\nWir können mit dem $ Operator auch direkter auf bestimmte Spalten zugreifen.\n\n# Spalte \"name\" des Data Frames\ndataFrameA$name\n\n[1] \"Alice\"   \"Bob\"     \"Charlie\" \"David\"  \n\n\n\n\n3.2.2.2 Auswahl aus Data Frames wieder als Data Frame\nWir können subsetting verwenden, um einen Teil des Data Frames auszuwählen und diesen wieder als Data Frame zu speichern.\n\n# Auswahl der Spalten \"name\" und \"age\" als Data Frame\ndataFrameB &lt;- dataFrameA[, c(\"name\", \"age\")]\n\n# Data Frame ausgeben\ndataFrameB\n\n     name age\n1   Alice  25\n2     Bob  30\n3 Charlie  35\n4   David  40\n\n\n\n\n3.2.2.3 Wichtige Funktionen für Data Frames\n\n\n\n\n\n\n\n\n\nFunktion\nBeschreibung\nBeispiel\nAusgabe\n\n\n\n\nnrow()\nAnzahl der Zeilen des Data Frames\nnrow(dataFrameA)\n4\n\n\nncol()\nAnzahl der Spalten des Data Frames\nncol(dataFrameA)\n3\n\n\ncolnames()\nSpaltennamen des Data Frames\ncolnames(dataFrameA)\nname, age, married\n\n\nrownames()\nZeilennamen des Data Frames\nrownames(dataFrameA)\n1, 2, 3, 4\n\n\nsummary()\nZusammenfassung des Data Frames\nsummary(dataFrameA)\nLength:4 , Class :character , Mode :character , NA, NA, NA, Min. :25.00 , 1st Qu.:28.75 , Median :32.50 , Mean :32.50 , 3rd Qu.:36.25 , Max. :40.00 , Mode :logical , FALSE:1 , TRUE :3 , NA, NA, NA",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen R</span>"
    ]
  },
  {
    "objectID": "grundlagen_R.html#daten-importieren-und-exportieren",
    "href": "grundlagen_R.html#daten-importieren-und-exportieren",
    "title": "2  Grundlagen R",
    "section": "3.3 Daten importieren und exportieren",
    "text": "3.3 Daten importieren und exportieren\nIn R können Daten aus verschiedenen Dateiformaten importiert und exportiert werden. Dazu gehören z.B. CSV-Dateien, Excel-Dateien, JSON-Dateien, etc.\n\n3.3.1 CSV-Dateien\nCSV-Dateien können mit der read.csv() Funktion in R eingelesen werden.\n\n# CSV-Datei einlesen\nmeteodaten &lt;- read.csv('Data/meteodaten_saison.csv', sep = ',', header = TRUE)\n\nIn diesem Beispiel wird die CSV-Datei meteodaten_saison.csv eingelesen. Der Parameter sep = ',' gibt an, dass die Werte in der CSV-Datei durch Kommas getrennt sind. Der Parameter header = TRUE gibt an, dass die erste Zeile der CSV-Datei die Spaltennamen enthält.\n\n\n\n\n\n\nAchtung\n\n\n\nDer Pfad zur Datei muss entweder absolut (Bsp.: C:/Users/username/Documents/data.csv) oder relativ zum aktuellen Arbeitsverzeichnis (Bsp.: Data/data.csv) angegeben werden.\nIn aller Regel ist es sinnvoll, den Pfad relativ zum aktuellen Arbeitsverzeichnis anzugeben. Das aktuelle Arbeitsverzeichnis kann in der Terminalkonsole mit dem Befehl getwd() abgefragt werden, und mit dem Befehl setwd() kann das Arbeitsverzeichnis geändert werden.\nEine gängige (und hier angewandte) Praxis ist es, ein Unterverzeichnis Data im Projektverzeichnis anzulegen und dort alle Daten abzulegen.\nDer einfachste Weg ist es im Explorer einen Ordner anzulegen in welchem alle Skripte gespeichert werden. In diesem Ordner kann dann ein Unterordner Data erstellt werden, in welchem die Daten abgelegt werden.\nIn RStudio kann das Arbeitsverzeichnis über das Menü Session -&gt; Set Working Directory -&gt; Choose Directory... gesetzt werden.\n\n\n\n3.3.1.1 Wichtige Funktionen für Datenimport\n\n\nstr(): Zeigt die Struktur des Data Frames an.\n\n\nstr(meteodaten)\n\n'data.frame':   492 obs. of  6 variables:\n $ Jahr                           : int  1901 1901 1901 1901 1902 1902 1902 1902 1903 1903 ...\n $ Saison                         : chr  \"Fruehling(MAM)\" \"Herbst(SON)\" \"Sommer(JJA)\" \"Winter(DJF)\" ...\n $ Bern_Mitteltemperatur          : num  7.73 7.4 16.8 -2.73 7.53 ...\n $ Bern_Niederschlagssumme        : num  278 245 381 112 323 ...\n $ GrStBernhard_Mitteltemperatur  : num  -4 -0.8 6.3 -10.6 -3.63 ...\n $ GrStBernhard_Niederschlagssumme: num  495 521 285 356 448 ...\n\n\n\n\n\nhead(): Zeigt die ersten Zeilen des Data Frames an.\n\n\nhead(meteodaten)\n\n  Jahr         Saison Bern_Mitteltemperatur Bern_Niederschlagssumme\n1 1901 Fruehling(MAM)              7.733333                   277.8\n2 1901    Herbst(SON)              7.400000                   244.9\n3 1901    Sommer(JJA)             16.800000                   381.1\n4 1901    Winter(DJF)             -2.733333                   112.4\n5 1902 Fruehling(MAM)              7.533333                   323.2\n6 1902    Herbst(SON)              7.466667                   231.7\n  GrStBernhard_Mitteltemperatur GrStBernhard_Niederschlagssumme\n1                     -4.000000                           494.7\n2                     -0.800000                           520.8\n3                      6.300000                           285.2\n4                    -10.600000                           356.2\n5                     -3.633333                           448.1\n6                     -1.000000                           335.6\n\n\n\n\n\ntail(): Zeigt die letzten Zeilen des Data Frames an.\n\n\ntail(meteodaten)\n\n    Jahr         Saison Bern_Mitteltemperatur Bern_Niederschlagssumme\n487 2022    Sommer(JJA)             20.000000                   238.3\n488 2022    Winter(DJF)              2.233333                   184.7\n489 2023 Fruehling(MAM)              9.533333                   272.5\n490 2023    Herbst(SON)             11.966667                   371.2\n491 2023    Sommer(JJA)             20.000000                   203.1\n492 2023    Winter(DJF)              2.700000                   233.1\n    GrStBernhard_Mitteltemperatur GrStBernhard_Niederschlagssumme\n487                     10.266667                           256.8\n488                     -4.966667                           270.0\n489                     -1.666667                           388.4\n490                      2.966667                           553.3\n491                      9.200000                           288.4\n492                     -5.333333                           228.7\n\n\n\n\n\nsummary(): Gibt eine Zusammenfassung des Data Frames aus.\n\n\nsummary(meteodaten)\n\n      Jahr         Saison          Bern_Mitteltemperatur\n Min.   :1901   Length:492         Min.   :-4.500       \n 1st Qu.:1931   Class :character   1st Qu.: 4.883       \n Median :1962   Mode  :character   Median : 8.750       \n Mean   :1962                      Mean   : 8.715       \n 3rd Qu.:1993                      3rd Qu.:12.750       \n Max.   :2023                      Max.   :21.100       \n Bern_Niederschlagssumme GrStBernhard_Mitteltemperatur\n Min.   : 47.9           Min.   :-11.2000             \n 1st Qu.:185.7           1st Qu.: -5.1000             \n Median :243.2           Median : -1.4000             \n Mean   :253.3           Mean   : -0.9232             \n 3rd Qu.:308.9           3rd Qu.:  3.2750             \n Max.   :600.1           Max.   : 10.5333             \n GrStBernhard_Niederschlagssumme\n Min.   : 125.9                 \n 1st Qu.: 383.1                 \n Median : 491.5                 \n Mean   : 513.9                 \n 3rd Qu.: 619.2                 \n Max.   :1351.6                 \n\n\n\n\n\nIndizierung: Mit der Indizierung können bestimmte Zeilen und Spalten des Data Frames ausgewählt werden.\n\n\nmeteodaten[1:10,] # Ersten 10 Zeilen\n\n   Jahr         Saison Bern_Mitteltemperatur Bern_Niederschlagssumme\n1  1901 Fruehling(MAM)              7.733333                   277.8\n2  1901    Herbst(SON)              7.400000                   244.9\n3  1901    Sommer(JJA)             16.800000                   381.1\n4  1901    Winter(DJF)             -2.733333                   112.4\n5  1902 Fruehling(MAM)              7.533333                   323.2\n6  1902    Herbst(SON)              7.466667                   231.7\n7  1902    Sommer(JJA)             16.466667                   295.9\n8  1902    Winter(DJF)             -0.800000                   193.9\n9  1903 Fruehling(MAM)              7.433333                   177.6\n10 1903    Herbst(SON)              8.766667                   267.3\n   GrStBernhard_Mitteltemperatur GrStBernhard_Niederschlagssumme\n1                     -4.0000000                           494.7\n2                     -0.8000000                           520.8\n3                      6.3000000                           285.2\n4                    -10.6000000                           356.2\n5                     -3.6333333                           448.1\n6                     -1.0000000                           335.6\n7                      5.3000000                           242.7\n8                     -7.4000000                           341.2\n9                     -4.4666667                           409.7\n10                    -0.4666667                           507.1",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen R</span>"
    ]
  },
  {
    "objectID": "grundlagen_R.html#daten-als-.csv--und-.txt-dateien-exportieren",
    "href": "grundlagen_R.html#daten-als-.csv--und-.txt-dateien-exportieren",
    "title": "2  Grundlagen R",
    "section": "3.4 Daten als .csv- und .txt-Dateien exportieren",
    "text": "3.4 Daten als .csv- und .txt-Dateien exportieren\nDaten können mit der write.csv() Funktion als CSV-Dateien und mit der write.table() Funktion als Textdateien exportiert werden.\n\n3.4.1 .csv-Dateien\n\n# CSV-Datei exportieren\nwrite.csv(meteodaten,\n          file = \"meteodaten.csv\",\n          row.names = FALSE)\n\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\nx\nDas Data Frame, das exportiert werden soll.\n\n\nfile\nDateiname und Speicherort. Auch hier können absolute und relative Dateipfade verwendet werden.\n\n\nrow.names\nGibt an, ob die Zeilennummern in der CSV-Datei gespeichert werden sollen.\n\n\n\n\n\n3.4.2 .txt-Dateien\n\n# Textdatei exportieren\nwrite.table(meteodaten, # Das Data Frame, das exportiert werden soll\n            file = \"meteodaten.txt\", # Dateiname und Speicherort\n            sep = \"\\t\", # Tabulator als Trennzeichen\n            eol = \"\\r\", # Zeilenumbruch\n            na = \"NA\", # Wert für fehlende Daten\n            row.names = FALSE,\n            col.names = TRUE)\n\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\nx\nDas Data Frame, das exportiert werden soll.\n\n\nfile\nDateiname und Speicherort. Auch hier können absolute und relative Dateipfade verwendet werden.\n\n\nsep\nTrennzeichen für die Spalten.\n\n\neol\nZeilenumbruch. Kann je nach Betriebssystem erforderlich sein.\n\n\nna\nWert für fehlende Daten.\n\n\nrow.names\nGibt an, ob die Zeilennummern in der Textdatei gespeichert werden sollen.\n\n\ncol.names\nGibt an, ob die Spaltennamen in der Textdatei gespeichert werden sollen.\n\n\n\nWie sonst auch, haben die meisten Parameter Standardwerte, die nicht explizit angegeben werden müssen.",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen R</span>"
    ]
  },
  {
    "objectID": "grundlagen_R.html#umgang-mit-fehlwerten",
    "href": "grundlagen_R.html#umgang-mit-fehlwerten",
    "title": "2  Grundlagen R",
    "section": "3.5 Umgang mit Fehlwerten",
    "text": "3.5 Umgang mit Fehlwerten\nFehlwerte sind in Datenanalysen ein häufiges Problem. In R werden Fehlwerte standardmässig mit NA (Not Available) dargestellt.\n\n3.5.1 Fehlwerte in Funktionsaufrufen behandeln\nErstellen wir ein Beispiel-Vektor mit Fehlwerten:\n\nvektor_mit_na &lt;- c(1, 2, NA, 4, 5)\n\nWenn wir nun z.B. die Summe des Vektors berechnen, erhalten wir:\n\nsum(vektor_mit_na)\n\n[1] NA\n\n\nDie Ausgabe ist NA, da R nicht weiss, wie es mit dem Fehlwert umgehen soll.\nWir können in diversen Funktionen definieren, wie mit Fehlwerten umgegangen werden soll. Dazu können wir den na.rm Parameter verwenden. (rm = remove) Dieser Parameter ist standardmässig auf FALSE gesetzt.\n\nsum(vektor_mit_na,\n    na.rm = TRUE)\n\n[1] 12\n\n\nDie Ausgabe ist 12, da der Fehlwert ignoriert wird.\nDies funktioniert auch bei anderen Funktionen, wie z.B. mean(), var(), sd(), etc.\n\n\n3.5.2 Fehlwerte im Dateiimport behandeln\nBeim Import von Daten können wir mit dem Parameter na.strings definieren, welche Werte als Fehlwerte interpretiert werden sollen.\n\n# CSV-Datei einlesen\nmeteodaten &lt;- read.csv('Data/meteodaten_saison.csv',\n                        sep = ',',\n                        header = TRUE,\n                        na.strings = c(\"NA\", \"N/A\", \"na\"))\n\nIn diesem Beispiel definieren wir, dass die Werte \"NA\", \"N/A\" und \"na\" als Fehlwerte interpretiert werden sollen.\n\n\n3.5.3 Fehlwerte identifizieren\nFehlwerte können mit der is.na() Funktion identifiziert und mit der na.omit() Funktion entfernt werden.\n\n# Fehlwerte identifizieren\nis.na(vektor_mit_na)\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n# Fehlwerte entfernen\nvektor_ohne_na &lt;- na.omit(vektor_mit_na)\n\nDer Vektor ist nun: 1, 2, 4, 5.\n\n3.5.3.1 Überprüfen auf Gleichheit\nFehlwerte müssen mit is.na() überprüft werden, da sie nicht mit == verglichen werden können.3\n\n# Überprüfen auf Gleichheit\nvektor_mit_na == NA\n\n[1] NA NA NA NA NA\n\n# Überprüfen auf Gleichheit mit is.na()\nis.na(vektor_mit_na)\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n\n\n\n\n3.5.4 Fehlwerte ersetzen\nMit der replace() Funktion können Fehlwerte am einfachsten ersetzt werden.\n\n# Vector mit falschen Fehlwerten\nvektor_mit_falschen_na &lt;- c(1, 2, -999, 4, 5)\n\n# Fehlwerte ersetzen\nvektor_mit_korrigierten_na &lt;- replace(vektor_mit_falschen_na,\n                                      vektor_mit_falschen_na == -999,\n                                      NA)\n\nDer Vektor ist nun: 1, 2, NA, 4, 5.",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen R</span>"
    ]
  },
  {
    "objectID": "grundlagen_R.html#übungen",
    "href": "grundlagen_R.html#übungen",
    "title": "2  Grundlagen R",
    "section": "3.6 Übungen",
    "text": "3.6 Übungen\nBerechne die Sommer (JJA) Temperaturanomalien zur Referenzperiode 1961 bis 1990 in Bern.\n\n# Daten einlesen\nmeteodaten &lt;- read.csv('Data/meteodaten_saison.csv', sep = ',', header = TRUE)\n\n# Daten filtern\nsaison_sommer &lt;- meteodaten[meteodaten$Saison == \"Sommer(JJA)\", ]\n\n# Referenzwert der Periode 1961 bis 1990 berechnen\nreferenzwert &lt;- mean(\n    saison_sommer$Bern_Mitteltemperatur[saison_sommer$Jahr &gt;= 1961\n    & saison_sommer$Jahr &lt;= 1990])\n\n# Sommer (JJA) Temperaturanomalien berechnen und direkt im data frame speichern\nsaison_sommer$Bern_Mitteltemperatur_anomalie &lt;-\n    saison_sommer$Bern_Mitteltemperatur - referenzwert\n\n# Plot erstellen\nplot(saison_sommer$Jahr,\n    saison_sommer$Bern_Mitteltemperatur_anomalie,\n    type = \"l\",\n    xlab = \"Jahr\",\n    ylab = \"Temperaturanomalie (°C)\",\n    main = \"Sommer (JJA) Temperaturanomalien in Bern\")\n\n# null-Linie hinzufügen\nabline(h = 0, col = \"red\")",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen R</span>"
    ]
  },
  {
    "objectID": "grundlagen_R.html#footnotes",
    "href": "grundlagen_R.html#footnotes",
    "title": "2  Grundlagen R",
    "section": "",
    "text": "Kleine Anmerkung: Hier wird der Vektor mit der c() Funktion erstellt. Diese Funktion wird verwendet, um Werte zu kombinieren (combine).↩︎\nWenn die Vektoren unterschiedliche Längen haben, wird der kürzere Vektor so oft wiederholt, bis er die Länge des längeren Vektors hat. Wenn die Länge des längeren Vektors kein Vielfaches der Länge des kürzeren Vektors ist, wird eine Warnung ausgegeben.↩︎\nDer Vergleich von Fehlwerten mit == ergibt immer NA, da R nicht weiss, ob der Fehlwert gleich einem anderen Wert ist oder nicht.↩︎",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Grundlagen R</span>"
    ]
  },
  {
    "objectID": "plots_in_R.html",
    "href": "plots_in_R.html",
    "title": "3  Einfache Plots erstellen",
    "section": "",
    "text": "4 Einfache Plots erstellen\n“High-level” Plots können in R mit der plot() Funktion erstellt werden. Diese Funktion erstellt automatisch für die gegebenen Daten ein (geeignetes) Diagramm.\nWenn wir aber Beispielsweise unser ganzes meteodaten Data Frame plotten wollen, weiss die Funktion nicht, wie sie das tun soll und plottet einfach alle Spalten gegen alle anderen Zeilen. Dies ist in diesem Fall nicht sinnvoll.\nplot(meteodaten)",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Einfache Plots erstellen</span>"
    ]
  },
  {
    "objectID": "plots_in_R.html#liniendiagramme",
    "href": "plots_in_R.html#liniendiagramme",
    "title": "3  Einfache Plots erstellen",
    "section": "4.1 Liniendiagramme",
    "text": "4.1 Liniendiagramme\nUm unsere Daten sinnvoller zu plotten müssen wir die Daten zuerst filtern und dann plotten.\nWir diskutieren die verschiedenen Schritte hier später:\n\n# Subsets der Daten erstellen\nsaison_fruehling &lt;- meteodaten[meteodaten$Saison == \"Fruehling(MAM)\", ]\nsaison_sommer &lt;- meteodaten[meteodaten$Saison == \"Sommer(JJA)\", ]\nsaison_herbst &lt;- meteodaten[meteodaten$Saison == \"Herbst(SON)\", ]\nsaison_winter &lt;- meteodaten[meteodaten$Saison == \"Winter(DJF)\", ]\n\n# Plot mit den Saisontemperaturen erstellen erstellen\nplot(saison_fruehling$Jahr, saison_fruehling$Bern_Mitteltemperatur,\n    type = \"l\",\n    xlab = \"Jahr\",\n    ylab = \"Durchschnittstemperatur in °C\",\n    main = \"Saisonale Durchschnittstemperaturen in Bern\",\n    xlim = c(1900, 2020),\n    ylim = c(-5, 35))\n\n# Weitere Linien für andere Jahreszeiten auf den selben Plot hinzufügen\nlines(saison_sommer$Jahr, saison_sommer$Bern_Mitteltemperatur,\n    col = \"red\")\nlines(saison_herbst$Jahr, saison_herbst$Bern_Mitteltemperatur,\n    col = \"green\")\nlines(saison_winter$Jahr, saison_winter$Bern_Mitteltemperatur,\n    col = \"blue\")\n\n# Gestrichelte Horizontale Linie bei 0 hinzufügen\nabline(h = 0, lty = 2)\n\n# Saisonale Mittelwertlinien hinzufügen\nabline(h = mean(saison_fruehling$Bern_Mitteltemperatur),\n    col = \"black\",\n    lty = 3)\nabline(h = mean(saison_sommer$Bern_Mitteltemperatur),\n    col = \"red\",\n    lty = 3)\nabline(h = mean(saison_herbst$Bern_Mitteltemperatur),\n    col = \"green\",\n    lty = 3)\nabline(h = mean(saison_winter$Bern_Mitteltemperatur),\n    col = \"blue\",\n    lty = 3)\n\n# Legende hinzufügen\nlegend(\"topright\",\n    legend = c(\"Frühling\", \"Sommer\", \"Herbst\", \"Winter\"),\n    col = c(\"black\", \"red\", \"green\", \"blue\"),\n    lty = 1,\n    xpd = TRUE)\n\n\n\n\n\n\n\n\nSchauen wir uns nun die verschiedenen Schritte im Detail an.\n\nCSV-Datei einlesen: Zuerst lesen wir die CSV-Datei meteodaten_saison.csv ein.\n\n\n# CSV-Datei einlesen\nmeteodaten &lt;- read.csv('Data/meteodaten_saison.csv',\n    sep = ',',\n    header = TRUE)\n\nDer Parameter sep = ',' gibt an, dass die Werte in der CSV-Datei durch Kommas getrennt sind.\nDer Parameter header = TRUE gibt an, dass die erste Zeile der CSV-Datei die Spaltennamen enthält.\n\nSubsets der Daten erstellen: Da unser Data Frame meteodaten Daten aus verschiedenen Jahreszeiten enthält, erstellen wir Subsets für jede Jahreszeit. Damit können wir die Daten für jede Jahreszeit separat plotten.\n\n\n# Subsets der Daten erstellen\nsaison_fruehling &lt;- meteodaten[meteodaten$Saison == \"Fruehling(MAM)\", ]\nsaison_sommer &lt;- meteodaten[meteodaten$Saison == \"Sommer(JJA)\", ]\nsaison_herbst &lt;- meteodaten[meteodaten$Saison == \"Herbst(SON)\", ]\nsaison_winter &lt;- meteodaten[meteodaten$Saison == \"Winter(DJF)\", ]\n\nMit dem == Operator vergleichen wir die Werte auf Gleichheit.\n\nmeteodaten$Saison == \"Fruehling(MAM)\"\n\nWir überprüfen also Zeile für Zeile, ob der Wert in der Spalte Saison gleich dem String \"Fruehling(MAM)\" ist. Dies wird in einen boolschen Vektor umgewandelt, der TRUE für Zeilen enthält, die dem Kriterium entsprechen, und FALSE für Zeilen, die es nicht tun.\nBetrachten wir nun eine Klammer weiter aussen, um zu verstehen, was genau wir dem Subset zuweisen.\n\nmeteodaten[meteodaten$Saison == \"Fruehling(MAM)\", ]\n\nHier wählen wir alle Zeilen aus dem Data Frame meteodaten aus, in denen die Spalte Saison den Wert \"Fruehling(MAM)\" hat. Da wir nach dem letzten Komma nichts weiter angeben, wählen wir alle Spalten aus.\nUnsere Saisonalen Data Frames enthalten also nicht nur die Temperatur-Mittelwerte aus Bern, sondern auch die Niederschlagswerte von Bern und dem Grossen St. Bernhard. Wir greifen im erstellten Plot nur auf die Temperaturwerte zu.\n\nPlot mit den Saisontemperaturen erstellen: Wir erstellen einen Plot mit den saisonalen Durchschnittstemperaturen in Bern.\n\n\n# Plot mit den Saisontemperaturen erstellen erstellen\nplot(saison_fruehling$Jahr, saison_fruehling$Bern_Mitteltemperatur,\n    type = \"l\",\n    xlab = \"Jahr\",\n    ylab = \"Durchschnittstemperatur in °C\",\n    main = \"Saisonale Durchschnittstemperaturen in Bern\",\n    xlim = c(1900, 2020),\n    ylim = c(-5, 35))\n\nDie plot() Funktion hat viele Parameter, die wir verwenden können, um den Plot anzupassen.\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\nplot(x, y)\nErstellt einen Plot der Werte in x gegen die Werte in y. Wir plotten hier die Spalte Jahr aus dem Subset saison_fruehling gegen die Spalte Bern_Mitteltemperatur aus dem gleichen subset.\n\n\ntype\nGibt den Typ des Plots an. Hier verwenden wir \"l\", um eine Linie zu zeichnen.\n\n\nxlab\nBeschriftung der x-Achse.\n\n\nylab\nBeschriftung der y-Achse.\n\n\nmain\nTitel des Plots.\n\n\nxlim\nBereich der x-Achse. Hier von 1900 bis 2020.\n\n\nylim\nBereich der y-Achse. Hier von -5 bis 35.\n\n\n\n\nWeitere Linien für andere Jahreszeiten auf den selben Plot hinzufügen: Wir fügen Linien für die anderen Jahreszeiten hinzu.\n\nIn R können wir mit der lines() Funktion weitere Linien zu einem bestehenden Plot hinzufügen.\n\n# Weitere Linien für andere Jahreszeiten auf den selben Plot hinzufügen\nlines(saison_sommer$Jahr, saison_sommer$Bern_Mitteltemperatur,\n    col = \"red\")\nlines(saison_herbst$Jahr, saison_herbst$Bern_Mitteltemperatur,\n    col = \"green\")\nlines(saison_winter$Jahr, saison_winter$Bern_Mitteltemperatur,\n    col = \"blue\")\n\nHier müssen wir jeweils nicht mehr ganz so viele Parameter angeben, da wir bereits die Achsenbesschriftung etc. vorgenommen haben. Was wir noch angeben müssen, ist die Farbe der Linie mit dem col Parameter.\n\nGestrichelte Horizontale Linie bei 0 hinzufügen: Wir fügen eine gestrichelte Horizontale Linie bei 0 hinzu.\n\n\n# Gestrichelte Horizontale Linie bei 0 hinzufügen\nabline(h = 0, lty = 2)\n\nMit der abline() Funktion können wir Linien zu einem Plot hinzufügen. Mit dem h Parameter geben wir die y-Position der Linie an, und mit dem lty Parameter geben wir den Linientyp an. Hier verwenden wir lty = 2, um eine gestrichelte Linie zu zeichnen.\n\nSaisonale Mittelwertlinien hinzufügen: Wir fügen Mittelwertlinien für jede Jahreszeit hinzu.\n\n\n# Saisonale Mittelwertlinien hinzufügen\nabline(h = mean(saison_fruehling$Bern_Mitteltemperatur),\n    col = \"black\",\n    lty = 3)\nabline(h = mean(saison_sommer$Bern_Mitteltemperatur),\n    col = \"red\",\n    lty = 3)\nabline(h = mean(saison_herbst$Bern_Mitteltemperatur),\n    col = \"green\",\n    lty = 3)\nabline(h = mean(saison_winter$Bern_Mitteltemperatur),\n    col = \"blue\",\n    lty = 3)\n\nHier fügen wir gestrichelte Linien für die Mittelwerte der Temperatur für jede Jahreszeit hinzu. Wir verwenden die mean() Funktion, um direkt im Aufruf den Mittelwert zu berechnen.\n\nLegende hinzufügen: Wir fügen eine Legende für die verschiedenen Linien hinzu.\n\n\n# Legende hinzufügen\nlegend(\"topright\",\n    legend = c(\"Frühling\", \"Sommer\", \"Herbst\", \"Winter\"),\n    col = c(\"black\", \"red\", \"green\", \"blue\"),\n    lty = 1,\n    xpd = TRUE)\n\nMit der legend() Funktion können wir eine Legende zu einem Plot hinzufügen. Wir geben die Position der Legende mit dem topright Parameter an. Mit dem legend Parameter geben wir die Beschriftungen für die Linien an. Mit dem col Parameter geben wir die Farben der Linien an. Mit dem lty Parameter geben wir den Linientyp an. Mit dem xpd Parameter geben wir an, ob die Legende ausserhalb des Plots sein soll.",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Einfache Plots erstellen</span>"
    ]
  },
  {
    "objectID": "plots_in_R.html#scatterplots-und-aggregate-funktion",
    "href": "plots_in_R.html#scatterplots-und-aggregate-funktion",
    "title": "3  Einfache Plots erstellen",
    "section": "4.2 Scatterplots und aggregate()-Funktion",
    "text": "4.2 Scatterplots und aggregate()-Funktion\nWenn wir von unseren nach Jahreszeiten sortierten Daten nun bspw. den Durchschnitt der Temperaturwerte pro Jahr berechnen wollen, können wir die aggregate() Funktion verwenden.\n\n# Durchschnittstemperatur pro Jahr berechnen\ndurchschnittstemperatur_pro_jahr &lt;- aggregate(Bern_Mitteltemperatur ~ Jahr,\n    data = meteodaten,\n    FUN = mean)\n\n# Plot erstellen\nplot(durchschnittstemperatur_pro_jahr$Jahr,\n    durchschnittstemperatur_pro_jahr$Bern_Mitteltemperatur,\n    type = \"p\",\n    xlab = \"Jahr\",\n    ylab = \"Durchschnittstemperatur in °C\",\n    main = \"Durchschnittstemperatur pro Jahr in Bern\")\n\n# Lineares Modell (lineare Regression) erstellen\ntrend &lt;- lm(Bern_Mitteltemperatur ~ Jahr,\n    data = durchschnittstemperatur_pro_jahr)\n\n# Trendlinie hinzufügen\nabline(trend,\n    col = \"red\",\n    lwd = 2)  # Die Farbe und Dicke der Linie anpassen\n\n\n\n\n\n\n\n\nDie aggregate() Funktion nimmt vier Parameter:\n\nDie Spalte, nach der aggregiert werden soll (Bern_Mitteltemperatur).\nDie Spalte, nach der gruppiert werden soll (Jahr).\nDie Daten, auf die die Funktion angewendet werden soll (meteodaten).\nDie Funktion, die auf die aggregierten Werte angewendet werden soll (mean).\n\nDer ~-Operator wird in R verwendet, um die linke Seite von der rechten Seite zu trennen. In diesem Fall bedeutet dies, dass wir die Spalte Bern_Mitteltemperatur nach der Spalte Jahr aggregieren wollen.\nZusätzlich haben wir hier noch eine Trendlinie hinzugefügt. Dazu haben wir ein lineares Modell mit der lm() Funktion erstellt und die Trendlinie mit der abline() Funktion hinzugefügt.",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Einfache Plots erstellen</span>"
    ]
  },
  {
    "objectID": "plots_in_R.html#boxplots-und-mehrere-plots-in-einem-diagramm",
    "href": "plots_in_R.html#boxplots-und-mehrere-plots-in-einem-diagramm",
    "title": "3  Einfache Plots erstellen",
    "section": "4.3 Boxplots und mehrere Plots in einem Diagramm",
    "text": "4.3 Boxplots und mehrere Plots in einem Diagramm\nWir wollen nun für die Temperaturen und Niederschlagswerte in Bern und auf dem Grossen St. Bernhard Boxplots erstellen und diese in einem Diagramm nebeneinander plotten. Zusätzlich wollen wir die Daten Zeitlich bis ins Jahr 1980 beschränken. Auch dieser Code enthält einige zusätzliche und neue Schritte, welche wir uns anschliessend genauer ansehen.\n\n# Definiere neue Kategorien-Namen und die gewünschte Reihenfolge\nneue_namen &lt;- c(\"Frühling\", \"Sommer\", \"Herbst\", \"Winter\")\nalte_namen &lt;- c(\"Fruehling(MAM)\", \"Sommer(JJA)\", \"Herbst(SON)\", \"Winter(DJF)\")\n\n# Konvertiere 'Saison' in einen Faktor mit den neuen Namen und der gewünschten Reihenfolge\nmeteodaten$Saison &lt;- factor(meteodaten$Saison,\n                            levels = alte_namen,\n                            labels = neue_namen)\n\n# Mehrere Plots in einem Diagramm\npar(mfrow = c(2, 2), # 2 Zeilen und 2 Spalten\n    mar = c(4, 4, 2, 1), # verkleinert die Ränder\n    oma = c(0, 0, 4, 0), # fügt Platz für den Titel hinzu\n    cex = 0.8) # verkleinert die Schriftgrösse\n\n# Boxplots erstellen\nboxplot(Bern_Mitteltemperatur ~ Saison,\n    data = meteodaten[meteodaten$Jahr &lt;= 1980, ],\n    ylim = c(-10, 20),\n    ylab = \"Durchschnittstemperatur in °C\",\n    main = \"Durchschnittstemperatur Bern\")\n\nboxplot(GrStBernhard_Mitteltemperatur ~ Saison,\n    data = meteodaten[meteodaten$Jahr &lt;= 1980, ],\n    ylim = c(-10, 20),\n    ylab = \"Durchschnittstemperatur in °C\",\n    main = \"Durchschnittstemperatur Gr. St. Bernhard\")\n\nboxplot(Bern_Niederschlagssumme ~ Saison,\n    data = meteodaten[meteodaten$Jahr &lt;= 1980, ],\n    ylim = c(0, 400),\n    ylab = \"Niederschlag in mm\",\n    main = \"Niederschlag Bern\")\n\nboxplot(GrStBernhard_Niederschlagssumme ~ Saison,\n    data = meteodaten[meteodaten$Jahr &lt;= 1980, ],\n    ylim = c(0, 400),\n    ylab = \"Niederschlag in mm\",\n    main = \"Niederschlag Gr. St. Bernhard\")\n\n# Gesamte Überschrift für alle Plots hinzufügen\ntitle(\"Klimadaten in Bern und auf dem Grossen St. Bernhard\", outer = TRUE)\n\n\n\n\n\n\n\n\nSchauen wir uns die verschiedenen Schritte im Detail an:\n\nDefiniere neue Kategorien-Namen und die gewünschte Reihenfolge: Die Kategorien-Namen haben bisher die Form Fruehling(MAM), Sommer(JJA), Herbst(SON), Winter(DJF). Um die Lesbarkeit zu verbessern und etwas kürzere Namen zu verwenden, definieren wir neue Namen. In einem nächsten Schritt konvertieren wir die Spalte Saison in einen Faktor1 mit den neuen Namen und der gewünschten Reihenfolge.\n\n\nneue_namen &lt;- c(\"Frühling\", \"Sommer\", \"Herbst\", \"Winter\")\nalte_namen &lt;- c(\"Fruehling(MAM)\",\n                \"Sommer(JJA)\",\n                \"Herbst(SON)\",\n                \"Winter(DJF)\")\n\nmeteodaten$Saison &lt;- factor(meteodaten$Saison,\n                            levels = alte_namen,\n                            labels = neue_namen)\n\nWir verwenden die factor() Funktion, um die Spalte Saison, die aktuell als Strings vorliegt, in einen Faktor zu konvertieren. Mit dem levels Parameter geben wir die Reihenfolge der vorhandenen Werte in den Daten an – hier die alten Namen, da diese in den Rohdaten stehen. Der labels Parameter definiert die neuen Namen, die im Plot oder bei Ausgaben angezeigt werden sollen.\nWichtig zu verstehen ist, dass die Umwandlung mit factor() nicht die zugrunde liegenden Daten ändert, sondern nur die Art und Weise, wie die Kategorien dargestellt werden. Die ursprünglichen Werte (also die alten Namen) bleiben im Data Frame erhalten2, aber R verwendet die neuen Labels, um diese Werte im Plot oder bei der Ausgabe anders zu präsentieren.\n\nMehrere Plots in einem Diagramm: Mit der par() Funktion können wir das Layout und die Platzierung der Plots anpassen.\n\n\nMit dem mfrow Parameter geben wir an, wie viele Zeilen und Spalten von Plots wir haben wollen. Hier haben wir 2 Zeilen und 2 Spalten.\nMit dem mar Parameter können wir die Ränder des Plots anpassen.\nMit dem oma Parameter können wir Platz für den Titel des gesamten Diagramms hinzufügen.\nMit dem cex Parameter können wir die Schriftgrösse anpassen.\n\n\npar(mfrow = c(2, 2), # 2 Zeilen und 2 Spalten\n    mar = c(4, 4, 2, 1), # verkleinert die Ränder\n    oma = c(0, 0, 4, 0), # fügt Platz für den Titel hinzu\n    cex = 0.8) # verkleinert die Schriftgrösse\n\n\nBoxplots erstellen: Wir erstellen Boxplots für die Durchschnittstemperaturen und Niederschlagssummen in Bern und auf dem Grossen St. Bernhard.\n\nZusätzlich beschränken wir die Daten auf die Jahre bis 1980.\n\nboxplot(Bern_Mitteltemperatur ~ Saison,\n    data = meteodaten[meteodaten$Jahr &lt;= 1980, ],\n    ylim = c(-10, 20),\n    ylab = \"Durchschnittstemperatur in °C\",\n    main = \"Durchschnittstemperatur Bern\")\n\nWir beachten auch hier wieder die Verwendung des ~-Operators, um die linke Seite von der rechten Seite zu trennen. In diesem Fall bedeutet dies, dass wir die Spalte Bern_Mitteltemperatur nach der Spalte Saison gruppieren wollen.\nWir verwenden die ylim Parameter, um die y-Achse auf einen bestimmten Bereich zu beschränken. Dies ist nützlich, um die Plots besser vergleichen zu können.\nDie Selelektion der Daten erflogt im data Parameter. Hier wählen wir nur die Daten bis ins Jahr 1980 aus.\n\nGesamte Überschrift für alle Plots hinzufügen: Wir fügen eine Überschrift für alle Plots hinzu.\n\n\ntitle(\"Klimadaten in Bern und auf dem Grossen St. Bernhard\", outer = TRUE)\n\nDer outer Parameter gibt an, dass die Überschrift über allen Plots platziert werden soll.",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Einfache Plots erstellen</span>"
    ]
  },
  {
    "objectID": "plots_in_R.html#grafiken-speichern",
    "href": "plots_in_R.html#grafiken-speichern",
    "title": "3  Einfache Plots erstellen",
    "section": "4.4 Grafiken speichern",
    "text": "4.4 Grafiken speichern\nGrafiken können entweder in RStudio unter dem Reiter “Export” … “Save Plot as PDF” oder “…Image” gespeichert werden.\nAlternativ können mit den Funktionen pdf(), jpeg() und png() Grafiken direkt in R gespeichert werden.\n\n# PDF-Datei erstellen\npdf(file = \"boxplots.pdf\",\n    width = 9,\n    height = 4.5) # Grösse des PDFs in Inch\n\n# Boxplot aus der vorherigen Sektion erstellen\nboxplot(Bern_Mitteltemperatur ~ Saison,\n        data = meteodaten[meteodaten$Jahr &lt;= 1980, ],\n        ylim = c(-10, 20),\n        ylab = \"Durchschnittstemperatur in °C\",\n        main = \"Durchschnittstemperatur Bern\")\n\n# PDF-Datei schliessen\ndev.off()\n\n\n\n\n\n\n\n\nParameter\nBeschreibung\n\n\n\n\nfile\nDateiname und Speicherort. Auch hier können absolute und relative Dateipfade verwendet werden. Wichtig ist die entsprechende Dateieindung (.pdf, .jpeg oder .png) anzugeben.\n\n\nwidth\nBreite des Plots in Inch.\n\n\nheight\nHöhe des Plots in Inch.\n\n\ndev.off()\nIst kein eigentlicher Parameter, aber ist am Ende jeder der Funktionen benötigt, um den Export zu beenden.",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Einfache Plots erstellen</span>"
    ]
  },
  {
    "objectID": "plots_in_R.html#übungen-3",
    "href": "plots_in_R.html#übungen-3",
    "title": "3  Einfache Plots erstellen",
    "section": "Übungen 3",
    "text": "Übungen 3\n\n3.1 Klimadiagramm\n\nLadet den Datensatz meteodaten_tag.csv nach dem Excel Export in R (ACHTUNG: NA-Werte sind sowohl mit’-’als auch mit’NA’)kodiert, deshalb: na.strings= c('-','NA'))\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n# CSV-Datei einlesen\nmeteodaten_tag &lt;- read.csv('Data/meteodaten_tag.csv',\n    sep = ',',\n    header = TRUE,\n    na.strings = c('-', 'NA'))\n\n\n\n\n\nMit str() ansehen, ob Daten korrekt (z.B.als numerisch) gelesen wurden.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nstr(meteodaten_tag)\n\n'data.frame':   4627 obs. of  7 variables:\n $ Jahr                : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ Monat               : int  1 1 1 1 1 1 1 1 1 1 ...\n $ Tag                 : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Wochentag           : chr  \"Sa\" \"So\" \"Mo\" \"Di\" ...\n $ Temperatur.C.       : num  3.6 4.8 4.6 6.2 8.1 6.9 3.4 2.4 5.5 5.1 ...\n $ Niederschlag.mm.Tag.: num  0 0 0 0 0.2 0 0 0 0.5 1.8 ...\n $ Bewoelkung.Achtel.  : int  3 7 3 3 3 3 6 6 6 7 ...\n\n\n\n\n\n\nErstellt ein Histogramm (hist()) mit den Tagestemperaturen mit feinen Abständen (breaks=40).\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nhist(meteodaten_tag$Temperatur.C.,\n     breaks = 40,\n     xlab = \"Temperatur in °C\",\n     ylab = \"Anzahl Tage\",\n     main = \"Histogramm der Tagestemperaturen in Bern\")\n\n\n\n\n\n\n\n\n\n\n\n\nWie sieht die Verteilung nach Augenmass aus?\nBerechnet die Monatsmittelwerte der Temperatur und der Bewölkung über alle Jahre (also Mittel über alle Jan, alle Feb,… wie in Klimadiagrammen). Achtung: Fehlwerte vorhanden!\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nmonatsmittel &lt;-\n    aggregate(cbind(Temperatur.C., Bewoelkung.Achtel.) ~ Monat,\n        data = meteodaten_tag,\n        FUN = mean,\n        na.rm = TRUE)\n\nHinweis zur Funktion cbind(): cbind() fügt die Spalten Bern_Temperatur und Bern_Bewoelkung zusammen, um sie dann nach Monat zu gruppieren. Alternativ könnten wir die beiden Mittelwerte auch seperat berechnen und anschliessend mit einem merge() zusammenfügen.\n\ntemp_mittel &lt;- aggregate(Temperatur.C. ~ Monat,\n    data = meteodaten_tag,\n    FUN = mean,\n    na.rm = TRUE)\nbewoelkung_mittel &lt;- aggregate(Bewoelkung.Achtel. ~ Monat,\n    data = meteodaten_tag,\n    FUN = mean,\n    na.rm = TRUE)\n\nmonatsmittel &lt;- merge(temp_mittel, bewoelkung_mittel,\n    by = \"Monat\")\n\n\n\n\n\nErstellt in eine Abbildung mit zwei Barplots der Ergebnisse übereinander(par(mfrow=c(2,1))). Was erwartet ihr?\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n# Mehrere Plots in einem Diagramm\npar(mfrow = c(2, 1))\n\n# Barplot für die Monatsmittel der Temperatur\nbarplot(temp_mittel$Temperatur.C.,\n    names.arg = temp_mittel$Monat,\n    xlab = \"Monat\",\n    ylab = \"Temp. in °C\",\n    main = \"Monatsmittel der Temperatur in Bern\")\n\n# Barplot für die Monatsmittel der Bewölkung\nbarplot(bewoelkung_mittel$Bewoelkung.Achtel.,\n    names.arg = bewoelkung_mittel$Monat,\n    xlab = \"Monat\",\n    ylab = \"Bewölkung in Achteln\",\n    main = \"Monatsmittel der Bewölkung in Bern\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2 Boxplots\n\nWählt den Zeitraum 200-2001 in den täglichen Daten.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n# Data Frame filtern und in einem neuen Objekt speichern\nmeteodaten_tag_2000_2001 &lt;- meteodaten_tag[meteodaten_tag$Jahr &gt;= 2000 & meteodaten_tag$Jahr &lt;= 2001, ]\n\n\n\n\n\nStellt die Temperaturen dieses Zeitrauemes als Funktion der Bewölkung in einem boxpolt()dar (je ein Boxplot pro Bewölkungsklasse). Beschriftet die Achsen und vergebt einen Titel\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nboxplot(Temperatur.C. ~ Bewoelkung.Achtel.,\n    data = meteodaten_tag_2000_2001,\n    ylab = \"Temperatur in °C\",\n    xlab = \"Bewölkung in Achteln\",\n    main = \"Temperaturen in Bern 2000-2001 nach Bewölkung\")\n\n\n\n\n\n\n\n\n\n\n\n\nUnter welchen Bewölkungsbedingungen ist die Spannweite und Varianz der Temperatur am grössten?\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n# Berechnung der Spannweite und Varianz der Temperatur für jede Bewölkungsklasse\nspannweite &lt;- aggregate(Temperatur.C. ~ Bewoelkung.Achtel.,\n    data = meteodaten_tag_2000_2001,\n    FUN = function(x) diff(range(x)))\n\nvarianz &lt;- aggregate(Temperatur.C. ~ Bewoelkung.Achtel.,\n    data = meteodaten_tag_2000_2001,\n    FUN = var)\n\n# Umbenennen der Spalten, um Verwechslungen zu vermeiden\nnames(spannweite)[2] &lt;- \"Spannweite\"\nnames(varianz)[2] &lt;- \"Varianz\"\n\n# Zusammenführen der Ergebnisse\nergebnisse &lt;- merge(spannweite, varianz, by = \"Bewoelkung.Achtel.\")\n\n# Ausgabe der Ergebnisse\nergebnisse\n\n  Bewoelkung.Achtel. Spannweite  Varianz\n1                  0       24.3 52.71585\n2                  1       29.0 60.76026\n3                  2       31.0 44.37131\n4                  3       33.5 54.83717\n5                  4       28.3 36.53915\n6                  5       24.5 34.10751\n7                  6       24.2 23.78013\n8                  7       21.7 29.81474\n9                  8       15.6 14.02061\n\n\n\n\n\n\nFindet heraus welcher Monat im Mittel der bewölkungsärmste und der -reichste ist (Im Mittel über die beiden Jahre). Wie viel Bewälkung gibt es im Mittel in diesen Monaten (in Achteln)?\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n# Berechnung der Monatsmittel der Bewölkung\nbewoelkung_mittel &lt;- aggregate(Bewoelkung.Achtel. ~ Monat,\n    data = meteodaten_tag_2000_2001,\n    FUN = mean,\n    na.rm = TRUE)\n\n# Bewölkungsärmster Monat\nbewoelkung_min &lt;- bewoelkung_mittel[which.min(bewoelkung_mittel$Bewoelkung.Achtel.), ]\nbewoelkung_min\n\n  Monat Bewoelkung.Achtel.\n8     8           1.854839\n\n# Bewölkungsreichster Monat\nbewoelkung_max &lt;- bewoelkung_mittel[which.max(bewoelkung_mittel$Bewoelkung.Achtel.), ]\nbewoelkung_max\n\n   Monat Bewoelkung.Achtel.\n12    12           5.016129",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Einfache Plots erstellen</span>"
    ]
  },
  {
    "objectID": "plots_in_R.html#footnotes",
    "href": "plots_in_R.html#footnotes",
    "title": "3  Einfache Plots erstellen",
    "section": "",
    "text": "Ein Faktor ist ein Datentyp in R, der kategorische Daten repräsentiert. Faktoren werden verwendet, um Daten zu kategorisieren und zu ordnen.↩︎\nWenn eine Spalte mit factor() bearbeitet wird, wird sie intern in diskrete Kategorien umgewandelt, jedoch ohne die ursprünglichen Daten zu überschreiben. Der levels Parameter bezieht sich auf die originalen Datenwerte, um sicherzustellen, dass R die Daten korrekt interpretiert. Die labels hingegen ändern nur, wie diese Daten für den Benutzer angezeigt werden. Dadurch bleibt der Inhalt des Data Frames unverändert, aber die Darstellung der Werte wird angepasst. Das ist nützlich, wenn man die Rohdaten beibehalten will, jedoch für Visualisierungen oder Präsentationen eine klarere oder kürzere Bezeichnung verwenden möchte.↩︎",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Einfache Plots erstellen</span>"
    ]
  },
  {
    "objectID": "packages_and_libraries.html",
    "href": "packages_and_libraries.html",
    "title": "4  Packages und Libraries",
    "section": "",
    "text": "5 Packages und Libraries\nIn R gibt es ähnlich wie in Python und anderen Programmiersprachen die Möglichkeit, zusätzliche Funktionalitäten durch das Einbinden von Packages und Libraries zu nutzen. In R werden diese durch den Befehl library() eingebunden.\nDie Pakete müssen einmalig installiert werden und können dann immer am Anfang eines Skripts oder Notebooks geladen werden.",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Packages und Libraries</span>"
    ]
  },
  {
    "objectID": "packages_and_libraries.html#pakete-installieren",
    "href": "packages_and_libraries.html#pakete-installieren",
    "title": "4  Packages und Libraries",
    "section": "5.1 Pakete installieren",
    "text": "5.1 Pakete installieren\nPakete können mit der Funktion install.packages() installiert werden. Zum Beispiel:\n\ninstall.packages(\"ggplot2\")\n\nUm Skripte und Notebooks portabel zu halten, ist es sinnvoll, die Installation von fehlenden Paketen am Anfang des Skripts oder Notebooks zu platzieren.\n\nif (!require(\"ggplot2\")) {\n  install.packages(\"ggplot2\")\n}",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Packages und Libraries</span>"
    ]
  },
  {
    "objectID": "packages_and_libraries.html#pakete-laden",
    "href": "packages_and_libraries.html#pakete-laden",
    "title": "4  Packages und Libraries",
    "section": "5.2 Pakete laden",
    "text": "5.2 Pakete laden\nPakete können mit der Funktion library() geladen werden. Zum Beispiel:\n\nlibrary(ggplot2)\n\nDie meisten Pakete haben eine Vielzahl von Funktionen, die genutzt werden können. Es ist ratsam, die Dokumentation des Pakets zu lesen, um die verfügbaren Funktionen und deren Anwendung zu verstehen. Die Dokumentation eines Pakets kann mit dem Befehl ? aufgerufen werden. Zum Beispiel:\n\n?ggplot2",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Packages und Libraries</span>"
    ]
  },
  {
    "objectID": "packages_and_libraries.html#häufig-genutzte-pakete",
    "href": "packages_and_libraries.html#häufig-genutzte-pakete",
    "title": "4  Packages und Libraries",
    "section": "5.3 Häufig genutzte Pakete",
    "text": "5.3 Häufig genutzte Pakete\nEinige der am häufigsten genutzten Pakete in R sind:\n\nggplot2: Ein Paket zur Erstellung von ansprechenden und aussagekräftigen Grafiken.\ndplyr: Ein Paket zur Datenmanipulation und -aggregation.\ntidyr: Ein Paket zur Datenbereinigung und -umformung.\nreadr: Ein Paket zur Einlesung von Daten aus verschiedenen Dateiformaten.\nstringr: Ein Paket zur Arbeit mit Zeichenketten.\nlubridate: Ein Paket zur Arbeit mit Datum und Uhrzeit.\ncaret: Ein Paket zur Erstellung von Modellen und zur Modellauswertung.\ntidyverse: Ein Paket, das eine Sammlung von Paketen für die Datenanalyse in R bereitstellt.\n\nEs gibt viele weitere Pakete, die für spezifische Anwendungen und Analysen entwickelt wurden. Es ist ratsam, die Dokumentation der Pakete zu lesen, um die verfügbaren Funktionen und deren Anwendung zu verstehen.",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Packages und Libraries</span>"
    ]
  },
  {
    "objectID": "packages_and_libraries.html#beispiel-einbinden-des-ggplot2-pakets",
    "href": "packages_and_libraries.html#beispiel-einbinden-des-ggplot2-pakets",
    "title": "4  Packages und Libraries",
    "section": "5.4 Beispiel: Einbinden des ggplot2 Pakets",
    "text": "5.4 Beispiel: Einbinden des ggplot2 Pakets\nAn einem einfachen Beispiel sehen wir, wie mit Hilfe der Pakete ggplot2 und plotly ein interaktives Diagramm erstellt werden kann.\n\n# Bibliotheken laden\nlibrary(ggplot2)\nlibrary(plotly)\n\n# Daten für den Plot vorbereiten\n# Konvertiere die Spalte Saison in einen einfacheren Faktor für die Darstellung\nmeteodaten$Saison &lt;- factor(meteodaten$Saison,\n                            levels = c(\"Fruehling(MAM)\",\n                                       \"Sommer(JJA)\",\n                                       \"Herbst(SON)\",\n                                       \"Winter(DJF)\"),\n                            labels = c(\"Frühling\",\n                                       \"Sommer\",\n                                       \"Herbst\",\n                                       \"Winter\"))\n\n# Plot mit ggplot2 erstellen\ngg &lt;- ggplot(meteodaten, aes(x = Jahr,\n        y = Bern_Mitteltemperatur,\n        color = Saison)) +\n    geom_line(linewidth = 1.2) +  # Linienbreite anpassen\n    geom_hline(yintercept = 0,\n        linetype = \"dashed\") +  # Horizontale Linie bei 0\n    labs(title = \"Saisonale Durchschnittstemperaturen in Bern\",\n        x = \"Jahr\",\n        y = \"Durchschnittstemperatur in °C\") +\n    theme_minimal() +  # Minimalistisches Theme für einen klaren Look\n    scale_color_manual(values = c(\"Frühling\" = \"green\",\n                                  \"Sommer\" = \"red\",\n                                  \"Herbst\" = \"orange\",\n                                  \"Winter\" = \"blue\")) +  # Farben anpassen\n    theme(plot.title = element_text(hjust = 0.5,\n        size = 16))  # Zentriere Titel und passe die Schriftgröße an\n\n# Plot interaktiv machen mit plotly\ngg_interaktiv &lt;- ggplotly(gg)\n\n# Interaktiver Plot anzeigen\ngg_interaktiv\n\n\n\nInteraktives Diagramm der saisonalen Durchschnittstemperaturen in Bern",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Packages und Libraries</span>"
    ]
  },
  {
    "objectID": "packages_and_libraries.html#übungen",
    "href": "packages_and_libraries.html#übungen",
    "title": "4  Packages und Libraries",
    "section": "Übungen",
    "text": "Übungen\n\n3.4 R als GIS Ersatz\n\nInstalliert das Paket maps und ladet es in R (z.B. library(maps)) Findet die x,y-Koordinaten von Bern und dem Gr. S. Bernhard heraus.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nlibrary(maps)\n\n# Koordinaten für Bern und Grosser St. Bernhard\nbern_coords &lt;- c(7.4474, 46.9481)\ngross_bernhard_coords &lt;- c(7.1761, 45.8689)\n\n\n\n\n\nVersucht eine Europakarte herzustellen und Bern und Gr. Bernhard als Punkte auf die Karte zu plotten und die Punkte mit Stationsnamen zu versehen\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\n# Erstelle eine Europakarte\nmap(\"world\",\n    xlim = c(-10, 20),\n    ylim = c(35, 55),\n    fill = TRUE,\n    col = \"lightgray\")\n\n\n# Punkte auf der Karte plotten\npoints(bern_coords[1],\n       bern_coords[2],\n       col = \"red\",\n       pch = 19,\n       cex = 1.5)\npoints(gross_bernhard_coords[1],\n       gross_bernhard_coords[2],\n       col = \"blue\",\n       pch = 19,\n       cex = 1.5)\n\n# Text hinzufügen\ntext(bern_coords[1],\n     bern_coords[2],\n     labels = \"Bern\",\n     pos = 3,\n     cex = 0.8,\n     col = \"red\")\ntext(gross_bernhard_coords[1],\n     gross_bernhard_coords[2],\n     labels = \"Gr. St. Bernhard\",\n     pos = 3,\n     cex = 0.8,\n     col = \"blue\")",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Packages und Libraries</span>"
    ]
  },
  {
    "objectID": "loops_and_functions.html",
    "href": "loops_and_functions.html",
    "title": "5  Schlaufen und Funktionen",
    "section": "",
    "text": "6 Schlaufen und Funktionen",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Schlaufen und Funktionen</span>"
    ]
  },
  {
    "objectID": "loops_and_functions.html#if-und-else",
    "href": "loops_and_functions.html#if-und-else",
    "title": "5  Schlaufen und Funktionen",
    "section": "6.1 if() und else()",
    "text": "6.1 if() und else()\nMit if() und else() können Bedingungen in R überprüft und entsprechende Aktionen ausgeführt werden. Die Syntax ist wie folgt:\n\nif (Bedingung) {\n  # Aktion, wenn die Bedingung TRUE ist\n} else {\n  # Aktion, wenn die Bedingung FALSE ist\n}\n\nZum Beispiel:\n\nx &lt;- 10\nif (x &gt; 5) {\n  print(paste(x, \"ist grösser als 5\"))\n} else {\n  print(\"x ist kleiner oder gleich 5\")\n}\n\n[1] \"10 ist grösser als 5\"\n\n\nHier verwenden wir zusätzlich die Funktion paste(), um Text und Variablen zu kombinieren. Dies ist ohne nicht direkt möglich.\n\n6.1.1 ifelse() als Vektoroperation\nifelse() ist eine Funktion, die eine Bedingung auf einen Vektor anwendet und basierend auf der Bedingung Werte zurückgibt. Die Syntax ist wie folgt:\n\nifelse(Bedingung, Wert_wenn_TRUE, Wert_wenn_FALSE)\n\nZum Beispiel:\n\nvectorA &lt;- c(1, 2, 3, 4, 5)\nifelse(vectorA == 3, \"Drei\", \"Nicht Drei\")\n\n[1] \"Nicht Drei\" \"Nicht Drei\" \"Drei\"       \"Nicht Drei\" \"Nicht Drei\"",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Schlaufen und Funktionen</span>"
    ]
  },
  {
    "objectID": "loops_and_functions.html#schleifen",
    "href": "loops_and_functions.html#schleifen",
    "title": "5  Schlaufen und Funktionen",
    "section": "6.2 Schleifen",
    "text": "6.2 Schleifen\nSchleifen sind nützlich, um eine bestimmte Aktion mehrmals auszuführen. In R gibt es verschiedene Arten von Schleifen, darunter for, while und repeat.\n\n6.2.1 for Schleife\nDie for Schleife wird verwendet, um eine Aktion für jedes Element in einer Sequenz auszuführen. Die Syntax ist wie folgt:\n\nfor (Element in Sequenz) {\n  # Aktion, die für jedes Element ausgeführt wird\n}\n\nZum Beispiel:\n\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nSchleifen mit Laufindex\nLaufindexe können in R auch ausserhalb der Schleife definiert werden. Zum Beispiel:\n\noriginal_vector &lt;- c(1, 2, 3, 4, 5)\nresult_vector &lt;- vector() # Leerer Vektor für das Ergebnis\n\n# Laufindex definieren\nj &lt;- 1\n\nfor (i in original_vector) {\n  result_vector[j] &lt;- i * 2\n  j &lt;- j + 1\n}\n\nresult_vector\n\n[1]  2  4  6  8 10\n\n\nBemerkung: aus Python kennen wir j++ oder j += 1 um den Laufindex zu erhöhen. In R gibt es keinen solchen Shortcut.",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Schlaufen und Funktionen</span>"
    ]
  },
  {
    "objectID": "loops_and_functions.html#eigene-funktionen",
    "href": "loops_and_functions.html#eigene-funktionen",
    "title": "5  Schlaufen und Funktionen",
    "section": "6.3 Eigene Funktionen",
    "text": "6.3 Eigene Funktionen\nFunktionen sind nützlich, um wiederkehrende Aktionen zu kapseln und zu abstrahieren. In R können eigene Funktionen mit dem function Schlüsselwort definiert werden. Die Syntax ist wie folgt:\n\nfunktion_name &lt;- function(Parameter1, Parameter2, ...) {\n  # Aktionen, die die Funktion ausführt\n  return(Ergebnis)\n}\n\nZum Beispiel:\nWir schreiben eine Funktion, die die Summe der Quadrate von zwei Zahlen berechnet.\n\nsumme_quadrate &lt;- function(x, y) {\n  summe &lt;- x^2 + y^2\n  return(summe)\n}\n\nDie Funktion kann dann wie folgt aufgerufen werden:\n\nsumme_quadrate(3, 4)\n\n[1] 25",
    "crumbs": [
      "Einführung R",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Schlaufen und Funktionen</span>"
    ]
  },
  {
    "objectID": "deskriptive-statistik.html",
    "href": "deskriptive-statistik.html",
    "title": "6  Deskriptive Statistik",
    "section": "",
    "text": "6.1 Grundgesamtheit vs. Stichprobe",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "deskriptive-statistik.html#grundgesamtheit-vs.-stichprobe",
    "href": "deskriptive-statistik.html#grundgesamtheit-vs.-stichprobe",
    "title": "6  Deskriptive Statistik",
    "section": "",
    "text": "Grundgesamtheit\nStichprobe\n\n\n\n\nBeispiele\nAlle Studierenden im Bachelorstudium\nDie 200 Studierenden, die an deiner Studie teilnehmen\n\n\n\nDie Wahlberechtigten einer Wahl\nDiejenigen, die bei einer Wahlumfrage befragt werden, um eine Prognose zu erstellen\n\n\n\nAlle Bücher in einer Bibliothek\nDie Bücher, die du aus dem Regal nimmst\n\n\nSchreibweise\n\n\n\n\nGenerell\nGriechische Buchstaben\nLateinische Buchstaben\n\n\nUmfang\n\\(N\\) (Gesamtanzahl der statistischen Einheiten in der Grundgesamtheit)\n\\(n\\) (Grösse der Stichprobe)\n\n\nMittelwert\n\\(\\mu\\)\n\\(\\bar{x}\\) oder \\(M\\)\n\n\nStandardabweichung\n\\(\\sigma\\)\n\\(s\\) oder \\(SD\\)",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "deskriptive-statistik.html#skalen",
    "href": "deskriptive-statistik.html#skalen",
    "title": "6  Deskriptive Statistik",
    "section": "6.2 Skalen",
    "text": "6.2 Skalen\n\n6.2.1 Kategoriale Variablen\n\n6.2.1.1 Nominalskala\n\n\n\n\n\n\nNominalskalen Beispiele\n\n\n\n\nGeschlecht\nAugenfarbe\nNationalität\n\n\nOperationen\n\nGleichheit: \\(x = y\\)\nUngleichheit: \\(x \\neq y\\)\nKategorien: \\(x \\in \\{A, B, C\\}\\)\n\n\n\n\nNominalskalen sind die einfachste Form der Skala und werden verwendet, um Kategorien zu unterscheiden. Die Kategorien haben keine natürliche Reihenfolge oder Rangfolge. Beispiele für Nominalskalen sind Geschlecht, Augenfarbe oder Nationalität.\n\n\n6.2.1.2 Ordinalskala\n\n\n\n\n\n\nOrdinalskalen Beispiele\n\n\n\n\nSchulnoten\nsozioökonomischer Status\nKundenzufriedenheit\n\n\nOperationen\n\nOrdnung: \\(x &lt; y\\)\nUngleichheit: \\(x \\neq y\\)\nGrösse: \\(x &gt; y\\)\nKategorien: \\(x \\in \\{A, B, C\\}\\)\n\n\n\n\nOrdinalskalen werden verwendet, um Kategorien zu unterscheiden, die eine natürliche Reihenfolge oder Rangfolge haben. Die Abstände zwischen den Kategorien sind jedoch nicht gleich. Beispiele für Ordinalskalen sind Schulnoten, sozioökonomischer Status oder Kundenzufriedenheit.\n\n\n\n6.2.2 Metrische Variablen\n\n6.2.2.1 Intervallskala\n\n\n\n\n\n\nIntervallskalen Beispiele\n\n\n\n\nTemperatur in Celsius\nIQ\nGeld\n\n\nOperationen\n\nGleichheit: \\(x = y\\)\nUngleichheit: \\(x \\neq y\\)\nGrösse: \\(x &gt; y\\)\nDifferenz: \\(x - y\\)\nKategorien: \\(x \\in \\{A, B, C\\}\\)\n\n\n\n\nIntervallskalen werden verwendet, um kontinuierliche Variablen zu messen, bei denen die Abstände zwischen den Werten gleich sind, aber kein absoluter Nullpunkt vorhanden ist. Beispiele für Intervallskalen sind Temperatur in Celsius oder IQ.\n\n\n6.2.2.2 Verhältnisskala\n\n\n\n\n\n\nVerhältnisskalen Beispiele\n\n\n\n\nGewicht\nGrösse\nEinkommen\n\n\nOperationen\n\nGleichheit: \\(x = y\\)\nUngleichheit: \\(x \\neq y\\)\nGrösse: \\(x &gt; y\\)\nDifferenz: \\(x - y\\)\nVerhältnis: \\(x / y\\)\nKategorien: \\(x \\in \\{A, B, C\\}\\)\n\n\n\n\nVerhältnisskalen werden verwendet, um kontinuierliche Variablen zu messen, bei denen die Abstände zwischen den Werten gleich sind und ein absoluter Nullpunkt vorhanden ist. Beispiele für Verhältnisskalen sind Gewicht, Grösse oder Einkommen.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "deskriptive-statistik.html#lageparameter-masse-der-zentraltendenz",
    "href": "deskriptive-statistik.html#lageparameter-masse-der-zentraltendenz",
    "title": "6  Deskriptive Statistik",
    "section": "6.3 Lageparameter / Masse der Zentraltendenz",
    "text": "6.3 Lageparameter / Masse der Zentraltendenz\n\n6.3.1 Modus \\(x_{mod}\\)\nDer Modus ist der Wert, der am häufigsten in einer Variablen vorkommt. Es ist möglich, dass eine Variable mehrere Modi hat (unimodal, bimodal, multimodal).\n\n\n6.3.2 Median \\(x_{med}\\)\nDer Median ist der Wert, der die Daten in zwei gleich grosse Teile teilt. Der Median ist robust gegenüber Ausreissern und wird verwendet, wenn die Daten nicht normalverteilt sind.\n\\[\n\\text{Median} = \\begin{cases}\n      x_{\\frac{n+1}{2}} & \\text{für ungerade Anzahl von Werten} \\\\\n      \\frac{1}{2} (x_{\\frac{n}{2}} + x_{\\frac{n}{2}+1}) & \\text{für gerade Anzahl von Werten}\n   \\end{cases}\n\\]\n\nRobustes Mass für die Lage von NICHT-symmetrisch verteilten Daten.\n\n\n\n6.3.3 Arithmetischer Mittelwert \\(\\bar{x}\\)\nDas arithmetische Mittel ist der Durchschnittswert einer Variablen und wird berechnet, indem alle Werte addiert und durch die Anzahl der Werte geteilt werden. Die Formel lautet:\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{x_1 + x_2 + \\ldots + x_n}{n}\n\\]\nWo \\(\\bar{x}\\) das arithmetische Mittel ist, \\(n\\) die Anzahl der Werte und \\(x_i\\) die einzelnen Werte.\n\nAussagekräftig bei symmetrisch verteilten Daten.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "deskriptive-statistik.html#streuungsparameter",
    "href": "deskriptive-statistik.html#streuungsparameter",
    "title": "6  Deskriptive Statistik",
    "section": "6.4 Streuungsparameter",
    "text": "6.4 Streuungsparameter\n\n6.4.1 Quantile \\(Q_{p}\\)\nDas \\(Q_{p}\\)-Quantil ist der Wert, unter dem \\(p\\%\\) der Daten liegen.\n\n6.4.1.1 Spezielle Quantile\n\nMedian ist \\(Q_{0.5}\\)\nQuartile: \\(Q_{0.25}\\), \\(Q_{0.5}\\), \\(Q_{0.75}\\)\nWhisker im Boxplot sind uneinheitlich definiert.\n\n\n\n\n6.4.2 Spannweite\nDie Spannweite ist die Differenz zwischen dem grössten und dem kleinsten Wert einer Variablen. Die Spannweite ist anfällig gegenüber Ausreissern.\n\\[\n\\text{Spannweite} = x_{\\text{max}} - x_{\\text{min}}\n\\]\n\n\n6.4.3 Varianz \\(s^2\\)\nMittle quadratische Abweichungen vom Mittelwert.\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\]\n\nSinnvoll bei metrischen Daten und wenn der Mittelwert ein geeignetes Mass für die Lage der Daten ist.\nStarker Einfluss von Ausreissern.\n\n\n\n6.4.4 Standardabweichung \\(s\\) oder \\(\\sigma\\)\nDie Standardabweichung ist die Quadratwurzel der Varianz und gibt an, wie stark die Werte einer Variablen um den Mittelwert streuen.\n\\[\ns = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2} = \\sqrt{s^2}\n\\]\n\nSinnvoll bei metrischen Daten und wenn der Mittelwert ein geeignetes Mass für die Lage der Daten ist.\nStarker Einfluss von Ausreissern.\nEinfache Interpretation: Standardabweichung = 10 bedeutet, dass die Werte im Durchschnitt 10 Einheiten um den Mittelwert streuen.\n\n\n\n6.4.5 Schiefe\nEinfaches Mass für die Asymmetrie der Verteilung.\n\\[\n\\text{Schiefe} = \\frac{\\text{arithm. Mittel} - \\text{Median}}{\\text{Standardabweichung}}\n\\]\n\nNegative Schiefe: linksschief, rechtssteil\nPositive Schiefe: rechtsschief, linkssteil\n\n\n\n\nSchiefe der Verteilung",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "deskriptive-statistik.html#kreuztabelle-kontingenztafel",
    "href": "deskriptive-statistik.html#kreuztabelle-kontingenztafel",
    "title": "6  Deskriptive Statistik",
    "section": "6.5 Kreuztabelle / Kontingenztafel",
    "text": "6.5 Kreuztabelle / Kontingenztafel\n\nFür nominale Daten\nOrdinale und metrische Daten können in nominale Daten transformiert werden (z.b. Grenzüberschreitung ja/nein)\n\nBeispiel:\nEs werden 2000 Personen darüber befragt, ob sie Produkt A oder B bevorzugen. Das Ergebnis wird nach Geschlecht des Befragten ausgewertet.\n\n\n\nProdukt / Geschlecht\nMännlich\nWeiblich\nSumme\n\n\n\n\nA\n660\n440\n1100\n\n\nB\n340\n560\n900\n\n\nSumme\n1000\n1000\n2000\n\n\n\n\n6.5.1 Freiheitsgrade\n\nAnzahl Beobachtungen abzüglich Anzahl geschätzter Parameter.\nBeispiel: Standardabweichung aus Stichprobe mit \\(n\\) Beobachtungen\n\n\\[\ns = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n\\]\n\nDer Mittelwert wurde bereits aus den Beobachtungen geschätzt. Wenn man den Mittwelwert und alle Beobachtungen ausser der letzten kennt (\\(n-1\\)) dann kann man diese berechnen, es besteht also keine “Freiheit” mehr.\n\n\n# Population erstellen\npopulation &lt;- rnorm(100000, mean = 0, sd = 10)\npopulation_sd &lt;- sqrt(sum((population - mean(population))^2) / length(population))\n\n# Viele Stichproben ziehen, aber mit kleinerer Stichprobengröße\nn_samples &lt;- 1000\nsample_size &lt;- 10  # kleinere Stichprobengröße\nmean_sd_n &lt;- 0\nmean_sd_n_minus_1 &lt;- 0\n\nfor(i in 1:n_samples) {\n    sample_data &lt;- sample(population, sample_size)\n    mean_sd_n &lt;- mean_sd_n + sqrt(sum((sample_data - mean(sample_data))^2) / sample_size)\n    mean_sd_n_minus_1 &lt;- mean_sd_n_minus_1 + sqrt(sum((sample_data - mean(sample_data))^2) / (sample_size-1))\n}\n\nmean_sd_n &lt;- mean_sd_n / n_samples\nmean_sd_n_minus_1 &lt;- mean_sd_n_minus_1 / n_samples\n\n# Prozentuale Abweichungen berechnen\nbias_n &lt;- (mean_sd_n - population_sd) / population_sd * 100\nbias_n_minus_1 &lt;- (mean_sd_n_minus_1 - population_sd) / population_sd * 100\n\n\nWahre Standardabweichung der Population: 9.99\nDurchschnittliche Schätzung mit n: 9.27 (Abweichung: -7.23%)\nDurchschnittliche Schätzung mit n-1: 9.77 (Abweichung: -2.22%)\n\nDer Effekt der Freiheitsgrade ist besonders bei kleinen Stichproben bedeutsam. Bei einer Stichprobengröße von n=10 führt die Berechnung mit n zu einer systematischen Unterschätzung von etwa 7%, während die Korrektur mit n-1 die Unterschätzung auf etwa 2% reduziert. Bei größeren Stichproben wird dieser Unterschied kleiner, da ein einzelner Freiheitsgrad weniger ins Gewicht fällt (bei n=100 macht ein Freiheitsgrad nur noch 1% aus, bei n=10 sind es 10%).",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Deskriptive Statistik</span>"
    ]
  },
  {
    "objectID": "verteilungen.html",
    "href": "verteilungen.html",
    "title": "7  Verteilungen",
    "section": "",
    "text": "7.1 Theoretische Verteilungen diskreter Zufallsvariablen",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "verteilungen.html#theoretische-verteilungen-diskreter-zufallsvariablen",
    "href": "verteilungen.html#theoretische-verteilungen-diskreter-zufallsvariablen",
    "title": "7  Verteilungen",
    "section": "",
    "text": "7.1.1 Diskrete Gleichverteilung\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\n\n# Funktion zum Berechnen der Wahrscheinlichkeits- und Verteilungsfunktionen\nsimulate_dice &lt;- function(n_dice) {\n  rolls &lt;- replicate(1000000, sum(sample(1:6, n_dice, replace = TRUE)))  # Simulation von n_dice Würfeln\n  df &lt;- as.data.frame(table(rolls) / length(rolls)) %&gt;%\n    rename(x = rolls, probability = Freq) %&gt;%\n    mutate(x = as.numeric(as.character(x))) %&gt;%\n    arrange(x) %&gt;%\n    mutate(cumulative_probability = cumsum(probability))\n\n  # Zusätzliche Punkte für 0 und 1\n  df &lt;- rbind(data.frame(x = 0, probability = 0, cumulative_probability = 0),\n              df,\n              data.frame(x = max(df$x) + 1, probability = 0, cumulative_probability = 1))\n  return(df)\n}\n\n# Daten für 1 Würfel\ndf_1dice &lt;- simulate_dice(1)\n\n# Plot für die Wahrscheinlichkeitsfunktion (PDF) von 1 Würfel\nplot_pdf_1 &lt;- ggplot(df_1dice, aes(x = x, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\", width = 0.7) +\n  labs(title = \"ein Würfel (PDF)\", x = \"x\", y = expression(f(x))) +\n  theme_minimal() +\n  ylim(0, max(df_1dice$probability, na.rm = TRUE) * 1.1) +\n  xlim(0, 7) +\n  scale_y_continuous(breaks = seq(0, 1/6, by = 1/6),\n                    labels = c(\"0\", \"1/6\"))\n\n# Plot für die kumulative Verteilungsfunktion (CDF) von 1 Würfel\nplot_cdf_1 &lt;- ggplot(df_1dice, aes(x = x, y = cumulative_probability)) +\n  geom_step(linewidth = 0.8, color = \"black\") +\n  labs(title = \"ein Würfel (CDF)\", x = \"x\", y = expression(F(x))) +\n  theme_minimal() +\n  ylim(0, 1) +\n  xlim(0, 7) +\n  scale_y_continuous(breaks = seq(0, 1, by = 1/6),\n                    labels = c(\"0\", \"1/6\", \"2/6\", \"3/6\", \"4/6\", \"5/6\", \"6/6\"))\n\n# Anordnung der beiden Plots in einem 2x1-Layout\ngrid.arrange(plot_pdf_1, plot_cdf_1, ncol = 2)\n\n\n\n\n\n\n\n\nFigure 7.1: Diskrete Wahrscheinlichkeits- und Verteilungsfunktionen für einen Würfel (Berechnet mit 1’000’000 Simulationen)\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\n\n# Funktion zum Berechnen der Wahrscheinlichkeits- und Verteilungsfunktionen\nsimulate_dice &lt;- function(n_dice) {\n  rolls &lt;- replicate(10000, sum(sample(1:6, n_dice, replace = TRUE)))  # Simulation von n_dice Würfeln\n  df &lt;- as.data.frame(table(rolls) / length(rolls)) %&gt;%\n    rename(x = rolls, probability = Freq) %&gt;%\n    mutate(x = as.numeric(as.character(x))) %&gt;%\n    arrange(x) %&gt;%\n    mutate(cumulative_probability = cumsum(probability))\n  return(df)\n}\n\n# Daten für 2, 3 und 4 Würfel\ndf_2dice &lt;- simulate_dice(2)\ndf_3dice &lt;- simulate_dice(3)\ndf_4dice &lt;- simulate_dice(4)\n\n# Plots für die PDF und CDF von 2 Würfeln\nplot_pdf_2 &lt;- ggplot(df_2dice, aes(x = x, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\", width = 0.7) +\n  labs(title = \"zwei Würfel (PDF)\", x = \"x\", y = expression(f(x))) +\n  theme_minimal() +\n  ylim(0, max(df_2dice$probability, na.rm = TRUE) * 1.1)\n\nplot_cdf_2 &lt;- ggplot(df_2dice, aes(x = x, y = cumulative_probability)) +\n  geom_step(linewidth = 0.8, color = \"black\") +\n  labs(title = \"zwei Würfel (CDF)\", x = \"x\", y = expression(F(x))) +\n  theme_minimal() +\n  ylim(0, 1)\n\n# Plots für die PDF und CDF von 3 Würfeln\nplot_pdf_3 &lt;- ggplot(df_3dice, aes(x = x, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\", width = 0.7) +\n  labs(title = \"drei Würfel (PDF)\", x = \"x\", y = expression(f(x))) +\n  theme_minimal() +\n  ylim(0, max(df_3dice$probability, na.rm = TRUE) * 1.1)\n\nplot_cdf_3 &lt;- ggplot(df_3dice, aes(x = x, y = cumulative_probability)) +\n  geom_step(linewidth = 0.8, color = \"black\") +\n  labs(title = \"drei Würfel (CDF)\", x = \"x\", y = expression(F(x))) +\n  theme_minimal() +\n  ylim(0, 1)\n\n# Plots für die PDF und CDF von 4 Würfeln\nplot_pdf_4 &lt;- ggplot(df_4dice, aes(x = x, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\", width = 0.7) +\n  labs(title = \"vier Würfel (PDF)\", x = \"x\", y = expression(f(x))) +\n  theme_minimal() +\n  ylim(0, max(df_4dice$probability, na.rm = TRUE) * 1.1)\n\nplot_cdf_4 &lt;- ggplot(df_4dice, aes(x = x, y = cumulative_probability)) +\n  geom_step(linewidth = 0.8, color = \"black\") +\n  labs(title = \"vier Würfel (CDF)\", x = \"x\", y = expression(F(x))) +\n  theme_minimal() +\n  ylim(0, 1)\n\n# Anordnung der Plots in einem 3x2-Layout\ngrid.arrange(plot_pdf_2, plot_pdf_3, plot_pdf_4, plot_cdf_2, plot_cdf_3, plot_cdf_4, ncol = 3, nrow = 2)\n\n\n\n\n\n\n\n\nFigure 7.2: Diskrete Wahrscheinlichkeits- und Verteilungsfunktionen für 2, 3 und 4 Würfel (Berechnet mit 100’000 Simulationen)\n\n\n\n\n\nMit mehr Würfeln kommt das immer näher an eine Normalverteilung\n\n\nCode\n# Lade benötigte Pakete\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\n\n# Parameter definieren\nn_dice &lt;- 15       # Anzahl Würfel\nn_simulations &lt;- 100000  # Anzahl Simulationen\n\n# Dynamische xlim basierend auf minimaler/maximaler Würfelsumme\nx_min &lt;- n_dice * 1      # Minimale Summe (alle Würfel = 1)\nx_max &lt;- n_dice * 6      # Maximale Summe (alle Würfel = 6)\n\n# Funktion zur Simulation der Würfelsummen und Berechnung der Wahrscheinlichkeiten\nsimulate_dice &lt;- function(n_dice, n_simulations) {\n  \n  # Simulation: Summe von n_dice Würfeln pro Durchlauf\n  rolls &lt;- replicate(n_simulations, sum(sample(1:6, n_dice, replace = TRUE)))\n  \n  # Häufigkeitstabelle mit Wahrscheinlichkeiten und kumulativen Wahrscheinlichkeiten\n  df &lt;- as.data.frame(table(rolls)) %&gt;%\n    rename(x = rolls, probability = Freq) %&gt;%\n    mutate(\n      x = as.numeric(as.character(x)),         # x als numerisch\n      probability = probability / sum(probability),   # Wahrscheinlichkeiten normieren\n      cumulative_probability = cumsum(probability)     # Kumulative Verteilungsfunktion\n    ) %&gt;%\n    arrange(x)\n  \n  return(df)\n}\n\n# Simulation durchführen\ndf_results &lt;- simulate_dice(n_dice, n_simulations)\n\n# Wahrscheinlichkeitsfunktion (PDF) plotten\nplot_pdf &lt;- ggplot(df_results, aes(x = x, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\", width = 0.7) +\n  labs(\n    title = paste(n_dice, \"Würfel (PDF)\"),\n    x = \"Summe der Würfel\",\n    y = expression(f(x))\n  ) +\n  theme_minimal() +\n  # x-Achse dynamisch setzen\n  xlim(c(x_min, x_max))\n\n# Kumulative Verteilungsfunktion (CDF) plotten\nplot_cdf &lt;- ggplot(df_results, aes(x = x, y = cumulative_probability)) +\n  geom_step(linewidth = 0.8, color = \"black\") +\n  labs(\n    title = paste(n_dice, \"Würfel (CDF)\"),\n    x = \"Summe der Würfel\",\n    y = expression(F(x))\n  ) +\n  theme_minimal() +\n  ylim(0, 1) +\n  # x-Achse dynamisch setzen\n  xlim(c(x_min, x_max))\n\n# Beide Plots nebeneinander ausgeben\ngrid.arrange(plot_pdf, plot_cdf, ncol = 2)\n\n\n\n\n\n\n\n\nFigure 7.3: Diskrete Wahrscheinlichkeits- und Verteilungsfunktionen für 15 Würfel (Berechnet mit 100’000 Simulationen)\n\n\n\n\n\n\n\n\n\n\n\nWas passiert bei extrem vielen Würfeln?\n\n\n\n\n\nSpannend ist dass die Streuung der Summen extrem klein wird.\n\n\nCode\n# Lade benötigte Pakete\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(gridExtra)\n\n# Parameter definieren\nn_dice &lt;- 1000       # Anzahl Würfel\nn_simulations &lt;- 100000  # Anzahl Simulationen\n\n# Dynamische xlim basierend auf minimaler/maximaler Würfelsumme\nx_min &lt;- n_dice * 1      # Minimale Summe (alle Würfel = 1)\nx_max &lt;- n_dice * 6      # Maximale Summe (alle Würfel = 6)\n\n# Funktion zur Simulation der Würfelsummen und Berechnung der Wahrscheinlichkeiten\nsimulate_dice &lt;- function(n_dice, n_simulations) {\n  \n  # Simulation: Summe von n_dice Würfeln pro Durchlauf\n  rolls &lt;- replicate(n_simulations, sum(sample(1:6, n_dice, replace = TRUE)))\n  \n  # Häufigkeitstabelle mit Wahrscheinlichkeiten und kumulativen Wahrscheinlichkeiten\n  df &lt;- as.data.frame(table(rolls)) %&gt;%\n    rename(x = rolls, probability = Freq) %&gt;%\n    mutate(\n      x = as.numeric(as.character(x)),         # x als numerisch\n      probability = probability / sum(probability),   # Wahrscheinlichkeiten normieren\n      cumulative_probability = cumsum(probability)     # Kumulative Verteilungsfunktion\n    ) %&gt;%\n    arrange(x)\n  \n  return(df)\n}\n\n# Simulation durchführen\ndf_results &lt;- simulate_dice(n_dice, n_simulations)\n\n# Wahrscheinlichkeitsfunktion (PDF) plotten\nplot_pdf &lt;- ggplot(df_results, aes(x = x, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\", width = 0.7) +\n  labs(\n    title = paste(n_dice, \"Würfel (PDF)\"),\n    x = \"Summe der Würfel\",\n    y = expression(f(x))\n  ) +\n  theme_minimal() +\n  # x-Achse dynamisch setzen\n  xlim(c(x_min, x_max))\n\n# Kumulative Verteilungsfunktion (CDF) plotten\nplot_cdf &lt;- ggplot(df_results, aes(x = x, y = cumulative_probability)) +\n  geom_step(linewidth = 0.8, color = \"black\") +\n  labs(\n    title = paste(n_dice, \"Würfel (CDF)\"),\n    x = \"Summe der Würfel\",\n    y = expression(F(x))\n  ) +\n  theme_minimal() +\n  ylim(0, 1) +\n  # x-Achse dynamisch setzen\n  xlim(c(x_min, x_max))\n\n# Beide Plots nebeneinander ausgeben\ngrid.arrange(plot_pdf, plot_cdf, ncol = 2)\n\n\n\n\n\n\n\n\nFigure 7.4: Diskrete Wahrscheinlichkeits- und Verteilungsfunktionen für 1000 Würfel (Berechnet mit 100’000 Simulationen)\n\n\n\n\n\n\n\n\n\n\n7.1.2 Normalverteilung\n\n\n\n\n\n\nZentraler Grenzwertsatz\n\n\n\n\n\n\\[\n\\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{i=1}^{n} X_i = \\mu\n\\]\nDer Zentrale Grenzwertsatz besagt, dass die Summe von unabhängigen, identisch verteilten Zufallsvariablen mit wachsendem \\(n\\) gegen eine Normalverteilung konvergiert.\n\n\n\n\\[\nf_{\\mu, \\sigma}(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\\]\n\n\nCode\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n# Erzeugung der Normalverteilung\nx &lt;- seq(-4, 4, length.out = 1000)\ny_pdf &lt;- dnorm(x, mean = 0, sd = 1)\ny_cdf &lt;- pnorm(x, mean = 0, sd = 1)\n\n# Datensätze erstellen\ndf_pdf &lt;- data.frame(x = x, y = y_pdf)\ndf_cdf &lt;- data.frame(x = x, y = y_cdf)\n\n# PDF Plot\nplot_pdf &lt;- ggplot(df_pdf, aes(x = x, y = y)) +\n  geom_ribbon(data = subset(df_pdf, x &gt;= -3 & x &lt;= 3),\n             aes(ymin = 0, ymax = y, fill = \"99.73%\")) +\n  geom_ribbon(data = subset(df_pdf, x &gt;= -2 & x &lt;= 2),\n             aes(ymin = 0, ymax = y, fill = \"95.45%\")) +\n  geom_ribbon(data = subset(df_pdf, x &gt;= -1 & x &lt;= 1),\n             aes(ymin = 0, ymax = y, fill = \"68.27%\")) +\n  geom_line(linewidth = 1) +\n  geom_vline(xintercept = c(-3, -2, -1, 1, 2, 3), \n            linetype = \"dashed\", color = \"gray40\") +\n  labs(x = \"Standardabweichungen (σ)\", \n       y = \"f(x)\",\n       title = \"Dichtefunktion\") +\n  scale_x_continuous(breaks = -3:3,\n                    labels = paste0(c(\"-3\", \"-2\", \"-1\", \"0\", \"1\", \"2\", \"3\"), \"σ\")) +\n  scale_fill_manual(values = c(\n    \"68.27%\" = \"#2C7BB6\",\n    \"95.45%\" = \"#81B9D9\",\n    \"99.73%\" = \"#D1E5F0\"\n  )) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    legend.position = \"bottom\",\n    legend.title = element_blank()\n  )\n\n# CDF Plot\nplot_cdf &lt;- ggplot(df_cdf, aes(x = x, y = y)) +\n  geom_line(linewidth = 1) +\n  geom_vline(xintercept = c(-3, -2, -1, 1, 2, 3), \n            linetype = \"dashed\", color = \"gray40\") +\n  labs(x = \"Standardabweichungen (σ)\", \n       y = \"F(x)\",\n       title = \"Verteilungsfunktion\") +\n  scale_x_continuous(breaks = -3:3,\n                    labels = paste0(c(\"-3\", \"-2\", \"-1\", \"0\", \"1\", \"2\", \"3\"), \"σ\")) +\n  scale_y_continuous(breaks = seq(0, 1, 0.2)) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank()\n  )\n\n# Plots nebeneinander anordnen\ngrid.arrange(plot_pdf, plot_cdf, ncol = 2)\n\n\n\n\n\n\n\n\nFigure 7.5: Links: Wahrscheinlichkeitsdichte (PDF), Rechts: Verteilungsfunktion (CDF) der Normalverteilung\n\n\n\n\n\n\nSymmetrisch zur Achse \\(x = \\mu\\)\nunimodal mit Maximum bei \\(x = \\mu\\)\nWendepunkte bei \\(x = \\mu \\pm \\sigma\\)\nasymptotisch gegen 0\n\n\n7.1.2.1 Standardisierung\nDurch eine Transformation zu \\(\\mu = 0\\) und \\(\\sigma = 1\\) erhält man eine standardisierte Zufallsvariable.\n\\[\nz = \\frac{X - \\mu}{\\sigma}\n\\]\nWobei \\(z\\) die standardisierte Zufallsvariable, \\(X\\) die ursprüngliche Zufallsvariable und \\(\\mu\\) und \\(\\sigma\\) der Mittelwert und die Standardabweichung der ursprünglichen Verteilung sind.\nDie Standardisierung wird verwendet, um verschiedene Elemente (d.h. Daten mit Bias oder unterschiedlichen Einheiten oder unterschiedlicher Varianz, etc.) zu vergleichen.\n\nBeispiel\nAngenommen, wir haben eine Stichprobe von 5 Studierenden und ihre Prüfungsnoten (Skala 0–100):\n\n\n\nTable 7.1\n\n\n\n\n\nStudent\nNote \\(x\\)\n\n\n\n\nA\n70\n\n\nB\n80\n\n\nC\n50\n\n\nD\n90\n\n\nE\n60\n\n\n\n\n\n\n\nSchritt 1: Berechnung des Mittelwerts\n\\[\n\\mu = \\frac{70 + 80 + 50 + 90 + 60}{5} = \\frac{350}{5} = 70\n\\tag{7.1}\\]\n\n\nSchritt 2: Berechnung der Standardabweichung\nDie Standardabweichung ist definiert als:\n\\[\n\\sigma = \\sqrt{\\frac{\\sum (x_i - \\mu)^2}{n}}\n\\]\nEinsetzen der Werte:\n\\[\n\\begin{aligned}\n\\sigma &= \\sqrt{\\frac{(70-70)^2 + (80-70)^2 + (50-70)^2 + (90-70)^2 + (60-70)^2}{5}} \\\\\n&= \\sqrt{\\frac{0 + 100 + 400 + 400 + 100}{5}} \\\\\n&= \\sqrt{\\frac{1000}{5}} \\\\\n&= \\sqrt{200} \\\\\n&\\approx 15.81\n\\end{aligned}\n\\tag{7.2}\\]\n\n\nSchritt 3: Berechnung der Z-Werte\nNun berechnen wir für jede Note den z-Wert:\n\\[\nz = \\frac{x - 70}{15.81}\n\\]\n\n\n\nStudent\nNote \\(x\\)\n\\(z = \\frac{x - 70}{14.14}\\)\n\n\n\n\nA\n70\n\\(\\frac{70-70}{15.81} = 0\\)\n\n\nB\n80\n\\(\\frac{80-70}{15.81} = 0.63\\)\n\n\nC\n50\n\\(\\frac{50-70}{15.81} = -1.27\\)\n\n\nD\n90\n\\(\\frac{90-70}{15.81} = 1.27\\)\n\n\nE\n60\n\\(\\frac{60-70}{15.81} = -0.63\\)",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Verteilungen</span>"
    ]
  },
  {
    "objectID": "statistische-tests.html",
    "href": "statistische-tests.html",
    "title": "8  Statistische Tests",
    "section": "",
    "text": "8.1 Standardabweichung vs. Standardfehler\nStatistische Tests sind essenziell, um Hypothesen über Daten zu überprüfen.\nDer Standardfehler ist ein Mass für die Genauigkeit eines Schätzers. Er ist definiert als die Standardabweichung der Schätzfunktion.\n\\[\ns_{\\bar{x}} = \\sqrt{\\frac{s^2}{n}}\n\\]\nWo \\(s_{\\bar{x}}\\) der Standardfehler des Mittelwerts ist, \\(s^2\\) die Varianz der Stichprobe und \\(n\\) die Anzahl der Beobachtungen.\nD.h. der Standardfehler ist gross, wenn die Varianz gross ist und/oder die Stichprobe klein ist.\nDie Standardfehler sind dank dem zentralen Grenzwertsatz normalverteilt.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistische Tests</span>"
    ]
  },
  {
    "objectID": "statistische-tests.html#standardabweichung-vs.-standardfehler",
    "href": "statistische-tests.html#standardabweichung-vs.-standardfehler",
    "title": "8  Statistische Tests",
    "section": "",
    "text": "Beispiel\n\n\n\nBeim Start zum Engadiner Skimarathon wird ein Bus vermisst. Bei der Suche findest du einen Parkplatz einen Bus. Du schaust in den Bus und stellt fest, dass das durchschnittliche Alter der Personen vermutlich bei ca. 80 Jahren liegt.\nDie Teilnehmer am Skimarathon haben ein mittleres Alter von 40 Jahren mit einer Standardabweichung von 10 Jahren, wobei wir annehmen, dass die Alter ungefähr normalverteilt ist.\nDer Standardfehler misst nun die Genauigkeit des Schätzers, also wie genau der Mittelwert der Stichprobe den Mittelwert der Population schätzt.\nIm gefundenen Bus befinden sich 50 Personen mit einem durchschnittlichen Alter von 80 Jahren.\nDer Standardfehler des Mittelwerts beträgt:\n\\[\ns_{\\bar{x}} = \\sqrt{\\frac{s^2}{n}} = \\sqrt{\\frac{10^2}{50}} = \\sqrt{2} \\approx 1.41 \\text{ Jahre}\n\\]\nDie Differenz zwischen dem Mittelwert der Stichprobe und dem Mittelwert der Population beträgt 40 Jahre, und ist damit grösser als 28 Standardfehler.\nAus der Normalverteilung können wir also schliessen, dass der Bus mit 99.9% Wahrscheinlichkeit nicht die Teilnehmer des Skimarathons enthält.\n\n\n\n8.1.1 Hypothesen\nEine Hypothese ist eine testbare Aussage über eine Population. In der Statistik gibt es zwei Hauptarten von Hypothesen:\n\n8.1.1.1 Nullhypothese \\(H_0\\)\nDie Nullhypothese postuliert, dass es keinen Effekt oder Unterschied gibt. Zum Beispiel könnte H₀ aussagen, dass es keinen Unterschied zwischen den Mittelwerten zweier Gruppen gibt.\n\n\n8.1.1.2 Alternativhypothese \\(H_A\\) oder \\(H_1\\)\nDie Alternativhypothese widerspricht der Nullhypothese und postuliert, dass es einen Effekt oder Unterschied gibt. Alternativhypothesen können einseitig (z.B. \\(H_1: \\mu &gt; \\mu_0\\)) oder zweiseitig (z.B. \\(H_1: \\mu \\neq \\mu_0\\)) sein.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistische Tests</span>"
    ]
  },
  {
    "objectID": "statistische-tests.html#testtheorie",
    "href": "statistische-tests.html#testtheorie",
    "title": "8  Statistische Tests",
    "section": "8.2 Testtheorie",
    "text": "8.2 Testtheorie\n\nHypothesen aufstellen\n\nFormuliere eine Nullhypothese \\(H_0\\) (z.B. „kein Unterschied zwischen Mittelwerten“) und eine Alternativhypothese \\(H_1\\) (z.B. „es gibt einen Unterschied“).\n\nSignifikanzniveau \\(\\alpha\\) festlegen\n\nHäufig \\(\\alpha = 0.05\\). Wenn dein p-Wert kleiner ist als 0.05, lehnst du \\(H_0\\) ab (auf 5%-Niveau).\n\nStichprobe erheben\n\nDaten sammeln (z.B. Zufallsstichprobe) und Kennwerte (Mittelwert, Varianz, etc.) berechnen.\n\nTeststatistik berechnen\n\nBeim t-Test rechnest du einen t-Wert (Teststatistik) aus. Dieser t-Wert sagt dir, wie viele „Standardfehler“ deine gemessene Differenz vom erwarteten Wert (unter \\(H_0\\)) entfernt ist.\n\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{s_{\\bar{x}}}\n\\]\n\n\\(\\bar{x}\\): Mittelwert deiner Stichprobe\n\n\\(\\mu_0\\): unter \\(H_0\\) vermuteter Populationsmittelwert (oder z.B. Differenz von 0 zwischen zwei Gruppen)\n\n\\(s_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\): Standardfehler des Mittelwerts, basierend auf der Stichproben-Standardabweichung \\(s\\) und der Stichprobengrösse \\(n\\)\n\np-Wert bestimmen und Entscheidung treffen\n\nAus dem t-Wert (und den Freiheitsgraden \\(\\text{df} = n-1\\) oder ähnlich) kannst du den p-Wert ablesen (z.B. mittels t-Verteilungstabellen oder Software). Der p-Wert gibt an, wie wahrscheinlich (oder selten) eine so grosse oder grössere Abweichung zufällig auftreten würde, wenn \\(H_0\\) wahr wäre.\n\nRichtlinie: Ist der p-Wert kleiner als \\(\\alpha\\) (z.B. &lt; 0.05), lehnen wir \\(H_0\\) ab – das Ergebnis gilt als „statistisch signifikant“.\n\n\n\n\n\n\n\n\nBeispiel\n\n\n\n1. Hypothesen aufstellen\nSchauen wir uns das Beispiel aus Kapitel 7.1.2.1.1 an. Wir haben eine Stichprobe von 5 Studierenden und ihre Prüfungsnoten (Skala 0–100).\nWir wollen testen, ob der Mittelwert der Grundgesamtheit 75 ist.\nDie Null- und Alternativhypothesen lauten:\n\\[\nH_0: \\widehat{\\bar{Y}} = 75 \\quad \\text{(kein Unterschied, Mittelwert entspricht 75)}\n\\]\n\\[\nH_1: \\widehat{\\bar{Y}} \\neq 75 \\quad \\text{(es gibt eine Abweichung)}\n\\]\nAnmerkung zur Notation\nWir kennzeichnen Schätzungen mit einem Dach \\(\\widehat{X}\\).\n\\(\\widehat{\\bar{Y}} = 75\\) bedeutet also, dass wir den Mittelwert der Variable \\(Y\\) auf 75 schätzen.\nWir halten zudem fest, dass wir einen zweiseitigen Test durchführen, da wir keine Richtung der Abweichung vorgeben.\n2. Signifikanzniveau \\(\\alpha\\) festlegen\nWir entscheiden uns für ein übliches Signifikanzniveau:\n\\[\n\\alpha = 0.05\n\\]\nDas bedeutet:\n\nFalls der p-Wert kleiner als 0.05 ist, lehnen wir \\(H_0\\) ab und nehmen eine signifikante Abweichung an.\nFalls der p-Wert größer als 0.05 ist, können wir \\(H_0\\) nicht ablehnen.\n\n3. Stichprobe erheben\nWir übernehmen die Daten aus Tabelle 7.1 welche die Noten von 5 Studierenden enthält.\nBerechnung der Kennwerte\n\nMittelwert der Stichprobe (siehe Gleichung 7.1)\n\n\\[\n\\bar{Y} = 70\n\\]\n\nStichprobenstandardabweichung (siehe Gleichung 7.2)\n\n\\[\n\\sigma_Y = 15.81\n\\]\n\nStandardfehler des Mittelwerts:\n\\[\n\\begin{aligned}\ns_{\\bar{Y}} &= \\frac{\\sigma_Y}{\\sqrt{n}} \\\\\n&= \\frac{15.81}{\\sqrt{5}} \\\\\n&\\approx 7.07\n\\end{aligned}\n\\]\n\n4. Teststatistik berechnen\nNun berechnen wir den t-Wert mit der Formel:\n\\[\n\\begin{aligned}\nt &= \\frac{\\bar{Y} - \\widehat{\\bar{Y}}}{s_{\\bar{Y}}} \\\\\n&= \\frac{70 - 75}{7.07} \\\\\n&= \\frac{-5}{7.07} \\\\\n&\\approx -0.71\n\\end{aligned}\n\\]\n\nDer t-Wert gibt an, wie viele Standardfehler der Stichprobenmittelwert von \\(\\widehat{\\bar{Y}}\\) entfernt ist.\nHier bedeutet \\(t = -0.71\\), dass unser Stichprobenmittelwert 0.71 Standardfehler unter 75 liegt.\n\n5. p-Wert bestimmen und Entscheidung treffen\nDer p-Wert wird aus der t-Verteilung mit \\(df = n - 1 = 4\\) Freiheitsgraden berechnet:\n\\[\np = 2 \\times (1 - P(T \\leq |t|))\n\\]\nWir machen das in R:\n\n\nCode\np &lt;- pt(-0.71, df = 4) * 2 # 2 * weil zweiseitig\np\n\n\n[1] 0.5169039\n\n\n\\[\np \\approx 0.519\n\\]\nErgebnis:\n\n\\(p = 0.519 &gt; 0.05\\), also ist die Abweichung nicht signifikant.\nWir können \\(H_0\\) nicht ablehnen, d.h., die Stichprobe liefert keine ausreichenden Beweise dafür, dass wir sagen können, dass der Mittelwert der Grundgesamtheit nicht 75 ist. Wichtig: Wir haben keinen Beweis dafür, dass der Mittelwert der Grundgesamtheit 75 ist.\n\n\n\n\n8.2.1 t-Test\nEin t-Test ist ein statistischer Test, der oft genutzt wird, um Mittelwerte zu vergleichen oder einen Mittelwert mit einem Referenzwert zu prüfen. Beispielsweise kannst du testen, ob das Durchschnittsgewicht einer Stichprobe signifikant von 70 kg abweicht (\\(H_0: \\mu = 70\\)).\nVoraussetzung ist, dass die Daten metrisch, ohne Ausreisser und symetrisch verteilt sind.\nDer Kern des t-Tests:\n\nDu berechnest den t-Wert als \\(\\frac{\\text{Abweichung des Mittelwerts}}{\\text{Standardfehler}}\\).\n\nAus diesem t-Wert und den Freiheitsgraden (z.B. \\(n-1\\)) bestimmt man den p-Wert mithilfe der t-Verteilung.\n\nIst der p-Wert kleiner als das vorab festgelegte Signifikanzniveau \\(\\alpha\\), so lehnt man \\(H_0\\) ab.\n\nEin typisches Beispiel ist der Zweistichproben-t-Test (unabhängige Gruppen), bei dem untersucht wird, ob sich zwei Mittelwerte (z.B. Gruppe A vs. Gruppe B) signifikant unterscheiden.\n\n8.2.1.1 t-Wert vs. p-Wert\n\nDer p-Wert berechnet sich aus der t-Verteilung mit den Freiheitsgraden \\(n-1\\).\n\n\\[\np = 2 \\times (1 - P(T \\leq |t|))\n\\]\n\nDer t-Wert ist der numerische „Abstand“ deiner beobachteten Daten (Mittelwertdifferenz) vom Wert unter \\(H_0\\), gemessen in Einheiten des Standardfehlers.\n\nDer p-Wert ist die Wahrscheinlichkeit, einen t-Wert (oder Teststatistik) zu erhalten, der mindestens so extrem ist wie dein beobachteter, wenn \\(H_0\\) gilt.\n\nOder vereinfacht:\n\nt-Wert: „Wir sind 2.5 Standardfehler vom erwarteten Wert entfernt.“\n\np-Wert: „Diese Abweichung kommt nur mit 1% Wahrscheinlichkeit zustande, wenn \\(H_0\\) stimmt.“\n\nBeide Werte gehören zusammen: Ohne t-Wert weisst du nicht, wie stark die Abweichung ist; ohne p-Wert weisst du nicht, wie (un)wahrscheinlich diese Abweichung unter der Nullhypothese wäre.\n\n\n\n8.2.2 Fehlerarten\n\n\n\nTestentscheidung\n\\(H_0\\) nicht ablehnen\n\\(H_0\\) ablehnen\n\n\n\n\n\\(H_0\\) wahr\nRichtige Entscheidung\nFehler 1. Art\n\n\n\\(H_0\\) falsch\nFehler 2. Art\nRichtige Entscheidung\n\n\n\n\nFehler 1. Art (Alpha-Fehler): Wir lehnen \\(H_0\\) ab, obwohl \\(H_0\\) wahr ist.\nFehler 2. Art (Beta-Fehler): Wir nehmen \\(H_0\\) an, obwohl \\(H_0\\) falsch ist.\nEs gibt keine Testverfahren, die beide Fehlerarten gleichzeitig minimieren können.\nDas Signifikanzniveau \\(\\alpha\\) ist die Wahrscheinlichkeit für einen Fehler 1. Art.\nDer Fehler 2. Art ist in der Regel weniger gravierend.\n\n\n\n8.2.3 Mittwelwerte Testen\n\nMittelwert \\(\\mu\\) und Standardabweichung \\(\\sigma\\) aus \\(X_{\\text{Mittel}}\\) und \\(s_x\\) schätzen.\nDas führt bei kleinen Stichproben zu grossen Standardfehlern.\nes ist unwahrscheinlich, dass die Stichprobe exakt das Mittel der Grundgesamtheit trifft.\nDadurch wird die Verteilung der Teststatistik \\(t\\) breiter.\n\n\n\n\nT-Verteilung\n\n\nDieser Test ist ein zweiseitiger Test mit \\(\\alpha = 0.05\\). Das führt dazu, dass wir die Quantile so verteilen, dass “unten” 2.5% der Fläche und “oben” 2.5% der Fläche liegen.\nBei einem einseitigen Test wäre \\(\\alpha = 0.05\\) und wir würden die Quantile so verteilen, dass “unten” 5% der Fläche und “oben” 95% der Fläche liegen.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistische Tests</span>"
    ]
  },
  {
    "objectID": "statistische-tests.html#konfidenzintervalle",
    "href": "statistische-tests.html#konfidenzintervalle",
    "title": "8  Statistische Tests",
    "section": "8.3 Konfidenzintervalle",
    "text": "8.3 Konfidenzintervalle\n\nMasszahl für die Unsicherheit der Parameterschätzung\nKonfidenzintervalle sind Intervalle, die den wahren Wert einer Grundgesamtheit mit einer bestimmten Wahrscheinlichkeit schätzen.\nEng verknüpft mit dem Signifikanzlevel \\(\\alpha\\)\n\nAllgemeine Formel:\nEin \\((1 - \\alpha)\\)-Konfidenzintervall für den Mittelwert wird berechnet mit:\n\\[\n\\operatorname{CI}_{(1-\\alpha)} = \\bar{x} \\pm t_{\\alpha/2, df} \\cdot s_{\\bar{x}}\n\\]\nwobei:\n\n\\(\\bar{x}\\) = Mittelwert der Stichprobe\n\\(t_{\\alpha/2, df}\\) = kritischer t-Wert aus der t-Verteilung mit \\(df = n - 1\\) Freiheitsgraden\n\\(s_{\\bar{x}}\\) = Standardfehler des Mittelwerts\n\n\n\n\n\n\n\nBeispiel\n\n\n\nNachdem wir in unserem t-Test festgestellt haben, dass wir die Nullhypothese \\(H_0: \\mu = 75\\) nicht ablehnen können, wollen wir nun ein 95%-Konfidenzintervall (CI) für den Mittelwert der Grundgesamtheit bestimmen. Das Konfidenzintervall gibt uns einen Bereich, in dem der wahre Mittelwert mit 95% Wahrscheinlichkeit liegt.\n1. Berechnungen für unser Beispiel aus Kapitel 7.1.2.1.1\nGegeben:\n\n\n\n\n\n\n\n\nWert\nBerechnung\nErgebnis\n\n\n\n\nStichprobe\n\\(Y = \\{70, 80, 50, 90, 60\\}\\)\n\n\n\nStichprobenmittelwert \\(\\bar{Y}\\)\n\\(\\frac{70+80+50+90+60}{5}\\)\n70.00\n\n\nStichprobenstandardabweichung \\(s\\)\n\\(\\sqrt{\\frac{\\sum (Y_i - \\bar{Y})^2}{n-1}}\\)\n15.81\n\n\nStandardfehler \\(s_{\\bar{Y}}\\)\n\\(\\frac{s}{\\sqrt{n}}\\)\n7.07\n\n\nFreiheitsgrade \\(df\\)\n\\(n-1 = 4\\)\n4\n\n\n\n2. Kritischen t-Wert für \\(\\alpha = 0.05\\) bestimmen\nDa wir ein 95%-Konfidenzintervall berechnen, setzen wir:\n\\[\n\\alpha = 0.05 \\quad \\Rightarrow \\quad t_{\\alpha/2, df=4}\n\\]\nDen kritischen t-Wert können wir in R mit qt() berechnen:\n\nalpha &lt;- 0.05\ndf &lt;- 4\nt_crit &lt;- qt(1 - alpha/2, df)\nt_crit\n\n[1] 2.776445\n\n\nErgebnis:\n\\[\nt_{\\alpha/2,4} = 2.776\n\\]\n3. Konfidenzintervall berechnen\nNun setzen wir alles in die Formel ein:\n\nx_bar &lt;- 70    # Mittelwert der Stichprobe\ns_x_bar &lt;- 7.07  # Standardfehler des Mittelwerts\nci_lower &lt;- x_bar - t_crit * s_x_bar\nci_upper &lt;- x_bar + t_crit * s_x_bar\nc(ci_lower, ci_upper)\n\n[1] 50.37053 89.62947\n\n\n\\[\nCI_{95\\%} = [50.37, 89.63]\n\\]\n4. Interpretation\n\nWir sind 95% sicher, dass der wahre Mittelwert der Grundgesamtheit zwischen 50.37 und 89.63 liegt.\nDa 75 innerhalb dieses Intervalls liegt, gibt es keine signifikante Abweichung von 75 – das bestätigt unser vorheriges Testergebnis.\nDas breite Intervall zeigt eine hohe Streuung oder eine kleine Stichprobe, was bedeutet, dass unsere Schätzung noch unsicher ist.\n\n\n\nDaumenregel:\n\\[\n\\text{Konfidenzintervall} = \\text{Stichprobenergebnis} \\pm 2 \\cdot \\text{Standardfehler}\n\\]",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistische Tests</span>"
    ]
  },
  {
    "objectID": "statistische-tests.html#überprüfung-auf-normalverteilung",
    "href": "statistische-tests.html#überprüfung-auf-normalverteilung",
    "title": "8  Statistische Tests",
    "section": "8.4 Überprüfung auf Normalverteilung",
    "text": "8.4 Überprüfung auf Normalverteilung\nWir stellen auch hier entsprechende Hypothesen auf.\n\\[\n\\begin{aligned}\nH_0: &\\text{Daten sind normalverteilt} \\\\\nH_A: &\\text{Daten sind nicht normalverteilt}\n\\end{aligned}\n\\]\n\n\n\n\n\n\nAnmerkung\n\n\n\n\n\nDass wir hier mit der \\(H_0\\) die Normalverteilung testen, ist nicht intuitiv. Aber wir testen hier nicht, ob die Daten normalverteilt sind, sondern ob sie nicht normalverteilt sind. Wenn wir das Gegenteil beweisen wollen, müssen wir das Gegenteil widerlegen.\nEine Normalverteilung kann man nicht beweisen, sondern nur widerlegen.\n\n\n\n\n\n\n\n\n\n\n\nTest\nVorteile\nNachteile\n\n\n\n\n\\(\\chi^2\\)-Test\n\ngeeignet für beliebig skalierte Variablen\n\n\nGruppierung der Beobachtungen notwendig\nungeeignet für kleine Stichproben\nquadratische Testgrösse, d.h. sensibel auf Ausreisser\n\n\n\nKolmogorov-Smirnov-Test\n\ngeeignet für kleine Stichproben\nwie Chi-Quadrat auch zum Vergleich anderer Verteilungen geeignet\nnicht-parametrischer Test, d.h. nicht sensibel auf Ausreisser\n\n\ngeringe Teststärke im Vergleich zu den folgenden Tests\n\n\n\nCramér-von-Mises-Test\n\nhöhere Güte als KS-Test\n\n\nquadratische Testgrösse\n\n\n\nLilliefors-Test\n\nbessere Trennschärfe als KS-Test\nnicht-parametrischer Test\n\n\nnur zum Test auf Normalverteilung\n\n\n\nAnderson-Darling-Test\n\nsehr hohe Güte bei Test auf Normalverteilung\n\n\nkeine kategorialen Daten\nquadratische Testgrösse\n\n\n\nShapiro-Wilk-Test\n\nTest mit höchster Güte\n\n\nausschliesslich Test auf Normalverteilung\nmanuell schlecht durchführbar\nsensibel auf Ausreisser und viele identische Werte\n\n\n\n\n\n8.4.1 Überprüfung auf Normalverteilung in R\n\n# Daten generieren\nset.seed(123)\nnormal_data &lt;- rnorm(500)        # 500 normalverteilte Zufallszahlen\nnon_normal_data &lt;- normal_data^2 # quadrierte Zufallszahlen (nicht normalverteilt)\n\n# Shapiro-Wilk-Test für beide Datensätze\nshapiro_result_normal &lt;- shapiro.test(normal_data)\nshapiro_result_non_normal &lt;- shapiro.test(non_normal_data)\n\n# Shapiro-Wilk-Test für normal_data:\ncat(\"W-Teststatistik:\", round(shapiro_result_normal$statistic, 4), \"\\n\")\n\nW-Teststatistik: 0.9981 \n\ncat(\"p-Wert:\", round(shapiro_result_normal$p.value, 4), \"\\n\")\n\np-Wert: 0.8639 \n\n# Shapiro-Wilk-Test für non_normal_data:\ncat(\"W-Teststatistik:\", round(shapiro_result_non_normal$statistic, 4), \"\\n\")\n\nW-Teststatistik: 0.708 \n\ncat(\"p-Wert:\", round(shapiro_result_non_normal$p.value, 4), \"\\n\")\n\np-Wert: 0 \n\n\n\nCode\nif (shapiro_result_normal$p.value &gt; 0.05) {\n  cat(\"Die Nullhypothese der Normalverteilung kann nicht verworfen werden. \\n`normal_data` ist normalverteilt.\\n\\n\")\n} else {\n  cat(\"Die Nullhypothese der Normalverteilung wird verworfen. \\n`normal_data` ist **nicht** normalverteilt.\\n\\n\")\n}\n\nDie Nullhypothese der Normalverteilung kann nicht verworfen werden. normal_data ist normalverteilt.\n\nCode\nif (shapiro_result_non_normal$p.value &gt; 0.05) {\n  cat(\"Die Nullhypothese der Normalverteilung kann nicht verworfen werden. \\n`non_normal_data` ist normalverteilt.\\n\\n\")\n} else {\n  cat(\"Die Nullhypothese der Normalverteilung wird verworfen. \\n`non_normal_data` ist **nicht** normalverteilt.\\n\\n\")\n}\n\nDie Nullhypothese der Normalverteilung wird verworfen. non_normal_data ist nicht normalverteilt.\n\n\nCode\n# Layout für nebeneinanderstehende Plots definieren\npar(mfrow = c(2, 2), mar = c(4, 4, 2, 1))\n\n# Histogramm\nhist(normal_data, \n     main = \"Histogramm (Normalverteilung)\", \n     xlab = \"Werte\", \n     col = \"lightblue\", \n     border = \"white\")\nhist(non_normal_data, \n     main = \"Histogramm (Nicht Normalverteilt)\", \n     xlab = \"Werte\", \n     col = \"lightcoral\", \n     border = \"white\")\n\n# Dichtefunktion\nplot(density(normal_data), \n     main = \"Dichtefunktion (Normalverteilung)\", \n     xlab = \"Werte\", \n     col = \"darkblue\", \n     lwd = 2)\nplot(density(non_normal_data), \n     main = \"Dichtefunktion (Nicht Normalverteilt)\", \n     xlab = \"Werte\", \n     col = \"darkred\", \n     lwd = 2)\n\n\n\n\n\nVisualisierung der Normalverteilung\n\n\n\n\nCode\n# Boxplot\nboxplot(normal_data, \n        main = \"Boxplot (Normalverteilung)\", \n        col = \"lightgreen\", \n        horizontal = TRUE)\nboxplot(non_normal_data, \n        main = \"Boxplot (Nicht Normalverteilt)\", \n        col = \"lightpink\", \n        horizontal = TRUE)\n\n# QQ-Plot\nqqnorm(normal_data, \n       main = \"QQ-Plot (Normalverteilung)\")\nqqline(normal_data, col = \"red\", lwd = 2)\n\nqqnorm(non_normal_data, \n       main = \"QQ-Plot (Nicht Normalverteilt)\")\nqqline(non_normal_data, col = \"red\", lwd = 2)\n\n\n\n\n\nVisualisierung der Normalverteilung\n\n\n\n\nCode\n# Layout zurücksetzen\npar(mfrow = c(1, 1))\n\n\n\n\n8.4.2 \\(\\chi^2\\)-Test\n\nSumme der quadrierten Abweichungen:\n\n\\[\n\\chi^2 = \\sum_{i=1}^{k} \\frac{(N_i - n_i)^2}{n_i}\n\\]\n\n\\(N_i\\): beobachtete Häufigkeit in der Klasse \\(i\\)\n\\(n_i\\): erwartete Häufigkeit in der Klasse \\(i\\)\n\\(k\\): Anzahl der Klassen\n\n\n8.4.2.1 \\(\\chi^2\\)-Verteilung\n\nstetige Wahrscheinlichkeitsverteilung mit der Anzahl Freiheitsgrade \\(k\\) als einzigem Parameter\nVerteilung der Summe der Quadrate von \\(k\\) unabhängigen und standardnormalverteilten Zufallsvariablen.\n\n\n\nCode\n# Chi-Quadrat-Verteilung plotten\nx &lt;- seq(0, 8, length.out = 500)\ndf_values &lt;- c(1, 2, 3, 4, 6, 9)\n\n# Farben definieren\ncolors &lt;- c(\"darkgreen\", \"green\", \"blue\", \"purple\", \"orange\", \"red\")\n\n# Plot erstellen\nplot(x, dchisq(x, df = 1), type = \"l\", lwd = 2, col = colors[1], \n     ylim = c(0, 0.5), xlab = \"x\", ylab = expression(f[k](x)), \n     main = expression(chi^2~\"Verteilung\"))\n\n# Weitere Linien hinzufügen\nfor (i in 2:length(df_values)) {\n  lines(x, dchisq(x, df = df_values[i]), col = colors[i], lwd = 2)\n}\n\n# Legende hinzufügen\nlegend(\"topright\", legend = paste(\"k=\", df_values), \n       col = colors, lwd = 2, bty = \"n\")\n\n\n\n\n\nChi-Quadrat-Verteilung",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistische Tests</span>"
    ]
  },
  {
    "objectID": "statistische-tests.html#testauswahl",
    "href": "statistische-tests.html#testauswahl",
    "title": "8  Statistische Tests",
    "section": "8.5 Testauswahl",
    "text": "8.5 Testauswahl\n\n\n\nTestauswahl Quelle: Methodenberatung UZH",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Statistische Tests</span>"
    ]
  },
  {
    "objectID": "korrelation.html",
    "href": "korrelation.html",
    "title": "9  Korrelation",
    "section": "",
    "text": "9.1 Von der Kovarianz zur Korrelation",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "korrelation.html#von-der-kovarianz-zur-korrelation",
    "href": "korrelation.html#von-der-kovarianz-zur-korrelation",
    "title": "9  Korrelation",
    "section": "",
    "text": "Das Produkt der Abweichungen:\n\nZentrale Frage: “Variieren zwei Variablen gemeinsam?”\nFormel: \\[\n(X_i - \\bar{X}) \\cdot (Y_i - \\bar{Y})\n\\]\n\n\\(X_i\\) und \\(Y_i\\) sind die beobachteten Werte der Variablen \\(X\\) und \\(Y\\) für die \\(i\\)-te Beobachtung.\n\\(\\bar{X}\\) und \\(\\bar{Y}\\) sind die Mittelwerte der Variablen \\(X\\) und \\(Y\\).\nPositives Produkt: Beide Abweichungen haben das gleiche Vorzeichen (gleichsinnige Variation).\nNegatives Produkt: Abweichungen haben unterschiedliche Vorzeichen (gegensätzliche Variation).\n\n\nKovarianz: Bedeutung und Berechnung:\n\nDie Kovarianz misst die durchschnittliche gemeinsame Abweichung zweier Variablen von ihren Mittelwerten.\nFormel der Stichprobenkovarianz: \\[\n\\text{Cov}(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y})\n\\]\n\n\\(n\\) ist die Anzahl der Beobachtungen.\n\\(X_i\\) und \\(Y_i\\) sind die beobachteten Werte der Variablen \\(X\\) und \\(Y\\) für die \\(i\\)-te Beobachtung.\n\\(\\bar{X}\\) und \\(\\bar{Y}\\) sind die Mittelwerte der Variablen \\(X\\) und \\(Y\\).\n\nInterpretation:\n\nPositive Kovarianz: Tendenz zu gleichsinniger Variation.\nNegative Kovarianz: Tendenz zu gegensätzlicher Variation.\nNahe 0: Kein linearer Zusammenhang.\n\n\nNormierung zur Berechnung des Korrelationskoeffizienten:\n\nProblem der Kovarianz: Abhängig von den Einheiten der Variablen.\nLösung: Normierung durch die Standardabweichungen von \\(X\\) und \\(Y\\): \\[\nr = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\cdot \\sigma_Y}\n\\]\n\n\\(\\sigma_X\\) und \\(\\sigma_Y\\) sind die Standardabweichungen der Variablen \\(X\\) und \\(Y\\).\nErgebnis: Der Korrelationskoeffizient (\\(r\\)), der immer zwischen -1 und +1 liegt.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "korrelation.html#der-pearson-korrelationskoeffizient",
    "href": "korrelation.html#der-pearson-korrelationskoeffizient",
    "title": "9  Korrelation",
    "section": "9.2 Der Pearson-Korrelationskoeffizient",
    "text": "9.2 Der Pearson-Korrelationskoeffizient\n\\[\n\\rho_{X,Y} = \\frac{\\sum_{i=1}^N (X_i - \\mu_x)(Y_i - \\mu_y)}{\\sqrt{\\sum_{i=1}^N (X_i - \\mu_x)^2 \\sum_{i=1}^N (Y_i - \\mu_y)^2}}\n\\]\n\n\\(\\rho\\) ist der Standardbuchstabe für den Korrelationskoeffizienten der Grundgesamtheit.\n\\(r\\) ist der Standardbuchstabe für den Korrelationskoeffizienten der Stichprobe.\n\\(\\mu_x\\) und \\(\\mu_y\\) sind die Mittelwerte der Variablen \\(X\\) und \\(Y\\).\n\\(N\\) ist die Anzahl der Beobachtungen (wird hier gross geschrieben da es sich um die Grundgesamtheit handelt)\n\\(X_i\\) und \\(Y_i\\) sind die beobachteten Werte der Variablen \\(X\\) und \\(Y\\) für die \\(i\\)-te Beobachtung.\n\n\n\n\n\n\n\nBeispiel\n\n\n\nWir ergänzen unsere Daten aus Tabelle 7.1 um eine zweite Variable, die Lernzeit pro Woche (h), um zu untersuchen, ob ein Zusammenhang zwischen Lernzeit und Prüfungsnoten besteht.\n\n\n\nTabelle 9.1\n\n\n\n\n\nStudent\nPrüfungsnote (\\(Y\\))\nLernzeit (\\(X\\))\n\n\n\n\nA\n70\n10\n\n\nB\n80\n12\n\n\nC\n50\n5\n\n\nD\n90\n15\n\n\nE\n60\n8\n\n\n\n\n\n\n1. Berechnung der Mittelwerte\n\nlibrary(tibble)\n\nnoten_lernzeit_tabelle &lt;- tibble(\n    student = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n    note = c(70, 80, 50, 90, 60),\n    lernzeit = c(10, 12, 5, 15, 8)\n)\n\n\nmean(noten_lernzeit_tabelle$note)\n\n[1] 70\n\n\n\\[\n\\begin{aligned}\n\\bar{Y} &= \\frac{70 + 80 + 50 + 90 + 60}{5} \\\\\n&= 70 \\\\\n\\end{aligned}\n\\tag{9.1}\\]\n\nmean(noten_lernzeit_tabelle$lernzeit)\n\n[1] 10\n\n\n\\[\n\\begin{aligned}\n\\bar{X} &= \\frac{10 + 12 + 5 + 15 + 8}{5} \\\\\n&= 10\n\\end{aligned}\n\\tag{9.2}\\]\n2. Kovarianz berechnen\n\ncov(noten_lernzeit_tabelle$note, noten_lernzeit_tabelle$lernzeit)\n\n[1] 60\n\n\n\\[\n\\begin{aligned}\n\\text{Cov}(X, Y) &= \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})(Y_i - \\bar{Y}) \\\\\n&= \\frac{1}{4}\n\\left[\n\\begin{array}{l}\n(10-10) \\cdot (70-70) + (12-10) \\cdot (80-70) +  (5-10) \\cdot (50-70) +  \\\\\n(15-10) \\cdot (90-70) +  (8-10) \\cdot (60-70)\n\\end{array}\n\\right] \\\\\n&= \\frac{1}{4} \\left[0 \\cdot 0 + 2 \\cdot 10 + (-5) \\cdot (-20) + 5 \\cdot 20 + (-2) \\cdot (-10)\\right] \\\\\n&= \\frac{1}{4} \\left[0 + 20 + 100 + 100 + 20\\right] \\\\\n&= \\frac{1}{4} 240 \\\\\n&= 60\n\\end{aligned}\n\\tag{9.3}\\]\n3. Standardabweichungen berechnen\n\nsd(noten_lernzeit_tabelle$note)\n\n[1] 15.81139\n\n\n\\[\n\\begin{aligned}\n\\sigma_Y &= \\sqrt{\\frac{1}{n-1} \\sum (Y_i - \\bar{Y})^2} \\\\\n&= \\sqrt{\\frac{1}{4} \\left[(70-70)^2 + (80-70)^2 + (50-70)^2 + (90-70)^2 + (60-70)^2\\right]} \\\\\n&= \\sqrt{\\frac{1}{4} \\left[0 + 100 + 400 + 400 + 100\\right]} \\\\\n&= \\sqrt{\\frac{1}{4} 1000} \\\\\n&= \\sqrt{250} \\\\\n&\\approx 15.81\n\\end{aligned}\n\\tag{9.4}\\]\n\nsd(noten_lernzeit_tabelle$lernzeit)\n\n[1] 3.807887\n\n\n\\[\n\\begin{aligned}\n\\sigma_X &= \\sqrt{\\frac{1}{n-1} \\sum (X_i - \\bar{X})^2} \\\\\n&= \\sqrt{\\frac{1}{4} \\left[(10-10)^2 + (12-10)^2 + (5-10)^2 + (15-10)^2 + (8-10)^2\\right]} \\\\\n&= \\sqrt{\\frac{1}{4} \\left[0 + 4 + 25 + 25 + 4\\right]} \\\\\n&= \\sqrt{\\frac{1}{4} 58} \\\\\n&= \\sqrt{14.5} \\\\\n&\\approx 3.81\n\\end{aligned}\n\\tag{9.5}\\]\n4. Pearson-Korrelationskoeffizient berechnen\n\ncor(noten_lernzeit_tabelle$note, noten_lernzeit_tabelle$lernzeit, method = \"pearson\")\n\n[1] 0.9965458\n\n\n\\[\n\\begin{aligned}\nr &= \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\cdot \\sigma_Y} \\\\\n&= \\frac{60}{3.81 \\cdot 15.81} \\\\\n&= \\frac{60}{60.21} \\\\\n&\\approx 0.997\n\\end{aligned}\n\\]\nErgebnis: \\(\\rho_{X,Y} \\approx 0.997\\), was auf eine sehr starke positive Korrelation hindeutet.\n5. Interpretation\n\nDa \\(\\rho_{X,Y} \\approx 0.997\\) nahe an 1 liegt, bedeutet das, dass Studierende, die mehr Lernzeit investiert haben, tendenziell bessere Noten erzielt haben. (Aber: wir können nicht sagen, dass die Lernzeit die Note beeinflusst hat, sondern nur, dass beide Variablen tendenziell gemeinsam variieren.)\nDas zeigt eine fast perfekte positive Korrelation zwischen den beiden Variablen.\n\n\n\n\n\n\n\n\n\nHerleitung\n\n\n\n\n\n\nIdee: Produkt der Anomalien\n\n\\[\n\\begin{aligned}\nX_i ^d &= X_i - \\mu_x \\\\\nY_i ^d &= Y_i - \\mu_y \\\\\n\\sum_{i=1}^N (X_i - \\mu_x)(Y_i - \\mu_y) &= \\sum_{i=1}^N X_i ^d Y_i ^d\n\\end{aligned}\n\\]\nWo \\(X_i ^d\\) die Abweichung von \\(X_i\\) vom Mittelwert \\(\\mu_x\\) der Variable \\(X\\) ist und \\(Y_i ^d\\) die Abweichung von \\(Y_i\\) vom Mittelwert \\(\\mu_y\\) der Variable \\(Y\\) ist.\nProblem: Die Summe der Produkte der Abweichungen ist abhängig von der Stichprobengrösse \\(N\\).\n\nDivision durch Stichprobengrösse Kovarianz zwischen \\(X\\) und \\(Y\\)\n\n\\[\n\\sigma_{x,y} = \\frac{i=1}{N-1} (X_i - \\mu_x)(Y_i - \\mu_y)\n\\]\nProblem: Die Kovarianz ist abhängig von den Einheiten der Variablen.\n\nStandardisierung durch die Standardabweichungen der Variablen \\(X\\) und \\(Y\\)\n\n\\[\n\\rho_{X,Y} = \\frac{\\sum_{i=1}^N \\frac{(X_i - \\mu_x)}{\\sigma_x} \\frac{(Y_i - \\mu_y)}{\\sigma_y}}{N}\n\\]\nwo:\n\\[\n\\begin{aligned}\n\\sigma_x &= \\sqrt{\\frac{\\sum_{i=1}^N (X_i - \\mu_x)^2}{N}} \\text{ ,und} \\\\\n\\sigma_y &= \\sqrt{\\frac{\\sum_{i=1}^N (Y_i - \\mu_y)^2}{N}}\n\\end{aligned}\n\\]\n\nErgebnis ist der Pearson-Korrelationskoeffizient \\(\\rho_{x,y}\\)\n\n\\[\n\\rho_{X,Y} = \\frac{\\sum_{i=1}^N (X_i - \\mu_x)(Y_i - \\mu_y)}{\\sqrt{\\sum_{i=1}^N (X_i - \\mu_x)^2 \\sum_{i=1}^N (Y_i - \\mu_y)^2}}\n\\]\n\n\n\nWenn wir Daten plotten, können wir häufig Korrelationen auch visuell schon relativ gut schätzen.\n\n\nCode\n# Set seed for reproducibility\nset.seed(123)\n\n# Hohe Korrelation (r nahe bei 1)\nx_high &lt;- rnorm(30, mean = 5, sd = 1)\ny_high &lt;- 2 * x_high + rnorm(30, mean = 0, sd = 0.5)\nr_high &lt;- cor(x_high, y_high)\n\n# Niedrige Korrelation (r nahe bei 0)\nx_low &lt;- rnorm(30, mean = 5, sd = 1)\ny_low &lt;- rnorm(30, mean = 5, sd = 1)\nr_low &lt;- cor(x_low, y_low)\n\n# Plots nebeneinander\npar(mfrow = c(1, 2), mar = c(4, 4, 2, 1))\n\n# Plot mit hoher Korrelation\nplot(x_high, y_high, \n     main = paste(\"Hohe Korrelation\\nr =\", round(r_high, 3)), \n     xlab = \"x\", ylab = \"y\", \n     pch = 19, col = rgb(0, 0, 1, 0.5))\n\n# Plot mit niedriger Korrelation\nplot(x_low, y_low, \n     main = paste(\"Niedrige Korrelation\\nr =\", round(r_low, 3)), \n     xlab = \"x\", ylab = \"y\", \n     pch = 19, col = rgb(0, 0, 1, 0.5))\n\n\n\n\n\nBeispiele für Pearson-Korrelationen\n\n\n\n\nDie visuelle Darstellung erlaubt es uns, die Sensitivität des Pearson-Korrelationskoeffizienten gegenüber Ausreissern zu betrachten. Dafür fügen wir den Daten einen Ausreisser hinzu und sehen, wie sich der Korrelationskoeffizient drastisch verändert.\n\n\nCode\n# Berechnung des Pearson-Korrelationskoeffizienten\n\n# Daten ohne Ausreisser generieren\nx &lt;- runif(30, 1, 10)\ny &lt;- runif(30, 1, 10)\n\n# Korrelationskoeffizient ohne Ausreisser\nr_no_outlier &lt;- cor(x, y)\n\n# Daten mit Ausreisser hinzufügen\nx_outlier &lt;- c(x, 20)\ny_outlier &lt;- c(y, 25)\n\n# Pearson-Korrelationskoeffizient mit Ausreisser\nr_with_outlier &lt;- cor(x_outlier, y_outlier)\n\n# Plots nebeneinander\npar(mfrow = c(1, 2), mar = c(4, 4, 2, 1))\n\n# Plot ohne Ausreisser\nplot(x, y, \n     main = paste(\"Ohne Ausreisser\\nr =\", round(r_no_outlier, 3)), \n     xlab = \"x\", ylab = \"y\", \n     pch = 19, col = rgb(0, 0, 1, 0.5), xlim = c(0, 22), ylim = c(0, 27))\n\n# Plot mit Ausreisser\nplot(x_outlier, y_outlier, \n     main = paste(\"Mit Ausreisser\\nr =\", round(r_with_outlier, 3)), \n     xlab = \"x\", ylab = \"y\", \n     pch = 19, col = c(rep(rgb(0, 0, 1, 0.5), 30), rgb(1, 0, 0, 0.5)), xlim = c(0, 22), ylim = c(0, 27))\n\n\n\n\n\nEinfluss von Ausreissern auf den Pearson-Korrelationskoeffizienten",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "korrelation.html#der-spearman-rangkorrelationskoeffizient",
    "href": "korrelation.html#der-spearman-rangkorrelationskoeffizient",
    "title": "9  Korrelation",
    "section": "9.3 Der Spearman-Rangkorrelationskoeffizient",
    "text": "9.3 Der Spearman-Rangkorrelationskoeffizient\nDer Spearman-Korrelationskoeffizient misst den monotonen Zusammenhang zwischen zwei Variablen anhand ihrer Ränge. Er ist robuster gegenüber Ausreissern als die Pearson-Korrelation und eignet sich auch für nicht-lineare Beziehungen.\n\nWird verwendet, wenn der Zusammenhang nicht-linear, aber monoton ist.\nRobuster gegen Ausreißer als Pearson.\nFunktioniert für metrische und ordinale Daten.\n\n\\[\nr_{(X,Y)} = 1 - \\frac{6 \\sum_{i=1}^n (r_{X_{i}} - r_{Y_{i}})^2}{n(n^2 - 1)}\n\\]\n\n\\(r_{X_{i}}\\) ist der Rang der \\(i\\)-ten Beobachtung von \\(X\\)\n\\(r_{Y_{i}}\\) ist der Rang der \\(i\\)-ten Beobachtung von \\(Y\\)\n\\(n\\) ist die Anzahl der Beobachtungen.\nACHTUNG: Vereinfachte Formel, wenn jeder Rang nur einmal vorkommt. Diese Formel darf bei unentschiedenen Rängen nicht verwendet werden.\n\n\n\n\n\n\n\nBeispiel\n\n\n\nWir können mit unseren Daten aus Tabelle 9.1 auch den Spearman-Rangkorrelationskoeffizienten berechnen.\n1. Ränge berechnen\n\nnoten_lernzeit_tabelle$note_rank &lt;- rank(noten_lernzeit_tabelle$note)\nnoten_lernzeit_tabelle$lernzeit_rank &lt;- rank(noten_lernzeit_tabelle$lernzeit)\n\nnoten_lernzeit_tabelle\n\n# A tibble: 5 × 5\n  student  note lernzeit note_rank lernzeit_rank\n  &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 A          70       10         3             3\n2 B          80       12         4             4\n3 C          50        5         1             1\n4 D          90       15         5             5\n5 E          60        8         2             2\n\n\n\n\n\n\n\n\n\n\n\n\nStudent\nPrüfungsnote (\\(Y\\))\n\\(r_{Y_{i}}\\)\nLernzeit (\\(X\\))\n\\(r_{X_{i}}\\)\n\n\n\n\nA\n70\n3\n10\n3\n\n\nB\n80\n4\n12\n4\n\n\nC\n50\n1\n5\n1\n\n\nD\n90\n5\n15\n5\n\n\nE\n60\n2\n8\n2\n\n\n\n2. Berechnung der Rangdifferenzen und quadrierten Rangdifferenzen\n\nnoten_lernzeit_tabelle$d_i &lt;- noten_lernzeit_tabelle$note_rank - noten_lernzeit_tabelle$lernzeit_rank\nnoten_lernzeit_tabelle$d_i_sq &lt;- noten_lernzeit_tabelle$d_i^2\n\nsum(noten_lernzeit_tabelle$d_i_sq)\n\n[1] 0\n\n\n\\[\n\\begin{aligned}\n\\sum d_i^2 &= \\sum_{i=1}^N (r_{X_{i}} - r_{Y_{i}})^2 \\\\\n&= (3-3)^2 + (4-4)^2 + (1-1)^2 + (5-5)^2 + (2-2)^2 \\\\\n&= 0 + 0 + 0 + 0 + 0 \\\\\n&= 0\n\\end{aligned}\n\\]\n3. Spearman-Korrelation berechnen\n\ncor(noten_lernzeit_tabelle$note, noten_lernzeit_tabelle$lernzeit, method = \"spearman\")\n\n[1] 1\n\n\n\\[\n\\begin{aligned}\nr_{(X,Y)} &= 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)} \\\\\n&= 1 - \\frac{6 \\times 0}{5(5^2 - 1)} \\\\\n&= 1 - 0 \\\\\n&= 1\n\\end{aligned}\n\\]\nErgebnis: \\(r_{X,Y} = 1\\), was auf eine perfekte positive Korrelation hindeutet.\n4. Interpretation\n\nDa \\(r_{(X,Y)} = 1\\), bedeutet das, dass die Ränge von Prüfungsnoten und Lernzeit perfekt übereinstimmen.\nDas zeigt, dass die Variablen monoton steigend zusammenhängen – also mehr Lernzeit immer mit einer besseren Note einhergeht.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "korrelation.html#vergleich-der-korrelationskoeffizienten",
    "href": "korrelation.html#vergleich-der-korrelationskoeffizienten",
    "title": "9  Korrelation",
    "section": "9.4 Vergleich der Korrelationskoeffizienten",
    "text": "9.4 Vergleich der Korrelationskoeffizienten\n\n9.4.1 Visualisierung der Korrelationen\n\n\nCode\n# Synthetische Daten generieren\nx &lt;- rnorm(100, mean = 10, sd = 1)\ny &lt;- 0.8 * x + rnorm(100, mean = 0, sd = 0.5)\n\n# Funktion zur Berechnung von Pearson- und Spearman-Korrelation\ncorrelations &lt;- function(x, y) {\n  list(\n    pearson = round(cor(x, y, method = \"pearson\"), 2),\n    spearman = round(cor(x, y, method = \"spearman\"), 2)\n  )\n}\n\n# Layout für 2x2 Plots\npar(mfrow = c(2, 2), mar = c(4, 4, 2, 1))\n\n# 1. Plot (ohne Ausreisser)\ncor_vals &lt;- correlations(x, y)\nplot(x, y, pch = 19, col = rgb(0, 0, 1, 0.5),\n     main = paste0(\"r(Spearman) = \", cor_vals$spearman, \n                   \"\\nr(Pearson) = \", cor_vals$pearson),\n     xlab = \"x\", ylab = \"y\")\n\n# 2. Plot (Ausreisser unten rechts)\nx2 &lt;- c(x, 12)\ny2 &lt;- c(y, -3)\ncor_vals2 &lt;- correlations(x2, y2)\nplot(x2, y2, pch = c(rep(19, 100), 19), col = c(rep(rgb(0, 0, 1, 0.5), 100), rgb(1, 0, 0, 0.5)),\n     main = paste0(\"r(Spearman) = \", cor_vals2$spearman, \n                   \"\\nr(Pearson) = \", cor_vals2$pearson),\n     xlab = \"x\", ylab = \"y\")\n\n# 3. Plot (Ausreisser oben links)\nx3 &lt;- c(x, 7.5)\ny3 &lt;- c(y, 1.5)\ncor_vals3 &lt;- correlations(x3, y3)\nplot(x3, y3, pch = c(rep(19, 100), 19), col = c(rep(rgb(0, 0, 1, 0.5), 100), rgb(1, 0, 0, 0.5)),\n     main = paste0(\"r(Spearman) = \", cor_vals3$spearman, \n                   \"\\nr(Pearson) = \", cor_vals3$pearson),\n     xlab = \"x\", ylab = \"y\")\n\n# 4. Plot (zwei Ausreisser oben links und unten rechts)\nx4 &lt;- c(x, 7.5, 12)\ny4 &lt;- c(y, 1.5, -3)\ncor_vals4 &lt;- correlations(x4, y4)\nplot(x4, y4, pch = c(rep(19, 100), 19, 19), col = c(rep(rgb(0, 0, 1, 0.5), 100), rgb(1, 0, 0, 0.5), rgb(1, 0, 0, 0.5)),\n     main = paste0(\"r(Spearman) = \", cor_vals4$spearman, \n                   \"\\nr(Pearson) = \", cor_vals4$pearson),\n     xlab = \"x\", ylab = \"y\")\n\n\n\n\n\nEinfluss von Ausreissern auf beide Korrelationskoeffizienten\n\n\n\n\n\n\nCode\n# Set seed for reproducibility\nset.seed(42)\n\n# Basisdaten generieren\nx &lt;- runif(100, 7, 12)  # x-Werte im Bereich 7 bis 12\n\n# 1. Lineare Beziehung\ny_linear &lt;- 0.8 * x + rnorm(100, mean = 0, sd = 0.5)\n\n# 2. U-förmige (quadratische) Beziehung\ny_quadratic &lt;- -1 * (x - 9.5)^2 + 8 + rnorm(100, mean = 0, sd = 0.5)\n\n# 3. Logarithmische Beziehung\ny_logarithmic &lt;- log(x - 6.5) + rnorm(100, mean = 0, sd = 0.3)\n\n# 4. Exponentielle Beziehung\ny_exponential &lt;- exp((x - 10) / 3) + rnorm(100, mean = 0, sd = 0.5)\n\n# Funktion zur Berechnung von Pearson- und Spearman-Korrelation\ncorrelations &lt;- function(x, y) {\n  list(\n    pearson = round(cor(x, y, method = \"pearson\"), 2),\n    spearman = round(cor(x, y, method = \"spearman\"), 2)\n  )\n}\n\n# Layout für 2x2 Plots\npar(mfrow = c(2, 2), mar = c(4, 4, 2, 1))\n\n# Farben definieren\ncolors &lt;- rgb(0, 0, 1, 0.5)\n\n# 1. Plot (lineare Beziehung)\ncor_vals1 &lt;- correlations(x, y_linear)\nplot(x, y_linear, pch = 19, col = colors,\n     main = paste0(\"r(Spearman) = \", cor_vals1$spearman, \n                   \"\\nr(Pearson) = \", cor_vals1$pearson),\n     xlab = \"x\", ylab = \"y\")\n\n# 2. Plot (quadratische Beziehung)\ncor_vals2 &lt;- correlations(x, y_quadratic)\nplot(x, y_quadratic, pch = 19, col = colors,\n     main = paste0(\"r(Spearman) = \", cor_vals2$spearman, \n                   \"\\nr(Pearson) = \", cor_vals2$pearson),\n     xlab = \"x\", ylab = \"y\")\n\n# 3. Plot (logarithmische Beziehung)\ncor_vals3 &lt;- correlations(x, y_logarithmic)\nplot(x, y_logarithmic, pch = 19, col = colors,\n     main = paste0(\"r(Spearman) = \", cor_vals3$spearman, \n                   \"\\nr(Pearson) = \", cor_vals3$pearson),\n     xlab = \"x\", ylab = \"y\")\n\n# 4. Plot (exponentielle Beziehung)\ncor_vals4 &lt;- correlations(x, y_exponential)\nplot(x, y_exponential, pch = 19, col = colors,\n     main = paste0(\"r(Spearman) = \", cor_vals4$spearman, \n                   \"\\nr(Pearson) = \", cor_vals4$pearson),\n     xlab = \"x\", ylab = \"y\")\n\n\n\n\n\nEinfluss von Nicht-Linearitäten auf beide Korrelationskoeffizienten\n\n\n\n\n\n\n9.4.2 Vergleichstabelle\n\n\n\n\n\n\n\n\nKriterium\nPearson\nSpearman\n\n\n\n\nArt des Zusammenhangs\n\nMisst lineare Zusammenhänge\n\n\nMisst monotone Zusammenhänge (linear oder nicht-linear)\n\n\n\nAnwendung\n\nHäufig in der Statistik für metrische Variablen\n\n\nIdeal für Rangdaten oder nicht normalverteilte Daten\n\n\n\nVoraussetzungen\n\nNormalverteilung der Variablen\nLinearität\n\n\nKeine Normalverteilung erforderlich\nMonotone Beziehung erforderlich\n\n\n\nDatentypen\n\nMetrische (intervall- oder verhältnisskalierte) Daten\n\n\nOrdinal-, Intervall- und Verhältnisskalen\n\n\n\nSensitivität gegenüber Ausreissern\n\nSehr empfindlich gegenüber Ausreissern\n\n\nRobust gegenüber Ausreissern\n\n\n\nRobustheit bei nicht-linearen Zusammenhängen\n\nNicht robust bei nicht-linearen Zusammenhängen\n\n\nRobust bei nicht-linearen, aber monotonen Zusammenhängen\n\n\n\nSkalenniveau\n\nIntervall- oder verhältnisskaliert\n\n\nMindestens ordinalskaliert\n\n\n\nBerechnungsgrundlage\n\nKovarianz, normiert durch Standardabweichung\n\n\nBerechnet auf Basis von Rangdifferenzen\n\n\n\nVorteile\n\nEinfach zu interpretieren\nWeit verbreitet\n\n\nRobust gegenüber Ausreissern\nGeeignet für nicht-lineare monotone Beziehungen\n\n\n\nNachteile\n\nNicht robust gegenüber Ausreissern\nNicht geeignet für nicht-lineare Zusammenhänge\n\n\nWeniger empfindlich bei linearen Zusammenhängen\nInformationsverlust durch Rangkodierung",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Korrelation</span>"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "10  Lineare Regression",
    "section": "",
    "text": "10.0.1 Einfache lineare Regression\nZiel:\nVorhersage einer metrischen abhängigen Variable (auch Zielvariable oder Kriterium) durch eine oder mehrere unabhängige Variablen (auch Prädiktoren oder Einflussgrössen).\nVoraussetzungen:\nBei einer Regressionsanalyse gibt es eine abhängige Variable (\\(y\\)), die erklärt werden soll, und eine oder mehrere unabhängige Variablen (\\(x_1, x_2, \\dots, x_k\\)), die mit der Zielvariable in Verbindung stehen.\nDie allgemeine Form der multiplen linearen Regression lautet:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + \\varepsilon\n\\]\nInterpretation der Regressionskoeffizienten\nWir wollen eine Gerade der Form:\n\\[\n\\hat{y} = \\beta_0 + \\beta_{1x} x\n\\]\nDie Parameter \\(\\beta_0\\) und \\(\\beta_1\\) bestimmen die Lage und Neigung der Regressionsgerade.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>**Lineare Regression**</span>"
    ]
  },
  {
    "objectID": "regression.html#bestimmung-der-modellgüte",
    "href": "regression.html#bestimmung-der-modellgüte",
    "title": "10  Lineare Regression",
    "section": "10.1 Bestimmung der Modellgüte",
    "text": "10.1 Bestimmung der Modellgüte\nDie Güte der Anpassung wird durch das Bestimmtheitsmass \\(R^2\\) beurteilt. Dieses Mass gibt an, welcher Anteil der Gesamtvariation von \\(Y\\) durch das Modell erklärt wird:\n\\[\nR^2 = \\frac{\\text{erklärte Variation}}{\\text{Gesamtvariation}} = 1 - \\frac{\\text{Residuenvariation}}{\\text{Gesamtvariation}}\n\\]\nDas bedeutet:\n\\[\nR^2 = 1 - \\frac{\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}\n\\]\n\n\\(Y_i\\) ist der tatsächliche Wert der abhängigen Variablen.\n\n\\(\\hat{Y}_i\\) ist der vorhergesagte Wert der abhängigen Variablen.\n\n\\(\\bar{Y}\\) ist der Mittelwert von \\(Y\\).\n\nDer Zähler beschreibt die Residuenvariation (nicht erklärte Abweichung).\n\nDer Nenner beschreibt die Gesamtvariation (Gesamtstreuung der Daten).\n\nEigenschaften von \\(R^2\\):\n\n\\(R^2\\) liegt immer zwischen 0 und 1.\n\n\\(R^2 = 1\\) bedeutet perfekte Anpassung (alle Datenpunkte liegen exakt auf der Regressionsgeraden).\n\n\\(R^2 = 0\\) bedeutet keine Anpassung (die unabhängige Variable erklärt nichts).\n\n\n10.1.1 Testen der Signifikanz der Regressionskoeffizienten\nUm zu überprüfen, ob die unabhängige Variable tatsächlich einen signifikanten Einfluss auf \\(Y\\) hat, testen wir die Hypothesen:\n\nNullhypothese \\(H_0\\): Kein linearer Zusammenhang \\(\\Rightarrow \\beta_1 = 0\\).\n\nAlternativhypothese \\(H_1\\): Es gibt einen linearen Zusammenhang \\(\\Rightarrow \\beta_1 \\neq 0\\).\n\nDa wir \\(\\beta_1\\) nicht direkt kennen, schätzen wir ihn mit \\(\\widehat{\\beta}_1\\) und testen, ob dieser signifikant von 0 verschieden ist. Dafür berechnen wir die Teststatistik:\n\\[\nT = \\frac{\\widehat{\\beta}_1}{\\operatorname{SE}_{\\beta_1}}\n\\]\nwobei der Standardfehler von \\(\\beta_1\\) durch:\n\\[\n\\operatorname{SE}_{\\beta_1} = \\sqrt{\\frac{\\frac{1}{n-2} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}}\n\\]\ngegeben ist.\n\n\\(T\\) folgt einer t-Verteilung mit \\(n - 2\\) Freiheitsgraden, da wir die beiden Regressionskoeffizienten (\\(\\beta_0, \\beta_1\\)) aus der Stichprobe schätzen.\n\nWenn \\(|T|\\) gross genug ist, lehnen wir \\(H_0\\) ab → \\(X\\) hat einen signifikanten Einfluss auf \\(Y\\).\n\nInterpretation des Tests:\n\nFalls der p-Wert kleiner als \\(0.05\\) ist, lehnen wir \\(H_0\\) ab. Es gibt einen signifikanten linearen Zusammenhang.\n\nFalls der p-Wert grösser als \\(0.05\\) ist, können wir keine signifikante Beziehung feststellen.\n\n\n\n\n\n\n\nBeispiel\n\n\n\nWir überprüfen die Signifikanz der Regressionskoeffizienten für das Beispiel der Lernzeit und der Prüfungsnote.\n1. Berechnung der Gesamtvariation \\(SS_{total}\\)\nDie Gesamtvariation ist die Summe der quadrierten Abweichungen jedes \\(Y_i\\) vom Mittelwert \\(\\bar{Y}\\):\n\\[\n\\begin{aligned}\nSS_{\\text{total}} &= \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2 \\\\\n&= \\sum_{i=1}^{n} (70 - 70)^2 + (80 - 70)^2 + (50 - 70)^2 + (90 - 70)^2 + (60 - 70)^2 \\\\\n&= 0 + 100 + 400 + 400 + 100 \\\\\n&= 1000\n\\end{aligned}\n\\]\n\nss_total &lt;- sum((noten_lernzeit_tabelle$note - mean(noten_lernzeit_tabelle$note))^2)\nss_total\n\n[1] 1000\n\n\n2. Berechnung der Residuenvariation \\(SS_{residual}\\)\nDie Residuenvariation ist die Summe der quadrierten Abweichungen der tatsächlichen Werte \\(Y_i\\) von den vorhergesagten Werten \\(\\hat{Y}_i\\), berechnet mit der Regressionsgleichung:\n\\[\n\\begin{aligned}\n\\hat{Y}_i &= 28.6 + 4.14 X_i \\\\\n&= 0^2 + 1.7^2 + 0.7^2 + (-0.7)^2 + (-1.7)^2 \\\\\n&\\approx 0 + 2.89 + 0.49 + 0.49 + 2.89 \\\\\n&\\approx 6.9\n\\end{aligned}\n\\]\n\nss_residual &lt;- sum(model$residuals^2)\nss_residual\n\n[1] 6.896552\n\n\n\n10.1.2 3. Berechnung des Bestimmtheitsmasses \\(R^2\\)\nNun setzen wir die Werte in die Formel ein:\n\\[\nR^2 = 1 - \\frac{SS_{\\text{residual}}}{SS_{\\text{total}}}\n\\]\n\nr_squared &lt;- summary(model)$r.squared\nr_squared\n\n[1] 0.9931034\n\n\n\\[\n\\begin{aligned}\nR^2 &\\approx 1 - \\frac{6.9}{1000} \\\\\n&\\approx 1 - 0.0069 \\\\\n&\\approx 0.993\n\\end{aligned}\n\\]\n\\(\\Rightarrow\\) 99.3% der Variation in den Prüfungsnoten wird durch die Lernzeit erklärt.\nDas Modell passt also sehr gut zu den Daten.\n4. Berechnung des Standardfehlers von \\(\\beta_1\\)\n\\[\n\\begin{aligned}\n\\operatorname{SE}_{\\beta_1} &= \\sqrt{\\frac{\\frac{1}{n-2} \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^{n} (X_i - \\bar{X})^2}} \\\\\n&= \\sqrt{\\frac{\\frac{1}{3} \\sum_{i=1}^{5} (Y_i - \\hat{Y}_i)^2}{\\sum_{i=1}^{5} (X_i - \\bar{X})^2}} \\\\\n&= \\sqrt{\\frac{\\frac{1}{3} \\left[(70-70)^2 + (80-78.3)^2 + (50-49.3)^2 + (90-90.7)^2 + (60-61.7)^2\\right]}{\\left[(10-10)^2 + (12-10)^2 + (5-10)^2 + (15-10)^2 + (8-10)^2\\right]}} \\\\\n&= \\sqrt{\\frac{\\frac{1}{3} \\left[0^2 + 1.7^2 + 0.7^2 + (-0.7)^2 + (-1.7)^2\\right]}{\\left[0^2 + 2^2 + (-5)^2 + 5^2 + (-2)^2\\right]}} \\\\\n&= \\sqrt{\\frac{\\frac{1}{3} \\left[0 + 2.89 + 0.49 + 0.49 + 2.89\\right]}{\\left[0 + 4 + 25 + 25 + 4\\right]}} \\\\\n&= \\sqrt{\\frac{\\frac{1}{3} \\left[6.76\\right]}{58}} \\\\\n&\\approx \\sqrt{\\frac{2.53}{58}} \\\\\n&\\approx \\sqrt{0.039} \\\\\n&\\approx 0.197\n\\end{aligned}\n\\]\n\nse_beta1 &lt;- summary(model)$coefficients[2, \"Std. Error\"]\nse_beta1\n\n[1] 0.1990863\n\n\n5. Berechnung der Teststatistik \\(T\\)\n\\[\n\\begin{aligned}\nT &= \\frac{\\widehat{\\beta}_1}{\\operatorname{SE}_{\\beta_1}} \\\\\n&= \\frac{4.14}{0.197} \\\\\n&\\approx 20.79\n\\end{aligned}\n\\]\n\nt_value &lt;- coef(summary(model))[2, \"t value\"]\nt_value\n\n[1] 20.78461\n\n\n6. Berechnung des p-Werts\nDa die Teststatistik \\(T\\) einer t-Verteilung mit \\(n-2\\) Freiheitsgraden folgt, berechnen wir den p-Wert für den zweiseitigen Test:\n\\[\np = 2 \\cdot P(T \\geq |t_{\\text{berechnet}}|)\n\\]\n\np_value &lt;- summary(model)$coefficients[2, \"Pr(&gt;|t|)\"]\np_value\n\n[1] 0.0002435779\n\n\nInterpretation:\n\nDer berechnete p-Wert ist sehr klein \\((p \\approx 0.0002)\\).\nDa \\(p &lt; 0.05\\), können wir die Nullhypothese \\(H_0\\) ablehnen.\nDas bedeutet: Die Lernzeit hat einen signifikanten Einfluss auf die Prüfungsnote.\nDa der p-Wert sogar weit unter 0.001 liegt, ist der Zusammenhang hochsignifikant.\n\nFazit:\nDie Regressionskoeffizienten sind signifikant. Es gibt starke Evidenz, dass eine höhere Lernzeit zu besseren Prüfungsnoten führt.\n\n\n\n\n10.1.2.1 Konfidenzintervall der Regressionskoeffizienten\n\\[\n\\beta_1 = \\widehat{\\beta_1} \\pm q_t \\cdot \\operatorname{SE}_{\\beta_1} \\quad \\text{mit} \\quad q_t \\text{ aus der T-Tabelle}\n\\]\n\n\\(t_{n-2}\\) ist der kritische Wert der t-Verteilung mit \\(n-2\\) Freiheitsgraden\n\\(\\operatorname{SE}_{\\beta_1}\\) ist der Standardfehler der Steigung\n\\(\\widehat{\\beta_1}\\) ist der geschätzte Regressionskoeffizient der Stichprobe\n\n\n\n\n10.1.3 Berechnung der linearen Regression in R\n\n# Lineare Regression durchführen\nmodel &lt;- lm(note ~ lernzeit, data = noten_lernzeit_tabelle)\nmodel\n\n\nCall:\nlm(formula = note ~ lernzeit, data = noten_lernzeit_tabelle)\n\nCoefficients:\n(Intercept)     lernzeit  \n     28.621        4.138  \n\n\nDie Ausgabe der Funktion lm() zeigt uns:\n\nCall: Die verwendete Formel für die Regression\n\ny ~ x bedeutet: y wird durch x vorhergesagt\n\nCoefficients: Die geschätzten Regressionskoeffizienten\n\n(Intercept): \\(\\widehat{\\beta_0}\\) = 28.621\n\nDies ist der y-Achsenabschnitt\nDer vorhergesagte y-Wert, wenn x = 0\n\nx: \\(\\widehat{\\beta_1}\\) = 4.138\n\nDies ist die Steigung der Geraden\nFür jede Einheit, die x zunimmt, steigt y um 4.138 Einheiten\n\n\n\nFür eine detailliertere Analyse können wir die Funktion summary() verwenden:\n\nsummary(model)\n\n\nCall:\nlm(formula = note ~ lernzeit, data = noten_lernzeit_tabelle)\n\nResiduals:\n         1          2          3          4          5 \n-1.220e-14  1.724e+00  6.897e-01 -6.897e-01 -1.724e+00 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  28.6207     2.1032   13.61 0.000858 ***\nlernzeit      4.1379     0.1991   20.79 0.000244 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.516 on 3 degrees of freedom\nMultiple R-squared:  0.9931,    Adjusted R-squared:  0.9908 \nF-statistic:   432 on 1 and 3 DF,  p-value: 0.0002436\n\n\nDie summary() zeigt uns zusätzlich:\n\nResiduals: Verteilung der Abweichungen zwischen vorhergesagten und tatsächlichen Werten\n\nMinimum: -1.724\nMaximum: 1.724\nDie Quartile zeigen, wie die Residuen verteilt sind\nIdealerweise symmetrisch um 0\n\nCoefficients-Tabelle:\n\nIntercept (\\(\\widehat{\\beta_0}\\) = 28.621):\n\nStandardfehler: 2.103\nt-Wert: 13.608\np-Wert: 0.001\nSignifikant auf dem 0.1% Niveau\n\nSteigung (\\(\\widehat{\\beta_1}\\) = 4.138):\n\nStandardfehler: 0.199\nt-Wert: 20.785\np-Wert: 0\nSignifikant auf dem 0.1% Niveau\n\n\nModellgüte:\n\n\\(R^2\\) = 0.993\n\n99.3% der Varianz in y wird durch x erklärt\n\nAdjustiertes \\(R^2\\) = 0.991\n\nBerücksichtigt die Anzahl der Prädiktoren\n\n\nF-Test:\n\nF-Wert: 432\nFreiheitsgrade: 1 und 3\np-Wert: 2.4^{-4}\nDas Modell ist statistisch signifikant",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>**Lineare Regression**</span>"
    ]
  },
  {
    "objectID": "regression.html#multiple-regression",
    "href": "regression.html#multiple-regression",
    "title": "10  Lineare Regression",
    "section": "10.2 Multiple Regression",
    "text": "10.2 Multiple Regression\nIn der einfachen linearen Regression versuchen wir, den Zusammenhang zwischen einer abhängigen Variable \\(Y\\) und einem Prädiktor \\(X_1\\) zu modellieren. Doch was passiert, wenn \\(Y\\) nicht vollständig durch \\(X_1\\) alleine erklärt werden kann?\nStellen wir uns vor, wir haben Daten, bei denen wir vermuten, dass \\(X_1\\) einen Einfluss auf \\(Y\\) hat. Wir beginnen mit einer einfachen linearen Regression:\n\n\nCode\nset.seed(42)\n# Daten simulieren\nn &lt;- 100\nX1 &lt;- rnorm(n, mean = 10, sd = 2)\nX2 &lt;- rnorm(n, mean = 5, sd = 1.5)\nY &lt;- 3 * X1 + 2 * X2 + rnorm(n, sd = 3)\n\n# Einfache lineare Regression Y ~ X1\nmodel_X1 &lt;- lm(Y ~ X1)\n\n# Plot\nplot(X1, Y, pch = 19, col = rgb(105/255, 89/255, 205/255, alpha = 0.5), \n     xlab = \"X1\", ylab = \"Y\")\nabline(model_X1, col = \"red\", lwd = 2)\n\n\n\n\n\nErste Regression: Y in Abhängigkeit von X1\n\n\n\n\nWir erkennen, dass \\(X_1\\) einen deutlichen Einfluss auf \\(Y\\) hat. Doch die Vorhersagen des Modells sind nicht perfekt – es bleiben Residuen übrig, also Abweichungen zwischen den tatsächlichen Werten von \\(Y\\) und den durch das Modell prognostizierten Werten.\nDiese Residuen sind nicht einfach nur zufälliges Rauschen. Sie könnten Hinweise darauf liefern, dass noch weitere Faktoren im Spiel sind, die wir bisher nicht berücksichtigt haben.\nUm das zu überprüfen, untersuchen wir, ob ein weiterer Prädiktor \\(X_2\\) möglicherweise einen Teil dieser unerklärten Varianz in \\(Y\\) aufklären kann. Dazu betrachten wir die Residuen der ersten Regression und analysieren, ob sie mit \\(X_2\\) zusammenhängen:\n\n\nCode\n# Berechne die Residuen der ersten Regression\nresiduals_X1 &lt;- resid(model_X1)\n\n# Regression der Residuen auf X2\nmodel_resid_X2 &lt;- lm(residuals_X1 ~ X2)\n\n# Plot\nplot(X2, residuals_X1, pch = 19, col = rgb(105/255, 89/255, 205/255, alpha = 0.5), \n     xlab = \"X2\", ylab = \"Residuen von Y ~ X1\")\nabline(model_resid_X2, col = \"orange\", lwd = 2)\n\n\n\n\n\nZweite Regression: Residuen von Y ~ X1 in Abhängigkeit von X2\n\n\n\n\nWir sehen, dass die Residuen tatsächlich einen Zusammenhang mit \\(X_2\\) aufweisen. Das bedeutet, dass \\(X_2\\) Varianz in \\(Y\\) erklärt, die nicht durch \\(X_1\\) erfasst wurde.\nMan könnte diesen Prozess theoretisch weiterführen: Nachdem wir den Einfluss von \\(X_2\\) modelliert haben, könnten wir die neuen Residuen betrachten und versuchen, diese durch einen weiteren Prädiktor \\(X_3\\) zu erklären. Und so weiter.\nDieses schrittweise Vorgehen wirft jedoch ein Problem auf: Was passiert, wenn \\(X_1\\), \\(X_2\\), …, \\(X_k\\) miteinander korrelieren?\n\nIn diesem Fall ist es schwierig, die individuellen Effekte der einzelnen Prädiktoren zu isolieren.\nDer Einfluss von \\(X_2\\) könnte bereits teilweise in der ersten Regression durch \\(X_1\\) berücksichtigt worden sein – und umgekehrt.\nDurch das schrittweise Vorgehen riskieren wir, Doppelerklärungen oder verzerrte Effekte zu erhalten.\n\nWir brauchen einen Ansatz, der es uns ermöglicht, den Einfluss mehrerer Prädiktoren gleichzeitig zu berücksichtigen.\n\n\n\n\n\n\nBeispiel\n\n\n\n\n\nWir versuchen, den Abfluss eines Gebirgsbachs zu modellieren.\n\n\\(Y\\): Abfluss\n\\(X_1\\): Schneeschmelze\n\\(X_2\\): Niederschlag\n\nWenn wir den Abfluss \\(Y\\) zunächst in Abhängigkeit von der Schneeschmelze \\(X_1\\) modellieren, stellen wir fest, dass ein Teil der Varianz von \\(Y\\) nicht erklärt wird. Wir vermuten, dass der Niederschlag \\(X_2\\) einen zusätzlichen Einfluss haben könnte. Also modellieren wir die Residuen aus der ersten Regression in Abhängigkeit von \\(X_2\\).\nDoch hier entsteht ein Problem: Schneeschmelze und Niederschlag sind oft korreliert. Nach starken Niederschlägen folgt häufig eine beschleunigte Schneeschmelze. Wenn wir \\(X_2\\) nur auf die Residuen von \\(X_1\\) anwenden, übersehen wir möglicherweise den gemeinsamen Einfluss beider Faktoren.\nDas führt zu verzerrten Ergebnissen, da der Niederschlag sowohl einen direkten Einfluss auf den Abfluss hat als auch indirekt über die Schneeschmelze wirkt.\n\n\n\nBemerkung: Wenn die Prädiktoren nicht korrelieren, ist die Regression der Residuen mit weiteren Variablen möglich.\n\n10.2.1 Ziel:\nVorhersage einer metrischen abhängigen Variable durch mehrere unabhängige Variablen (Prädiktoren).\nVoraussetzungen:\n\nEin statistischer Zusammenhang zwischen den Prädiktoren und der Zielvariable sollte plausibel sein.\n\nEin Modell, das die Abhängigkeit der Zielvariable von den Prädiktoren beschreibt.\n\nDie allgemeine Form der multiplen linearen Regression lautet:\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k + \\varepsilon\n\\]\n\n\\(Y\\) ist die abhängige Variable.\n\n\\(X_1, X_2, \\ldots, X_k\\) sind die unabhängigen Variablen (Prädiktoren).\n\n\\(\\beta_0\\) ist die Regressionskonstante (Achsenabschnitt).\n\n\\(\\beta_1, \\beta_2, \\ldots, \\beta_k\\) sind die Regressionskoeffizienten, die den Einfluss der jeweiligen Prädiktoren auf \\(Y\\) beschreiben.\n\n\\(\\varepsilon\\) ist der Fehlerterm, der alle nicht erfassten Einflüsse berücksichtigt.\n\nInterpretation der Regressionskoeffizienten\n\nWenn \\(X_i\\) um eine Einheit steigt, verändert sich \\(Y\\) um \\(\\beta_i\\), unter der Annahme, dass alle anderen Prädiktoren konstant bleiben.\n\nBeispiel: Wenn \\(\\beta_1 = 2\\), dann bedeutet das, dass eine Erhöhung von \\(X_1\\) um eine Stunde Lernzeit zu einer durchschnittlichen Erhöhung der Prüfungsnote um 2 Punkte führt.\n\n\n\n10.2.2 Ansatz\n\nMinimierung der Summe der quadrierten Residuen:\n\n\\[\n\\sum_{i=1}^{n} (Y_i - \\widehat{\\beta_0} - \\widehat{\\beta_1} X_{1i} - \\widehat{\\beta_2} X_{2i} - \\ldots - \\widehat{\\beta_k} X_{ki})^2 \\quad \\rightarrow \\quad \\text{minimal}\n\\]\n\nDafür müssen die partiellen Ableitungen nach allen \\(\\beta_j\\) gleich null gesetzt werden\nKoeffizienten der multiplen Regression werden auch “partielle Regressionskoeffizienten” genannt\n\n\\[\n\\widehat{Y_i} = \\widehat{\\beta_0} + \\widehat{\\beta_1} X_{1i} + \\widehat{\\beta_2} X_{2i} + \\ldots + \\widehat{\\beta_k} X_{ki}\n\\]\noder in Matrixnotation:\n\\[\n\\widehat{Y} = X \\widehat{\\beta} \\quad \\text{mit} \\quad \\widehat{Y} = \\begin{bmatrix} \\widehat{Y_1} \\\\ \\widehat{Y_2} \\\\ \\vdots \\\\ \\widehat{Y_n} \\end{bmatrix}, \\quad \\widehat{\\beta} = \\begin{bmatrix} \\widehat{\\beta_0} \\\\ \\widehat{\\beta_1} \\\\ \\vdots \\\\ \\widehat{\\beta_k} \\end{bmatrix} \\quad \\text{und} \\quad X = \\begin{bmatrix} 1 & X_{11} & X_{21} & \\ldots & X_{k1} \\\\ 1 & X_{12} & X_{22} & \\ldots & X_{k2} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & X_{1n} & X_{2n} & \\ldots & X_{kn} \\end{bmatrix}\n\\]\n\n\n\n\n\n\nBeispiel\n\n\n\n\nDa die Berechnung einer multiplen Regression sehr komplex ist, verwenden wir hierfür R und verzichten auf die manuelle Berechnung.\n\nWir verwenden die Daten aus Tabelle 9.1 und ergänzen sie um zwei weitere Prädiktoren: Schlafdauer und Kaffeekonsum.\n\n\n\nTabelle 10.1\n\n\n\n\n\n\n\n\n\n\n\n\nStudent\nLernzeit \\(X_1\\)\nSchlafdauer \\(X_2\\)\nKaffee \\(X_3\\)\nPrüfungsnote \\(Y\\)\n\n\n\n\nA\n10\n7.5\n2\n70\n\n\nB\n12\n6.5\n3\n80\n\n\nC\n5\n6.0\n0\n50\n\n\nD\n15\n8.5\n5\n90\n\n\nE\n8\n8.0\n1\n60\n\n\n\n\n\n\n1. Regressionsmodell aufstellen\nWir schätzen die folgende multiple lineare Regression:\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\varepsilon\n\\]\n\nlibrary(tibble)\n\nnoten_lernzeit_schlaf_kaffee &lt;- tibble(\n    student = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n    lernzeit = c(10, 12, 5, 15, 8),\n    schlafdauer = c(7.5, 6.5, 6.0, 8.5, 8.0),\n    kaffee = c(2, 3, 0, 5, 1),\n    note = c(70, 80, 50, 90, 60)\n)\n\n# Multiple lineare Regression durchführen\nmodel_multiple &lt;- lm(note ~ lernzeit + schlafdauer + kaffee, data = noten_lernzeit_schlaf_kaffee)\n\n# Ergebnisse anzeigen\nsummary(model_multiple)\n\n\nCall:\nlm(formula = note ~ lernzeit + schlafdauer + kaffee, data = noten_lernzeit_schlaf_kaffee)\n\nResiduals:\n      1       2       3       4       5 \n 0.2626 -0.1050  0.0105 -0.0105 -0.1576 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)  36.8067     1.9959  18.441   0.0345 *\nlernzeit      4.5273     0.3028  14.951   0.0425 *\nschlafdauer  -1.5756     0.1960  -8.041   0.0788 .\nkaffee       -0.2626     0.5926  -0.443   0.7345  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3241 on 1 degrees of freedom\nMultiple R-squared:  0.9999,    Adjusted R-squared:  0.9996 \nF-statistic:  3173 on 3 and 1 DF,  p-value: 0.01305\n\n\n2. Berechnung der Regressionskoeffizienten\nDie geschätzte Regressionsgleichung lautet:\n\\[\n\\begin{aligned}\n\\hat{Y} &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 \\\\\n       &\\approx 36.8 + 4.53 X_1 + (-1.58) X_2 + (- 0.26) X_3\n\\end{aligned}\n\\]\nInterpretation:\n\n\\(\\beta_1\\) (Lernzeit) ist erwartungsgemäss positiv und signifikant (p &lt; 0.05).\n\n\\(\\beta_2\\) (Schlafdauer) zeigt einen leichten negativen Effekt, mit knapp über unserem Signifikanzniveau von 0.05 (p &lt; 0.1)\n\n\\(\\beta_3\\) (Kaffeekonsum) ist leicht negativ, zeigt aber keinen signifikanten Zusammenhang (p &gt; 0.05).\n\n3. Modellgüte und Vorhersagekraft\nDas Bestimmtheitsmaß \\(R^2\\) gibt an, wie gut unser Modell die Prüfungsnoten erklärt:\n\\[\nR^2 = 1 - \\frac{SS_{\\text{residual}}}{SS_{\\text{total}}}\n\\]\n\nsummary(model_multiple)$r.squared\n\n[1] 0.999895\n\n\nInterpretation:\n\nMit \\(R^2 =\\) 1 sehen wir, dass das unser Modell äusserst gut zu den Daten passt.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>**Lineare Regression**</span>"
    ]
  },
  {
    "objectID": "regression.html#anwendungsbedingungen",
    "href": "regression.html#anwendungsbedingungen",
    "title": "10  Lineare Regression",
    "section": "10.3 Anwendungsbedingungen",
    "text": "10.3 Anwendungsbedingungen\n\n10.3.1 Lineare Beziehung zwischen den Variablen\nDie lineare Regression setzt voraus, dass der Zusammenhang zwischen der abhängigen Variable (\\(Y\\)) und den Prädiktoren (\\(X_1, X_2, \\dots, X_k\\)) linear ist.\n- In einem Scatterplot sollte eine ungefähr geradlinige Beziehung zwischen den Variablen erkennbar sein.\n- Falls ein nichtlinearer Zusammenhang vorliegt, kann eine Transformation der Variablen (z.B. logarithmisch oder quadratisch) sinnvoll sein.\n\n\n10.3.2 Keine perfekte Multikollinearität\nKollinearität beschreibt die Tatsache, dass zwei oder mehrere Prädiktoren in einem Regressionsmodell stark miteinander korrelieren.\n\nDie unabhängigen Variablen dürfen untereinander nicht perfekt korrelieren (\\(i \\neq j: \\text{corr}(X_i, X_j) = 1\\)) d.h. es darf keine Linearkombination anderer unabhängiger Variablen sein.\nMatrix ist sonst nicht invertierbar.\n\nFolgen:\n\nSchätzungen der Regressionsparameter sind unzuverlässig\nStandardfehler der Regressionskoeffizienten sind gross\nt-Werte sind klein\n\nAnzeichen:\n\nResultate werden stark vom Weglassen einer Beobachtung beeinflusst\nVorzeichen der Regressionskoeffizienten ist anders als erwartet\nHohe Korrelationen der unabhängigen Variablen (\\(&gt; |0.8|\\)) deuten auf mögliche Kollinearität hin\nVarianzinflationsfaktor (VIF) misst die Abhängigkeit der Varianz des geschätzten Regressionskoeffizienten aufgrund der Korrelation zwischen unabhängigen Variablen. VIF-Werte \\(&gt; 10\\) gelten als kritisch.\n\nvif()\n\n\nBehandlung:\n\nWeglassen von korrelierenden Prädiktoren\nNeue Prädiktoren finden\nHauptkomponentenanalyse (PCA): Reduktion der Dimensionalität der Daten durch PCA, um unkorrelierte Hauptkomponenten zu erhalten.\n\n\n\n10.3.3 Heteroskedastizität\nHeteroskedastizität beschreibt die Tatsache, dass die Varianz der Residuen nicht konstant ist. Das bedeutet, dass die Residuen nicht gleichmässig um den Mittelwert streuen, sondern eine zunehmende oder abnehmende Varianz aufweisen.\nUrsachen dafür können sein:\n\nMessfehler werden über die Zeit kleiner\nBefragungen vor und nach dem Lernprozess\nVerhalten ist abhängig vom Einkommen, so dass reichere Personen mehr Wahlmöglichkeiten haben als ärmere\nBei aggregierten Werten sind Klassen mit kleinem \\(n\\) unsicherer und streuen mehr als Klassen mit grossem \\(n\\)\n\nTests:\n\nZerlegung der Daten und Vergleich von Subsets (z.B. Zeitperioden)\nGoldfeld-Quandt-Test (univariate Regression)\nWhite-Test (multiple Regression)\n\nBehandlung:\n\nMethode der gewichteten kleinsten Quadrate. Werte bekommen dort weniger Gewicht, wo die Streuung gross ist.\n\n\n\n10.3.4 Normalverteilte Residuen\nDie Verteilung der Residuen sollte ungefähr normal sein. Dies ist besonders wichtig für:\n\nt-Tests auf die Regressionskoeffizienten\n\nF-Tests zur Bewertung der Modellgüte\n\nKonfidenzintervalle für die \\(\\beta\\)-Koeffizienten\n\nEin Q-Q-Plot kann helfen, die Normalverteilung der Residuen zu überprüfen.\nFalls die Residuen nicht normalverteilt sind:\n\nTransformation der abhängigen Variable\n\nBootstrapping-Verfahren zur Schätzung der Koeffizienten\n\n\n\n10.3.5 Keine starken Ausreißer oder einflussreichen Datenpunkte\nEinzelne Datenpunkte mit extremen Werten können die Regression stark beeinflussen.\n\nLeverage-Plot oder Cook’s Distance kann verwendet werden, um einflussreiche Punkte zu identifizieren.\n\nFalls ein Ausreißer zu stark ist, sollte überprüft werden, ob ein Messfehler vorliegt oder ob der Punkt sinnvoll entfernt werden kann.\n\n\n\n10.3.6 Beispielauswertungen\n\n10.3.6.1 Optimale Residuendiagnostik mit geeigneten Daten\n\nCode\nlibrary(ggplot2)\n\n# Synthetische Daten perfekt simulieren\nn &lt;- 500\nX1 &lt;- rnorm(n, mean = 10, sd = 2)    # Normalverteilte Prädiktoren\nX2 &lt;- rnorm(n, mean = 5, sd = 1.5)\n\nX1 &lt;- scale(X1, center = TRUE, scale = FALSE)\nX2 &lt;- scale(X2, center = TRUE, scale = FALSE)\n\n# Perfekte lineare Beziehung\n# Fehler sind normalverteilt mit konstanter Varianz\nerrors &lt;- rnorm(n, mean = 0, sd = 1)  # Normalverteilte Residuen\n\n# Lineares Modell\nY &lt;- 3 * X1 + 2 * X2 + errors\n\n# Lineares Regressionsmodell\nperfektes_modell &lt;- lm(Y ~ X1 + X2)\n\n# Diagnostische Plots\ndot_color &lt;- rgb(0, 0, 1, 0.5)\nplot(perfektes_modell, col = dot_color, pch = 19)\n\n\n\n\n\n\n\nLinearität: Die Residuen streuen zufällig um die Nulllinie. Kein systematisches Muster erkennbar – ein Hinweis auf eine lineare Beziehung.\n\n\n\n\n\n\n\nNormalverteilung der Residuen: Die Punkte liegen nahe der Diagonale im Q-Q-Plot. Dies zeigt, dass die Residuen normalverteilt sind.\n\n\n\n\n\n\n\n\n\nVarianzhomogenität: Die Punkte im Scale-Location-Plot sind gleichmässig verteilt, ohne Trichterform. Das deutet auf konstante Varianz (Homoskedastizität) hin.\n\n\n\n\n\n\n\nAusreisser: Im Leverage-Plot gibt es keine Punkte mit hoher Cook’s Distance. Dies zeigt, dass es keine einflussreichen Ausreisser gibt.\n\n\n\n\n\n\n\n10.3.6.2 Negative Residuendiagnostik mit schlechten Daten\n\nCode\nset.seed(123)\n\n# Nicht-Linearität\nn &lt;- 500\nX_nl &lt;- rnorm(n, mean = 0, sd = 1)\nY_nl &lt;- 2 * X_nl^2 + rnorm(n, 0, 1)\nmodell_nl &lt;- lm(Y_nl ~ X_nl)  # Falsch spezifiziert (linear)\n\n# Nicht-normalverteilte Residuen\nX_nn &lt;- rnorm(n)\nY_nn &lt;- 3 * X_nn + rexp(n, rate = 1)  # Exponentiell verteilte Fehler\nmodell_nn &lt;- lm(Y_nn ~ X_nn)\n\n# Heteroskedastizität\nX_het &lt;- rnorm(n)\nY_het &lt;- 4 * X_het + rnorm(n, 0, sd = abs(X_het) * 2)\nmodell_het &lt;- lm(Y_het ~ X_het)\n\n# Ausreisser\nX_out &lt;- rnorm(n)\nY_out &lt;- 5 * X_out + rnorm(n, 0, 1)\nY_out[c(50, 100)] &lt;- Y_out[c(50, 100)] + 20  # Ausreisser hinzufügen\nmodell_out &lt;- lm(Y_out ~ X_out)\n\n# Diagnostische Plots (jeweils der relevante)\nplot(modell_nl, which = 1, col = dot_color, pch = 19)   # Nicht-Linearität: Residuals vs Fitted\nplot(modell_nn, which = 2, col = dot_color, pch = 19)   # Nicht-normalverteilte Residuen: Q-Q-Plot\nplot(modell_het, which = 3, col = dot_color, pch = 19)  # Heteroskedastizität: Scale-Location\nplot(modell_out, which = 5, col = dot_color, pch = 19)  # Ausreisser: Residuals vs Leverage\n\n\n\n\n\n\n\nNicht-Linearität: Die Residuen zeigen ein gebogenes Muster. Dies deutet darauf hin, dass das Modell die wahre Beziehung nicht korrekt abbildet.\n\n\n\n\n\n\n\nNicht-Normalverteilte Residuen: Im Q-Q-Plot weichen die Punkte deutlich von der Diagonalen ab. Dies deutet auf eine Verletzung der Normalverteilungsannahme hin.\n\n\n\n\n\n\n\n\n\nHeteroskedastizität: Im Scale-Location-Plot ist ein trichterförmiges Muster zu erkennen. Dies weist auf eine zunehmende Varianz der Residuen hin.\n\n\n\n\n\n\n\nAusreisser: Im Residuals vs Leverage-Plot sind Punkte mit hoher Cook’s Distance sichtbar. Sie haben einen starken Einfluss auf das Modell.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>**Lineare Regression**</span>"
    ]
  },
  {
    "objectID": "regression.html#modellvalidierung",
    "href": "regression.html#modellvalidierung",
    "title": "10  Lineare Regression",
    "section": "10.4 Modellvalidierung",
    "text": "10.4 Modellvalidierung\n\n10.4.1 Kreuzvalidierung\nDie Kreuzvalidierung ist eine Methode zur Bewertung der Vorhersagegüte eines Regressionsmodells. Dabei wird der Datensatz in mehrere Teilmengen (sogenannte Folds) aufgeteilt. Das Modell wird wiederholt auf verschiedenen Kombinationen von Trainings- und Testdaten geschätzt, um zu überprüfen, wie gut es auf unbekannte Daten generalisiert.\n\n10.4.1.1 Vorgehensweise bei der K-Fold-Kreuzvalidierung:\n\nAufteilung der Daten: Der Datensatz wird in \\(k\\) gleich grosse Teilmengen (Folds) aufgeteilt.\n\nModelltraining: In jeder der \\(k\\) Iterationen wird das Modell mit \\((k-1)\\) Folds trainiert.\n\nModelltest: Der verbliebene Fold dient als Testdatensatz zur Bewertung des Modells.\n\nErgebnissynthese: Die Gütekriterien (z.B. mittlerer quadratischer Fehler (MSE), \\(R^2\\)) werden über alle Iterationen gemittelt.\n\nDiese Methode liefert eine robustere Schätzung der Modellgüte als eine einfache Trainings-Test-Aufteilung.\n\n\n\n\n\n\nBeispiel einer Multiplen Regression\n\n\n\n\n\nIn diesem Beispiel verwenden wir den Datensatz mtcars, um ein multiples lineares Regressionsmodell zu erstellen. Unser Ziel ist es, den Benzinverbrauch (mpg) von Fahrzeugen basierend auf mehreren Einflussgrössen vorherzusagen.\n\nDatensatz und Ziel\n\nDatensatz: mtcars mit 32 Fahrzeugen\n\nZielvariable: mpg (Miles per Gallon, Benzinverbrauch)\n\nPrädiktoren:\n\nwt (Gewicht des Fahrzeugs in 1000 Pfund)\n\nhp (Motorleistung in PS)\n\ncyl (Anzahl Zylinder)\n\n\nWir möchten überprüfen, wie gut diese Variablen den Benzinverbrauch gemeinsam vorhersagen.\n\n\nDurchführung der 5-Fold-Kreuzvalidierung\nFür die 5-Fold-Kreuzvalidierung wird der Datensatz in 5 gleich grosse Teilmengen (Folds) aufgeteilt. In jeder der 5 Iterationen wird das Modell mit 4 Folds trainiert und mit dem verbleibenden Fold getestet. Die Gütekriterien werden über alle Iterationen gemittelt.\n\n# Daten und benötigte Pakete laden\ndata(mtcars)\nlibrary(caret)\n\n# Kreuzvalidierung mit 5 Folds\n1cv_control &lt;- trainControl(method = \"cv\", number = 5)\n\n# Training des multiplen Regressionsmodells mit Kreuzvalidierung\ncv_model &lt;- train(            \n2    mpg ~ wt + hp + cyl,\n3    data = mtcars,\n4    method = \"lm\",\n5    trControl = cv_control\n)\n\n# Ergebnisse anzeigen\ncv_model\n\n\n1\n\ncv_control: Definiert die Kreuzvalidierung mit 5 Folds.\n\n2\n\nmpg ~ wt + hp + cyl: Definiert die abhängige Variable (mpg) und die unabhängigen Variablen (wt, hp, cyl).\n\n3\n\ndata = mtcars: Definiert den Datensatz, der für das Modelltraining verwendet wird.\n\n4\n\nmethod = \"lm\": Definiert das Regressionsmodell als lineare Regression.\n\n5\n\ntrControl = cv_control: Definiert die Kreuzvalidierung mit 5 Folds.\n\n\n\n\n\n\nLinear Regression \n\n32 samples\n 3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (5 fold) \nSummary of sample sizes: 24, 24, 28, 26, 26 \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  2.536794  0.8652719  2.134368\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\n\nErgebnisse der Kreuzvalidierung\nNach der Durchführung der Kreuzvalidierung liefert das Modell folgende Kennwerte:\n\nRMSE (Root Mean Squared Error): 2.537\nDer RMSE misst den durchschnittlichen quadratischen Fehler der Vorhersagen. Ein niedriger Wert deutet auf eine gute Modellanpassung hin.\n\\(R^2\\) (Bestimmtheitsmass): 0.865\nDer \\(R^2\\)-Wert zeigt, wie viel der Varianz von mpg durch die Prädiktoren wt, hp und cyl erklärt werden kann. Werte nahe 1 deuten auf eine hohe Erklärungskraft des Modells hin.\nMAE (Mean Absolute Error): 2.134\nDer MAE misst den durchschnittlichen absoluten Vorhersagefehler. Im Gegensatz zum RMSE ist er weniger empfindlich gegenüber Ausreissern.\n\n\n\nInterpretation der Ergebnisse\nDas Modell zeigt folgende Leistungswerte:\n\nVorhersagegenauigkeit (RMSE): Der RMSE beträgt 2.537.\n\nEin RMSE &lt; 3 deutet auf eine gute Anpassung des Modells hin.\n\nWerte &gt; 5 würden auf eine ungenügende Modellanpassung hindeuten.\n\nIn diesem Fall ist der Wert zufriedenstellend.\n\nErklärte Varianz (\\(R^2\\)): Der \\(R^2\\)-Wert beträgt 0.865.\n\nEin \\(R^2\\) &gt; 0.7 gilt als sehr gut, da mehr als 70 % der Varianz von mpg durch die Prädiktoren erklärt wird.\n\nEin Wert &lt; 0.5 würde darauf hindeuten, dass das Modell wichtige Prädiktoren vermissen könnte.\n\nHier zeigt der Wert eine starke Erklärungskraft.\n\nDurchschnittlicher Fehler (MAE): Der MAE beträgt 2.134.\n\nEin MAE &lt; 3 deutet darauf hin, dass die durchschnittlichen Vorhersagefehler gering sind.\n\nWerte &gt; 5 könnten auf systematische Fehler im Modell hinweisen.\n\nIn unserem Fall ist der Fehler akzeptabel.\n\n\n\n\nFazit\nDas multiple Regressionsmodell erklärt einen grossen Anteil der Varianz von mpg und liefert eine präzise Vorhersage. Die Kreuzvalidierung zeigt, dass das Modell auch bei unbekannten Daten stabile Ergebnisse liefert. Eventuelle Optimierungen könnten durch die Einbeziehung weiterer Prädiktoren oder Interaktionsterme erreicht werden.\n\n\n\n\n\n\n\n10.4.2 F-Test\nDer F-Test ist ein Hypothesentest, der die Güte des Regressionsmodells als Ganzes überprüft.\n\\[\nF = \\frac{\\frac{R^2}{k}}{\\frac{1-R^2}{n-(k+1)}} = \\frac{\\text{erklärte Varianz}}{\\text{unerklärte Varianz}}\n\\]\n\n\\(R^2\\) ist das multiple Bestimmtheitsmass\n\\(k\\) ist die Anzahl der unabhängigen Variablen\n\\(n\\) ist die Anzahl der Beobachtungen\n\nDer F-Wert sagt, ob das Modell besser ist als einfach die Annahme des Mittelwerts von \\(Y\\) zu nehmen. D.h. ob\n\\[\nH_0 : R^2 = 0\n\\]\nabgelehnt werden kann.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>**Lineare Regression**</span>"
    ]
  },
  {
    "objectID": "fehlermasse.html",
    "href": "fehlermasse.html",
    "title": "11  Fehlermasse",
    "section": "",
    "text": "11.1 Bias\nCode\nimport matplotlib.pyplot as plt\n\n# Funktion zum Zeichnen eines \"Zielscheiben\"-Diagramms mit Punkten\ndef draw_target(ax, points, title, label_y=None):\n    # Zeichnen der Zielscheibe (zwei konzentrische Kreise)\n    target = plt.Circle((0, 0), 1, color='black', fill=False, lw=1.5)\n    inner_circle = plt.Circle((0, 0), 0.3, color='black', fill=False, lw=1.5)\n    ax.add_artist(target)\n    ax.add_artist(inner_circle)\n\n    # Punkte hinzufügen\n    for (x, y) in points:\n        ax.plot(x, y, 'ko', markersize=8)\n\n    # Formatierung\n    ax.set_xlim(-1.5, 1.5)\n    ax.set_ylim(-1.5, 1.5)\n    ax.set_aspect('equal')\n    ax.axis('off')\n    ax.set_title(title, fontsize=12)\n\n    if label_y:\n        ax.text(-2.5, 0, label_y, va='center', ha='center', rotation=90, fontsize=12, fontweight='bold')\n\n# Punkte für jede der vier Kategorien\npoints_precise_biased = [(0.8, 0.8), (0.85, 0.75), (0.75, 0.85), (0.8, 0.9)]\npoints_imprecise_biased = [(1.1, 1.1), (0.9, 0.7), (1.2, 0.8), (0.7, 1.2)]\npoints_precise_unbiased = [(0.05, 0.05), (-0.05, 0.05), (0.05, -0.05), (-0.05, -0.05)]\npoints_imprecise_unbiased = [(-1, 1), (1, -1), (-1, -1), (1, 1)]\n\n# Erstellen des 2x2 Plots\nfig, axs = plt.subplots(2, 2, figsize=(8, 8))\n\n# Obere Reihe (biased)\ndraw_target(axs[0, 0], points_precise_biased, \"precise\", label_y=\"biased\")\ndraw_target(axs[0, 1], points_imprecise_biased, \"imprecise\")\n\n# Untere Reihe (unbiased)\ndraw_target(axs[1, 0], points_precise_unbiased, \"accurate\", label_y=\"unbiased\")\ndraw_target(axs[1, 1], points_imprecise_unbiased, \"inaccurate\")\n\n# Layout anpassen\nplt.subplots_adjust(wspace=0.5, hspace=0.5)\n\nplt.show()\n\n\n\n\n\nUnterschiedliche Bias-Arten",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fehlermasse</span>"
    ]
  },
  {
    "objectID": "fehlermasse.html#bias",
    "href": "fehlermasse.html#bias",
    "title": "11  Fehlermasse",
    "section": "",
    "text": "\\(\\text{Bias}_{\\text{additiv}} = \\bar{\\text{Vorhersage}} - \\bar{\\text{Beobachtung}}\\)\n\\(\\text{Bias}_{\\text{multiplikativ}} = \\frac{\\bar{\\text{Vorhersage}}}{\\bar{\\text{Beobachtung}}}\\)",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fehlermasse</span>"
    ]
  },
  {
    "objectID": "fehlermasse.html#mittlerer-absoluter-fehler-mae",
    "href": "fehlermasse.html#mittlerer-absoluter-fehler-mae",
    "title": "11  Fehlermasse",
    "section": "11.2 Mittlerer absoluter Fehler (MAE)",
    "text": "11.2 Mittlerer absoluter Fehler (MAE)\n\\[\n\\begin{align*}\n\\operatorname{MAE} &= \\frac{1}{N} \\sum_{i=1}^{N} |\\hat{y}_i - y_i| \\\\\n\\operatorname{MAE} &= \\frac{1}{N} \\sum_{i=1}^{N} |\\text{Vorhersage}_i - \\text{Beobachtung}_i|\n\\end{align*}\n\\]",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fehlermasse</span>"
    ]
  },
  {
    "objectID": "fehlermasse.html#mittlerer-quadratischer-fehler-mse",
    "href": "fehlermasse.html#mittlerer-quadratischer-fehler-mse",
    "title": "11  Fehlermasse",
    "section": "11.3 Mittlerer quadratischer Fehler (MSE)",
    "text": "11.3 Mittlerer quadratischer Fehler (MSE)\n\\[\n\\begin{align*}\n\\operatorname{MSE} &= \\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2 \\\\\n\\operatorname{MSE} &= \\frac{1}{N} \\sum_{i=1}^{N} (\\text{Vorhersage}_i - \\text{Beobachtung}_i)^2\n\\end{align*}\n\\]\noft wird noch die Wurzel gezogen, um wieder die gleiche Einheit wie die Beobachtung zu erhalten:\n\\[\n\\operatorname{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (\\hat{y}_i - y_i)^2}\n\\]\nHier bekommen grosse Abweichungen mehr gewicht, als beim MAE.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Fehlermasse</span>"
    ]
  },
  {
    "objectID": "clusteranalyse.html",
    "href": "clusteranalyse.html",
    "title": "12  Clusteranalyse",
    "section": "",
    "text": "12.1 Schwellenwertanalyse\nEine Clusteranalyse klassifiziert Bezugs/Raumeinheiten auf der Grundlage von Ähnlichkeitsmassen (z.B. Klimatypen, Stadttypen, etc.).\nEine einfachste Methode zur Klassifizierung ist die Schwellenwertanalyse. Diese erfordert theoretische oder empirische Kenntnisse über die zu klassifizierenden Objekte und nutzt subjektie Schwellenwerte.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Clusteranalyse</span>"
    ]
  },
  {
    "objectID": "clusteranalyse.html#nicht-hierarchische-verfahren",
    "href": "clusteranalyse.html#nicht-hierarchische-verfahren",
    "title": "12  Clusteranalyse",
    "section": "12.2 Nicht-hierarchische Verfahren",
    "text": "12.2 Nicht-hierarchische Verfahren\n\nWerte so lange immer wieder auf die Cluster verteilt, bis die Summe der Abstände von zugehörigen Cluster-Mitten minimal ist\nVorteil: Werte werden flexibel auf Cluster verteilt\nNachteil: Anzahl der Cluster muss a-priori vorgegeben werden\n\n\n12.2.1 k-Means\n\nInitialisierung: \\(k\\) Clusterzentren auf \\(k\\) zufällige, aber unterschiedliche Positionen im \\(p\\)-dimensionalen Raum setzen, möglichst so dass die Abstände zwischen initialen Clusterzentren maximal sind. Jedem Clusterzentrum wird eine eindeutige Klassennummer (1 bis \\(k\\)) zugewiesen.\nKlassifizierung: Finde für jeden Datenpunkt das nächste Clusterzentrum und weise dem Datenpunkt die Klassennummer dieses Clusterzentrums zu.\nClusterzentren berechnen: Berechne die Position der Clusterzentren neu, in dem alle Datenpunkte die zu einer bestimmten Klasse gehören gemittelt werden.\nIteration: Wiederholung ab Schritt 2, bis die Klassifizierung stabil ist\n\n\n\nCode\nlibrary(ggplot2)\n\n# Synthetische Daten erzeugen\nset.seed(123)\nn &lt;- 150\ndata_kmeans &lt;- data.frame(\n  x = c(rnorm(n, mean = 2), rnorm(n, mean = 6), rnorm(n, mean = 4)),\n  y = c(rnorm(n, mean = 3), rnorm(n, mean = 7), rnorm(n, mean = 5))\n)\n\n# K-Means Clustering\nkmeans_result &lt;- kmeans(data_kmeans, centers = 3)\ndata_kmeans$cluster &lt;- as.factor(kmeans_result$cluster)\n\n# Plot für K-Means Clustering\nggplot(data_kmeans, aes(x = x, y = y, color = cluster)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_point(data = as.data.frame(kmeans_result$centers), \n             aes(x = x, y = y), \n             color = \"black\", \n             size = 4, \n             shape = 8) +  # Zentren als Sterne\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\nK-Means-Clustering mit synthetischen Daten",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Clusteranalyse</span>"
    ]
  },
  {
    "objectID": "clusteranalyse.html#hierarchisches-clustering",
    "href": "clusteranalyse.html#hierarchisches-clustering",
    "title": "12  Clusteranalyse",
    "section": "12.3 Hierarchisches Clustering",
    "text": "12.3 Hierarchisches Clustering\n\niterative Vorgehensweise, bei der die Cluster des letzten Schrittes immer weiter zusammengefasst werden\nNachteil: Einmal klassifizierte Werte verbleiben in dem Cluster auch wenn sich die Eigenschaften des Cluster im Laufe der Verfahrensschritte verändern\nVorteil: Anzahl der Cluster muss nicht a-priori vorgegeben werden\n\n\n\nCode\nlibrary(ggdendro)\n\nn &lt;- 50\n\n# Synthetische Daten erzeugen\ndata_hclust &lt;- data.frame(\n  x = c(rnorm(n, mean = 1), rnorm(n, mean = 5), rnorm(n, mean = 3)),\n  y = c(rnorm(n, mean = 2), rnorm(n, mean = 6), rnorm(n, mean = 4))\n)\n\n# Hierarchisches Clustering\ndist_matrix &lt;- dist(data_hclust)\nhclust_result &lt;- hclust(dist_matrix)\n\n# Dendrogramm-Daten für ggplot\ndend_data &lt;- dendro_data(hclust_result, type = \"rectangle\")\n\n# Plot für das Dendrogramm\nggplot() +\n  geom_segment(data = dend_data$segments, \n               aes(x = x, y = y, xend = xend, yend = yend), \n               color = \"black\") +\n  theme_minimal() +\n  labs(title = \"Hierarchisches Clustering (Dendrogramm)\",\n       x = \"\", y = \"Distanz\") +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(),\n        panel.grid = element_blank())\n\n\n\n\n\nDendrogramm mit synthetischen Daten",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Clusteranalyse</span>"
    ]
  },
  {
    "objectID": "hauptkomponentenanalyse.html",
    "href": "hauptkomponentenanalyse.html",
    "title": "13  Hauptkomponentenanalyse",
    "section": "",
    "text": "13.1 Grundidee\nDie Hauptkomponentenanalyse (PCA) ist eine Methode der multivariaten Statistik, die zur Dimensionsreduktion von Datensätzen mit vielen Variablen eingesetzt wird. Ihr Ziel ist es, die Komplexität der Daten zu reduzieren, während möglichst viel der ursprünglichen Varianz erhalten bleibt. PCA wird in verschiedenen Bereichen wie der Datenanalyse, Mustererkennung und Visualisierung angewendet.\nDie PCA transformiert die Ausgangsdaten in ein neues Koordinatensystem, in dem die größte Varianz der Daten entlang der ersten Achse (der ersten Hauptkomponente) liegt. Die zweite Hauptkomponente erklärt die zweitgrößte Varianz und steht orthogonal zur ersten, und so weiter.\nWichtige Konzepte: - Varianz: Ein Maß für die Streuung der Daten. - Kovarianzmatrix: Beschreibt, wie stark zwei Variablen gemeinsam variieren. - Eigenwerte und Eigenvektoren: Eigenwerte geben an, wie viel Varianz von einer Komponente erklärt wird; Eigenvektoren definieren die Richtung dieser Varianz.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hauptkomponentenanalyse</span>"
    ]
  },
  {
    "objectID": "hauptkomponentenanalyse.html#mathematische-herleitung",
    "href": "hauptkomponentenanalyse.html#mathematische-herleitung",
    "title": "13  Hauptkomponentenanalyse",
    "section": "13.2 Mathematische Herleitung",
    "text": "13.2 Mathematische Herleitung\n\nStandardisierung der Daten:\n\nUm Variablen mit unterschiedlichen Einheiten vergleichbar zu machen, werden die Daten zentriert und skaliert.\n\nBerechnung der Kovarianzmatrix: \\[\n\\Sigma = \\frac{1}{n-1} X^T X\n\\]\nEigenwertzerlegung:\n\nLösung der Eigenwertgleichung: \\[\n\\Sigma v = \\lambda v\n\\] wobei \\(\\lambda\\) die Eigenwerte und \\(v\\) die Eigenvektoren sind.\n\nSortierung der Eigenwerte:\n\nDie Eigenwerte werden in absteigender Reihenfolge sortiert, die zugehörigen Eigenvektoren bilden die Hauptkomponenten.\n\nTransformation der Daten:\n\nProjektion der Daten in den neuen Raum: \\[\nZ = XW\n\\] wobei \\(W\\) die Matrix der Eigenvektoren ist.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hauptkomponentenanalyse</span>"
    ]
  },
  {
    "objectID": "hauptkomponentenanalyse.html#interpretation",
    "href": "hauptkomponentenanalyse.html#interpretation",
    "title": "13  Hauptkomponentenanalyse",
    "section": "13.3 Interpretation",
    "text": "13.3 Interpretation\n\nErklärte Varianz: Der Anteil der Gesamtvarianz, der von jeder Hauptkomponente erklärt wird.\nBiplots: Visualisieren die Projektion der Daten und die Lasten der Variablen auf den Hauptkomponenten.\n\n\nCode\n# Daten simulieren\nset.seed(123)\ndata &lt;- matrix(rnorm(100*5), ncol = 5)\ncolnames(data) &lt;- paste0(\"Var\", 1:5)\n\n# PCA durchführen\npca_result &lt;- prcomp(data, scale. = TRUE)\n\n# Zusammenfassung\nsummary(pca_result)\n# Scree-Plot\nplot(pca_result, type = \"l\")\n\n# Biplot\nbiplot(pca_result, scale = 0)\n\n\n\n\nImportance of components:\n                          PC1    PC2    PC3    PC4    PC5\nStandard deviation     1.1077 1.0610 1.0191 0.9468 0.8440\nProportion of Variance 0.2454 0.2251 0.2077 0.1793 0.1425\nCumulative Proportion  0.2454 0.4705 0.6783 0.8575 1.0000\n\n\n\n\n\nScree-Plot zeigt die Eigenwerte der Hauptkomponenten\n\n\n\n\n\n\n\n\n\nBiplot zeigt die Projektion der Daten und die Lasten der Variablen auf den Hauptkomponenten",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hauptkomponentenanalyse</span>"
    ]
  },
  {
    "objectID": "hauptkomponentenanalyse.html#interpretation-der-pca-ergebnisse",
    "href": "hauptkomponentenanalyse.html#interpretation-der-pca-ergebnisse",
    "title": "13  Hauptkomponentenanalyse",
    "section": "13.4 Interpretation der PCA-Ergebnisse",
    "text": "13.4 Interpretation der PCA-Ergebnisse\n\nScree-Plot: Zeigt die Eigenwerte der Hauptkomponenten. Ein “Knick” im Plot deutet darauf hin, dass ab diesem Punkt weniger Varianz erklärt wird.\nBiplot: Zeigt sowohl die Beobachtungen als Punkte als auch die Variablen als Vektoren. Die Länge der Vektoren zeigt die Bedeutung der Variablen, und der Winkel zwischen ihnen die Korrelation.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hauptkomponentenanalyse</span>"
    ]
  },
  {
    "objectID": "hauptkomponentenanalyse.html#fazit",
    "href": "hauptkomponentenanalyse.html#fazit",
    "title": "13  Hauptkomponentenanalyse",
    "section": "13.5 Fazit",
    "text": "13.5 Fazit\nDie Hauptkomponentenanalyse ist ein leistungsfähiges Werkzeug zur Reduktion von Datenkomplexität. Sie hilft, Muster in den Daten zu identifizieren und zu visualisieren, sollte jedoch mit Vorsicht interpretiert werden, da die Hauptkomponenten nicht immer eine klare inhaltliche Bedeutung haben.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Hauptkomponentenanalyse</span>"
    ]
  },
  {
    "objectID": "extremwertstatistik.html",
    "href": "extremwertstatistik.html",
    "title": "14  Extremwertstatistik",
    "section": "",
    "text": "14.1 Grundlagen der Extremwertstatistik\nExtremereignisse, wie starke Niederschläge, stellen eine bedeutende Herausforderung für Hochwasserschutz, Infrastrukturplanung und Klimarisikoanalysen dar. Die Extremwertstatistik liefert Methoden, um seltene, extreme Ereignisse zu quantifizieren. In diesem Kapitel betrachten wir zwei zentrale Ansätze:\nWir vergleichen beide Methoden hinsichtlich ihrer Annahmen, Stärken und Schwächen.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Extremwertstatistik</span>"
    ]
  },
  {
    "objectID": "extremwertstatistik.html#grundlagen-der-extremwertstatistik",
    "href": "extremwertstatistik.html#grundlagen-der-extremwertstatistik",
    "title": "14  Extremwertstatistik",
    "section": "",
    "text": "Wiederkehrwert \\(X(T)\\): Der Wert, der im Durchschnitt alle \\(T\\) Jahre überschritten wird.\n\nWiederkehrperiode \\(T\\): Gibt an, wie selten ein Ereignis ist (z.B. ein 100-jähriges Ereignis wird im Mittel alle 100 Jahre überschritten).\n\nÜberschreitungswahrscheinlichkeit: \\(P = \\frac{1}{T}\\) pro Jahr.\n\n\n\n\n\n\n\nDatenaufbereitung\n\n\n\nWir verwenden tägliche Niederschlagsdaten, um Extremereignisse zu identifizieren.\n\nlibrary(dplyr)\ndata &lt;- read.csv(\"Data/meteodaten_tag.csv\", sep = \",\", na.strings = \"-\")\ndata$Datum &lt;- as.Date(paste(data$Jahr, data$Monat, data$Tag, sep = \"-\"))\n\n# Jahresmaxima für den Blockmaxima-Ansatz\njahres_maxima &lt;- data %&gt;%\n  group_by(Jahr) %&gt;%\n  summarise(MaxNiederschlag = max(Niederschlag.mm.Tag., na.rm = TRUE))\n\n# Schwellenwert für den POT-Ansatz (95. Perzentil)\nthreshold &lt;- quantile(data$Niederschlag.mm.Tag., 0.95, na.rm = TRUE)\nextreme_events &lt;- data %&gt;%\n  filter(Niederschlag.mm.Tag. &gt; threshold) %&gt;%\n  pull(Niederschlag.mm.Tag.)  # Extrahiert nur die Niederschlagswerte als Vektor\n\nDatenüberblick\n\nMaximaler Tagesniederschlag in der Zeitreihe: 64.2 mm/Tag\n\n95%-Schwellenwert für Extremereignisse: 15.54 mm/Tag",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Extremwertstatistik</span>"
    ]
  },
  {
    "objectID": "extremwertstatistik.html#blockmaxima-ansatz-gev-verteilung",
    "href": "extremwertstatistik.html#blockmaxima-ansatz-gev-verteilung",
    "title": "14  Extremwertstatistik",
    "section": "14.2 Blockmaxima-Ansatz (GEV-Verteilung)",
    "text": "14.2 Blockmaxima-Ansatz (GEV-Verteilung)\nDer Blockmaxima-Ansatz betrachtet den höchsten Wert in festgelegten Zeitblöcken (hier: jährlich). Diese Maxima werden mit der Generalized Extreme Value (GEV)-Verteilung modelliert.\n\\[\n\\operatorname{GEV}(x ; \\mu, \\sigma, \\xi)=\\exp \\left\\{-\\left[1+\\xi\\left(\\frac{x-\\mu}{\\sigma}\\right)\\right]^{-1 / \\xi}\\right\\}\n\\]\n\n\\(\\mu\\) ist der Lageparameter\n\\(\\sigma\\) ist der Skalenparameter\n\\(\\xi\\) ist der Formparameter\n\n\\(\\xi = 0\\) entspricht der Gumbel-Verteilung und die Verteilung ist unbeschränkt\n\\(\\xi &gt; 0\\) entspricht der Fréchet-Verteilung und die Verteilung ist nach unten beschränkt\n\\(\\xi &lt; 0\\) entspricht der Weibull-Verteilung und die Verteilung ist nach oben beschränkt\n\n\n\nlibrary(extRemes)\ngev_fit &lt;- fevd(jahres_maxima$MaxNiederschlag, type = \"GEV\")\nsummary(gev_fit)\n\n\n\n\nfevd(x = jahres_maxima$MaxNiederschlag, type = \"GEV\")\n\n[1] \"Estimation Method used: MLE\"\n\n\n Negative Log-Likelihood Value:  50.47947 \n\n\n Estimated parameters:\n  location      scale      shape \n44.1527070 14.4947231 -0.6719294 \n\n Standard Error Estimates:\n location     scale     shape \n4.8703829 4.9359710 0.4169397 \n\n Estimated parameter covariance matrix.\n           location      scale      shape\nlocation 23.7206294  0.2456008 -1.1427263\nscale     0.2456008 24.3638095 -1.6521606\nshape    -1.1427263 -1.6521606  0.1738387\n\n AIC = 106.9589 \n\n BIC = 108.6538 \n\n\n\n14.2.1 Parameter der GEV-Verteilung:\n\nLageparameter (\\(\\mu\\)): 44.15 mm/Tag\n\nSkalenparameter (\\(\\sigma\\)): 14.49 mm/Tag\n\nFormparameter (\\(\\xi\\)): -0.672 also Weibull-Verteilt und nach oben beschränkt\n\n\n\n14.2.2 Berechnung von Wiederkehrwerten (GEV)\n\ngev_return_levels &lt;- return.level(gev_fit, c(100, 1000))\ngev_return_levels\n\nfevd(x = jahres_maxima$MaxNiederschlag, type = \"GEV\")\nget(paste(\"return.level.fevd.\", newcl, sep = \"\"))(x = x, return.period = return.period)\n\n GEV model fitted to  jahres_maxima$MaxNiederschlag  \nData are assumed to be  stationary \n[1] \"Return Levels for period units in years\"\n 100-year level 1000-year level \n       64.74390        65.51642 \n\n\n\n100-jähriges Ereignis: 64.74 mm/Tag\n\n1000-jähriges Ereignis: 65.52 mm/Tag\n\nWenn wir diese Werte mit dem höchsten Wert aus unserer Zeitreihe (64.2 mm/Tag) vergleichen, sehen wir, dass die GEV-Verteilung grosse Probleme mit nur so wenigen Datenpunkten (unsere Zeitreihe ist nur 13 Jahre lang, die Methode berechnet also die Wiederkehrwerte mit nur 13 Datenpunkten) hat.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Extremwertstatistik</span>"
    ]
  },
  {
    "objectID": "extremwertstatistik.html#peaks-over-threshold-pot-ansatz-gpd",
    "href": "extremwertstatistik.html#peaks-over-threshold-pot-ansatz-gpd",
    "title": "14  Extremwertstatistik",
    "section": "14.3 Peaks-over-Threshold (POT-Ansatz, GPD)",
    "text": "14.3 Peaks-over-Threshold (POT-Ansatz, GPD)\nEinen Ansatz um bei kurzen Zeitreihen die Wiederkehrwerte zu berechnen, ist der POT-Ansatz. Hier wird die Verteilung der Extremwerte oberhalb eines Schwellenwerts betrachtet. Das setzt voraus, dass die Extremwerte unabhängig voneinander sind und erfordert theoretisches Grundlagenwissen um den Schwellenwert subjektiv zu bestimmen.\nDer POT-Ansatz betrachtet alle Werte, die einen definierten Schwellenwert überschreiten (hier das 95. Perzentil). Diese Extremwerte werden mit der Generalized Pareto Distribution (GPD) modelliert.\n\npot_fit &lt;- fevd(extreme_events, \n                threshold = threshold,\n                type = \"GP\")\nsummary(pot_fit)\npot_params &lt;- pot_fit$results$par\n\n\n\n\nfevd(x = extreme_events, threshold = threshold, type = \"GP\")\n\n[1] \"Estimation Method used: MLE\"\n\n\n Negative Log-Likelihood Value:  739.7819 \n\n\n Estimated parameters:\n     scale      shape \n8.68543965 0.04087106 \n\n Standard Error Estimates:\n     scale      shape \n0.85257220 0.07289508 \n\n Estimated parameter covariance matrix.\n            scale        shape\nscale  0.72687936 -0.044552840\nshape -0.04455284  0.005313693\n\n AIC = 1483.564 \n\n BIC = 1490.449 \n\n\n\n14.3.1 Parameter der GPD:\n\nSchwellenwert: 15.54 mm/Tag\n\nSkalenparameter (\\(\\sigma\\)): 8.69 mm/Tag\n\nFormparameter (\\(\\xi\\)): 0.041\n\n\n\n14.3.2 Berechnung von Wiederkehrwerten (POT)\n\npot_return_levels &lt;- return.level(pot_fit, c(100, 1000))\npot_return_levels\n\nfevd(x = extreme_events, threshold = threshold, type = \"GP\")\nget(paste(\"return.level.fevd.\", newcl, sep = \"\"))(x = x, return.period = return.period)\n\n GP model fitted to  extreme_events  \nData are assumed to be  stationary \n[1] \"Return Levels for period units in years\"\n 100-year level 1000-year level \n       129.5096        161.7263 \n\n\n\n100-jähriges Ereignis (POT): 129.51 mm/Tag\n\n1000-jähriges Ereignis (POT): 161.73 mm/Tag\n\nZur Erinnerung: der höchste Wert in unserer Zeitreihe war 64.2 mm/Tag. Das liefert uns also eine komplett andere Schätzung für die Wiederkehrwerte.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Extremwertstatistik</span>"
    ]
  },
  {
    "objectID": "extremwertstatistik.html#visualisierung-der-wiederkehrwerte",
    "href": "extremwertstatistik.html#visualisierung-der-wiederkehrwerte",
    "title": "14  Extremwertstatistik",
    "section": "14.4 Visualisierung der Wiederkehrwerte",
    "text": "14.4 Visualisierung der Wiederkehrwerte\n\n\nCode\nlibrary(ggplot2)\n\n# 1. Definition des Bereichs für die Wiederkehrperioden\nreturn_periods &lt;- seq(2, 1000, length.out = 500)\n\n# 2. Berechnung der Wiederkehrwerte für beide Modelle\ngev_return_levels_full &lt;- return.level(gev_fit, return.period = return_periods)\npot_return_levels_full &lt;- return.level(pot_fit, return.period = return_periods)\n\n# 3. Berechnung der Konfidenzintervalle\ngev_ci &lt;- ci(gev_fit, return.period = return_periods)\npot_ci &lt;- ci(pot_fit, return.period = return_periods)\n\n# 4. Daten für den Plot zusammenfassen\nplot_data &lt;- data.frame(\n  ReturnPeriod = rep(return_periods, 2),\n  ReturnLevel = c(gev_return_levels_full, pot_return_levels_full),\n  LowerCI = c(gev_ci[, \"95% lower CI\"], pot_ci[, \"95% lower CI\"]),\n  UpperCI = c(gev_ci[, \"95% upper CI\"], pot_ci[, \"95% upper CI\"]),\n  Methode = rep(c(\"GEV (Blockmaxima)\", \"GPD (POT)\"), each = length(return_periods))\n)\n\n# 5. Plot erstellen\nggplot(plot_data, aes(x = ReturnPeriod, y = ReturnLevel, color = Methode, fill = Methode)) +\n  geom_ribbon(aes(ymin = LowerCI, ymax = UpperCI), alpha = 0.2, linetype = 0) +  # Unsicherheitsbereich\n  geom_line(linewidth = 1) +                                                    # Verteilungsfunktion\n  geom_point(data = plot_data %&gt;% filter(ReturnPeriod %in% c(100, 1000)),        # Markierung für 100- und 1000-jähriges Ereignis\n             aes(x = ReturnPeriod, y = ReturnLevel),\n             size = 3, shape = 21, fill = \"white\") +\n  scale_x_log10(breaks = c(2, 5, 10, 20, 50, 100, 200, 500, 1000)) +             # Log-Skala\n  labs(\n    title = \"Vergleich der Wiederkehrwerte mit Konfidenzintervallen\",\n    x = \"Wiederkehrperiode (Jahre, log-Skala)\",\n    y = \"Niederschlag (mm/Tag)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Extremwertstatistik</span>"
    ]
  },
  {
    "objectID": "extremwertstatistik.html#direkter-vergleich-der-methoden",
    "href": "extremwertstatistik.html#direkter-vergleich-der-methoden",
    "title": "14  Extremwertstatistik",
    "section": "14.5 Direkter Vergleich der Methoden",
    "text": "14.5 Direkter Vergleich der Methoden\n\nvergleich &lt;- data.frame(\n  Methode = c(\"GEV (Blockmaxima)\", \"GPD (POT)\"),\n  `100-jährig (mm)` = c(gev_return_levels[1], pot_return_levels[1]),\n  `1000-jährig (mm)` = c(gev_return_levels[2], pot_return_levels[2])\n)\nvergleich\n\n            Methode X100.jährig..mm. X1000.jährig..mm.\n1 GEV (Blockmaxima)          64.7439          65.51642\n2         GPD (POT)         129.5096         161.72632\n\n\n\n14.5.1 Interpretation:\n\nGEV (Blockmaxima): Nutzt nur ein Extremereignis pro Jahr.\n\nGPD (POT): Berücksichtigt alle extremen Ereignisse oberhalb der Schwelle → häufig realistischere Werte bei kurzen Zeitreihen.\n\n\n\n14.5.2 Vorteile des Blockmaxima-Ansatzes (GEV):\n\nEinfach zu berechnen und zu interpretieren.\n\nRobuster bei langen Zeitreihen.\n\n\n\n14.5.3 Nachteile des Blockmaxima-Ansatzes (GEV):\n\nViele extreme Ereignisse werden nicht berücksichtigt.\n\nWenige Datenpunkte führen zu hoher Unsicherheit.\n\n\n\n14.5.4 Vorteile des POT-Ansatzes (GPD):\n\nNutzt mehr Extremwerte → bessere statistische Basis.\n\nFlexibler bei kurzen Zeitreihen.\n\n\n\n14.5.5 Nachteile des POT-Ansatzes (GPD):\n\nWahl des Schwellenwerts ist kritisch.\n\nAbhängigkeiten zwischen Extremereignissen müssen beachtet werden.\n\nBeide Methoden haben ihre Berechtigung:\n\nBlockmaxima (GEV) ist geeignet für lange Zeitreihen mit stabilen Extremwerten.\n\nPOT (GPD) liefert bei kurzen Datensätzen oft realistischere Ergebnisse.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Extremwertstatistik</span>"
    ]
  },
  {
    "objectID": "fallen-der-statistik.html",
    "href": "fallen-der-statistik.html",
    "title": "16  Fallen der Statistik",
    "section": "",
    "text": "16.1 p-Werte",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Fallen der Statistik</span>"
    ]
  },
  {
    "objectID": "fallen-der-statistik.html#p-werte",
    "href": "fallen-der-statistik.html#p-werte",
    "title": "16  Fallen der Statistik",
    "section": "",
    "text": "“The effect of the drug on blood pressure was statistically significant (p = 0.02)”\n\n\nEin p-Wert von &lt; 0.05 bedeutet die Ablehnung der Nullhypothese\nDie Nullhypothese sagt normalerweise, dass es keine Korrelation oder einen Unterschied zwischen A und B gibt\nABER:\n\n“It is foolish to ask ‘Are the effects of A and B different?’ They are always different for some decimal place”\nWir sind nicht wirklich an “statistischer Signifikanz” interessiert, sondern an physikalisch oder sozial signifikanten Effekten.\nAber wie gross war der Effekt",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Fallen der Statistik</span>"
    ]
  },
  {
    "objectID": "fallen-der-statistik.html#simpsons-paradoxon",
    "href": "fallen-der-statistik.html#simpsons-paradoxon",
    "title": "16  Fallen der Statistik",
    "section": "16.2 Simpsons Paradoxon",
    "text": "16.2 Simpsons Paradoxon\n\nUnaufgesplittete Daten aus einer Medikamentenstudie.\n\n\n\nControl\nTreatment\n\n\n\n\nAlive\n60\n200\n\n\nDead\n60\n200\n\n\nRate\n50%\n50%\n\n\n\nWenn wir diese Daten betrachten, sehen wir, dass das Medikament wirkungslos ist.\nWenn wir die Daten nach Geschlecht aufsplitten, sehen wir einen anderen Effekt:\n\nAufgesplittete Daten nur für Männer.\n\n\n\nControl\nTreatment\n\n\n\n\nAlive\n40\n80\n\n\nDead\n30\n50\n\n\nRate\n43%\n38%\n\n\n\nDas Medikament hat einen positiven Effekt. Was erwarten wir, wenn die Daten für Frauen betrachtet werden?\n\nAufgesplittete Daten nur für Frauen.\n\n\n\nControl\nTreatment\n\n\n\n\nAlive\n20\n120\n\n\nDead\n30\n150\n\n\nRate\n60%\n55%\n\n\n\nDas Medikament hat ebenfalls einen positiven Effekt. Wie kann das sein?\nEinzelpersonen der Gruppen bekommen ein unterschiedliches Gewicht in Prozent am Ausgang.",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Fallen der Statistik</span>"
    ]
  },
  {
    "objectID": "fallen-der-statistik.html#probleme-mit-seltenen-ereignissen",
    "href": "fallen-der-statistik.html#probleme-mit-seltenen-ereignissen",
    "title": "16  Fallen der Statistik",
    "section": "16.3 Probleme mit seltenen Ereignissen",
    "text": "16.3 Probleme mit seltenen Ereignissen\n\nUmfragen haben eine Tendenz, seltene Ereignisse zu unterschätzen",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Fallen der Statistik</span>"
    ]
  },
  {
    "objectID": "fallen-der-statistik.html#falsche-interpretation-wahrnehmung",
    "href": "fallen-der-statistik.html#falsche-interpretation-wahrnehmung",
    "title": "16  Fallen der Statistik",
    "section": "16.4 Falsche Interpretation / Wahrnehmung",
    "text": "16.4 Falsche Interpretation / Wahrnehmung\nWahrscheinlichkeit, dass mindestens zwei Personen am gleichen Tag Geburtstag haben nach Anzahl Personen.\n\n\nCode\nviewof numPeople = Inputs.range(\n  [2, 100], \n  { value: 23, step: 1, label: \"Anzahl Personen:\" }\n)\n\n\n\n\n\n\n\n\n\nCode\nfunction birthdayProbability(n) {\n  let prob = 1;\n  for (let i = 0; i &lt; n; i++) {\n    prob *= (365 - i) / 365;\n  }\n  return 1 - prob;\n}\n\n// Daten für den Plot generieren\ndata = Array.from({ length: 99 }, (_, i) =&gt; ({\n  numPeople: i + 2,\n  probability: birthdayProbability(i + 2)\n}));\n\n// Aktuelle Wahrscheinlichkeit berechnen\ncurrentProb = birthdayProbability(numPeople);\n\n// Plot erstellen\nPlot.plot({\n  marks: [\n    Plot.line(data, { x: \"numPeople\", y: \"probability\", stroke: \"blue\" }),\n    Plot.dot(\n      [{ numPeople, probability: currentProb }],\n      { x: \"numPeople\", y: \"probability\", fill: \"red\", r: 5 }\n    ),\n    // Textanzeige unten rechts\n    Plot.text(\n      [{ label: `Wahrscheinlichkeit: ${(currentProb * 100).toFixed(1)}%` }],\n      {\n        x: 60, \n        y: 0.5, \n        text: \"label\", \n        fill: \"black\", \n        dy: 10, \n        dx: 10,\n        fontSize: 14,\n        anchor: \"end\"\n      }\n    )\n  ],\n  x: { label: \"Anzahl Personen\", domain: [2, 100] },\n  y: { label: \"Wahrscheinlichkeit\", tickFormat: \"%\" },\n  width: 600,\n  height: 400\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBereits bei 23 Personen liegt die Wahrscheinlichkeit bei über 50 %, dass zwei Personen am gleichen Tag Geburtstag haben. Dies verdeutlicht, wie unsere Intuition bei Wahrscheinlichkeiten oft trügt.\nDenn:\n\\[\nP(A) = 1 - \\frac{365!}{(365-n)! \\cdot 365^n}\n\\]",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Fallen der Statistik</span>"
    ]
  },
  {
    "objectID": "glossar.html",
    "href": "glossar.html",
    "title": "17  Glossar",
    "section": "",
    "text": "17.1 Mittelwert \\(\\bar{x}\\) oder \\(\\mu\\)",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Glossar</span>"
    ]
  },
  {
    "objectID": "glossar.html#mittelwert-barx-oder-mu",
    "href": "glossar.html#mittelwert-barx-oder-mu",
    "title": "17  Glossar",
    "section": "",
    "text": "Symbol: \\(\\bar{x}\\) (Stichprobe) oder \\(\\mu\\) (Population)\n\nBeschreibung: Der Mittelwert ist der Durchschnitt aller Werte in einer Stichprobe oder Population. Er gibt an, wo das Zentrum der Daten liegt.\nFormel:\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\nAnwendung: Wird verwendet, um den typischen Wert in Datensätzen zu beschreiben, z.B. den durchschnittlichen Lohn in einer Population.\nR-Code:\n\n\nmean(x)",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Glossar</span>"
    ]
  },
  {
    "objectID": "glossar.html#median-x_0.5",
    "href": "glossar.html#median-x_0.5",
    "title": "17  Glossar",
    "section": "17.2 Median \\(x_{0.5}\\)",
    "text": "17.2 Median \\(x_{0.5}\\)\n\nSymbol: \\(x_{0.5}\\)\n\nBeschreibung: Der Median ist der Wert, der die Daten in zwei gleiche Hälften teilt. Er ist robust gegen Ausreißer und gibt einen durchschnittlichen Wert an, der nicht durch Extremwerte beeinflusst wird.\nFormel:\n\\[\nx_{0.5} = \\begin{cases}\nx_{(n+1)/2} & \\text{falls } n \\text{ ungerade} \\\\\n\\frac{x_{n/2} + x_{n/2+1}}{2} & \\text{falls } n \\text{ gerade}\n\\end{cases}\n\\]\nAnwendung: Wird verwendet, um den typischen Wert in Datensätzen zu beschreiben, z.B. den durchschnittlichen Lohn in einer Population.\nR-Code:\n\n\nmedian(x)",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Glossar</span>"
    ]
  },
  {
    "objectID": "glossar.html#varianz-s2-sigma2",
    "href": "glossar.html#varianz-s2-sigma2",
    "title": "17  Glossar",
    "section": "17.3 Varianz \\(s^2\\), \\(\\sigma^2\\)",
    "text": "17.3 Varianz \\(s^2\\), \\(\\sigma^2\\)\n\nSymbol: \\(s^2\\) (Stichprobe), \\(\\sigma^2\\) (Population)\n\nBeschreibung: Die Varianz misst die durchschnittliche quadratische Abweichung der Werte vom Mittelwert und beschreibt die Streuung der Daten.\nFormel (Stichprobe):\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\]\nFormel (Population):\n\\[\n\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2\n\\]\nAnwendung: Wichtig zur Berechnung der Standardabweichung und zur Analyse der Datenstreuung.\nR-Code:\n\n\nvar(x)\n# Für Population: var(x) * (n-1)/n",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Glossar</span>"
    ]
  },
  {
    "objectID": "glossar.html#standardabweichung-s-sigma",
    "href": "glossar.html#standardabweichung-s-sigma",
    "title": "17  Glossar",
    "section": "17.4 Standardabweichung \\(s\\), \\(\\sigma\\)",
    "text": "17.4 Standardabweichung \\(s\\), \\(\\sigma\\)\n\nSymbol: \\(s\\) (Stichprobe), \\(\\sigma\\) (Population)\n\nBeschreibung: Die Standardabweichung ist die Wurzel der Varianz und beschreibt die durchschnittliche Abweichung der Werte vom Mittelwert.\nFormel:\n\\[\ns = \\sqrt{s^2}, \\quad \\sigma = \\sqrt{\\sigma^2}\n\\]\nAnwendung: Zeigt, wie weit die Daten im Durchschnitt um den Mittelwert streuen. Häufig verwendet in der deskriptiven Statistik.\nR-Code:\n\n\nsd(x)",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Glossar</span>"
    ]
  },
  {
    "objectID": "glossar.html#standardfehler-se",
    "href": "glossar.html#standardfehler-se",
    "title": "17  Glossar",
    "section": "17.5 Standardfehler \\(SE\\)",
    "text": "17.5 Standardfehler \\(SE\\)\n\nSymbol: \\(SE\\)\n\nBeschreibung: Der Standardfehler des Mittelwerts misst, wie genau der Mittelwert einer Stichprobe den wahren Mittelwert der Population schätzt.\nFormel:\n\\[\nSE = \\frac{s}{\\sqrt{n}}\n\\]\nAnwendung: Grundlage für Konfidenzintervalle und Hypothesentests.\nR-Code:\n\n\nsd(x) / sqrt(length(x))",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Glossar</span>"
    ]
  },
  {
    "objectID": "glossar.html#t-wert-t",
    "href": "glossar.html#t-wert-t",
    "title": "17  Glossar",
    "section": "17.6 t-Wert \\(t\\)",
    "text": "17.6 t-Wert \\(t\\)\n\nSymbol: \\(t\\)\n\nBeschreibung: Der t-Wert misst, wie stark ein beobachteter Wert vom erwarteten Wert (unter der Nullhypothese) abweicht, gemessen in Standardfehlern.\nFormel (für Mittelwert-Test):\n\\[\nt = \\frac{\\bar{x} - \\mu_0}{SE}\n\\]\nAnwendung: Verwendung in t-Tests, um Hypothesen über Mittelwerte zu überprüfen.\nR-Code:\n\n\nt.test(x, mu = 50)$statistic",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Glossar</span>"
    ]
  },
  {
    "objectID": "glossar.html#p-wert-p",
    "href": "glossar.html#p-wert-p",
    "title": "17  Glossar",
    "section": "17.7 p-Wert \\(p\\)",
    "text": "17.7 p-Wert \\(p\\)\n\nSymbol: \\(p\\)\n\nBeschreibung: Der p-Wert gibt die Wahrscheinlichkeit an, ein Ergebnis zu beobachten, das mindestens so extrem ist wie das tatsächliche Ergebnis, wenn die Nullhypothese wahr ist.\nFormel (zweiseitig):\n\\[\np = 2 \\cdot P(T &gt; |t|)\n\\]\nAnwendung: Dient zur Entscheidungsfindung in Hypothesentests. Ein kleiner p-Wert deutet darauf hin, dass das Ergebnis signifikant ist.\nR-Code:\n\n\nt.test(x, mu = 50)$p.value",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Glossar</span>"
    ]
  },
  {
    "objectID": "glossar.html#freiheitsgrade-df",
    "href": "glossar.html#freiheitsgrade-df",
    "title": "17  Glossar",
    "section": "17.8 Freiheitsgrade \\(df\\)",
    "text": "17.8 Freiheitsgrade \\(df\\)\n\nSymbol: \\(df\\)\n\nBeschreibung: Freiheitsgrade geben an, wie viele Werte in einer Berechnung frei variieren können, ohne dass eine Bedingung verletzt wird.\nFormel (für einfache Stichprobe):\n\\[\ndf = n - 1\n\\]\nAnwendung: Wichtig bei der Bestimmung der kritischen Werte für t- und F-Tests.\nR-Code:\n\n\nlength(x) - 1",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Glossar</span>"
    ]
  },
  {
    "objectID": "glossar.html#regressionskoeffizient-hatbeta",
    "href": "glossar.html#regressionskoeffizient-hatbeta",
    "title": "17  Glossar",
    "section": "17.9 Regressionskoeffizient \\(\\hat{\\beta}\\)",
    "text": "17.9 Regressionskoeffizient \\(\\hat{\\beta}\\)\n\nSymbol: \\(\\hat{\\beta}\\)\n\nBeschreibung: Der Regressionskoeffizient misst den Einfluss eines Prädiktors auf die abhängige Variable in einem linearen Regressionsmodell.\nFormel (lineare Regression):\n\\[\nY = \\beta_0 + \\beta_1 X + \\varepsilon\n\\]\nAnwendung: Analyse von Zusammenhängen zwischen Variablen in Regressionsmodellen.\nR-Code:\n\n\nlm(Y ~ X)$coefficients",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Glossar</span>"
    ]
  },
  {
    "objectID": "glossar.html#standardisiertes-residuum-r_i",
    "href": "glossar.html#standardisiertes-residuum-r_i",
    "title": "17  Glossar",
    "section": "17.10 Standardisiertes Residuum \\(r_i\\)",
    "text": "17.10 Standardisiertes Residuum \\(r_i\\)\n\nSymbol: \\(r_i\\)\n\nBeschreibung: Das standardisierte Residuum misst die Abweichung eines beobachteten Werts vom vorhergesagten Wert in Standardabweichungseinheiten.\nFormel:\n\\[\nr_i = \\frac{e_i}{SE(e_i)}\n\\]\nAnwendung: Identifikation von Ausreißern in Regressionsmodellen.\nR-Code:\n\n\nrstandard(lm(Y ~ X))",
    "crumbs": [
      "Statistische Datenanalyse",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Glossar</span>"
    ]
  },
  {
    "objectID": "excercises/ex01.html",
    "href": "excercises/ex01.html",
    "title": "Appendix A — Übung 1: Grundlagen R",
    "section": "",
    "text": "B Übung 1: Grundlagen R",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Übung 1: Grundlagen R</span>"
    ]
  },
  {
    "objectID": "excercises/ex01.html#vektoren",
    "href": "excercises/ex01.html#vektoren",
    "title": "Appendix A — Übung 1: Grundlagen R",
    "section": "B.1 Vektoren",
    "text": "B.1 Vektoren\nÜberlegt euch die erwarteten Lösungen vor dem Eintippen\n\nx &lt;- c(5, 2, 1, 4)\nxx &lt;- c(1, 10, 15, 18)\ny &lt;- rep(1, 5)\nz &lt;- c(TRUE, FALSE, TRUE, TRUE)\n\n\nB.1.1 Aufgabe a)\n\nsum(x)\nrange(x)\nlength(x)\nmax(x)\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nsum(x)\n\n[1] 12\n\nrange(x)\n\n[1] 1 5\n\nlength(x)\n\n[1] 4\n\nmax(x)\n\n[1] 5\n\n\n\n\n\n\n\nB.1.2 Aufgabe b)\n\nc(x, y, 13)\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nc(x, y, 13)\n\n [1]  5  2  1  4  1  1  1  1  1 13\n\n\n\n\n\n\n\nB.1.3 Aufgabe c)\n\nx[4] * y[2]\nxx[2:4] + x[1:3]\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nx[4] * y[2]\n\n[1] 4\n\nxx[2:4] + x[1:3]\n\n[1] 15 17 19\n\n\n\n\n\n\n\nB.1.4 Aufgabe d)\n\nxx &lt;= 12\nxx[xx &lt;= 12]\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nxx &lt;= 12\n\n[1]  TRUE  TRUE FALSE FALSE\n\nxx[xx &lt;= 12]\n\n[1]  1 10\n\n\n\n\n\n\n\nB.1.5 Aufgabe e)\n\nplot(x, xx)\nplot(x[z], xx[z])\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nplot(x, xx)\n\n\n\n\n\n\n\nplot(x[z], xx[z])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Übung 1: Grundlagen R</span>"
    ]
  },
  {
    "objectID": "excercises/ex01.html#zahlenfolgen",
    "href": "excercises/ex01.html#zahlenfolgen",
    "title": "Appendix A — Übung 1: Grundlagen R",
    "section": "B.2 Zahlenfolgen",
    "text": "B.2 Zahlenfolgen\nErzeugt mit den rep()und seq() Funktionen die folgenden Zahlenfolgen:\n\nB.2.1 Aufgabe a)\n1 2 3 4 5 6 7 8 9\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nseq(1, 9)\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\noder gleichwertig:\n\n1:9\n\n[1] 1 2 3 4 5 6 7 8 9\n\n\n\n\n\n\n\nB.2.2 Aufgabe b)\n\"m\" \"w\" \"m\" \"w\" \"m\" \"w\"\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nrep(c(\"m\", \"w\"), 3)\n\n[1] \"m\" \"w\" \"m\" \"w\" \"m\" \"w\"\n\n\noder gleichwertig:\n\nrep(c(\"m\", \"w\"), length = 6)\n\n[1] \"m\" \"w\" \"m\" \"w\" \"m\" \"w\"\n\n\n\n\n\n\n\nB.2.3 Aufgabe c)\n1 2 3 4 1 2 3 4 1 2 3 4\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nrep(1:4, 3)\n\n [1] 1 2 3 4 1 2 3 4 1 2 3 4\n\n\n\n\n\n\n\nB.2.4 Aufgabe d)\n1 2 2 3 3 3 4 4 4 4\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nrep(1:4, 1:4)\n\n [1] 1 2 2 3 3 3 4 4 4 4",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Übung 1: Grundlagen R</span>"
    ]
  },
  {
    "objectID": "excercises/ex01.html#datei-einlesen",
    "href": "excercises/ex01.html#datei-einlesen",
    "title": "Appendix A — Übung 1: Grundlagen R",
    "section": "B.3 Datei einlesen",
    "text": "B.3 Datei einlesen\nLest die Datei meteodaten_saison.csv in R ein:\n\n1saison &lt;- read.table(\"Pfad/zur/Datei.csv\",\n2                     sep = \",\",\n                     header = TRUE)\n\n\n1\n\nSetzt den korrekten Pfad zur Datei ein.\n\n2\n\nWeitere Argumente für den Funktionsaufruf: sep (separator) gibt an welches Trennzeichen in der Datei verwendet wird und header ob die erste Zeile als Spaltennamen verwendet werden soll.\n\n\n\n\nÜberprüft, ob der Import korrekt verlief.\n\n\n\n\n\n\nTip\n\n\n\nDer Pfad zur Datei kann relativ oder absolut sein. In aller Regel ist es einfacher, Daten in einem Unterordner (z.B. Data) zu speichern und dann nur den Dateinamen anzugeben.\nWichtig ist das korrekte Setzten des Arbeitsverzeichnisses in RStudio:\nSession -&gt; Set Working Directory -&gt; To Source File Location\nDanach können Dateien relativ zum aktuellen Skriptpfad geladen werden.\nBeispiel für einen relativen Pfad:\n\nsaison &lt;- read.table(\"Data/meteodaten_saison.csv\",\n                     sep = \",\",\n                     header = TRUE)\n\n\n\nAnschliessend könnt ihr die Daten mit str(), head(), summary(), tail(), class(), etc. überprüfen.\n\nstr(saison)\n\n'data.frame':   492 obs. of  6 variables:\n $ Jahr                           : int  1901 1901 1901 1901 1902 1902 1902 1902 1903 1903 ...\n $ Saison                         : chr  \"Fruehling(MAM)\" \"Herbst(SON)\" \"Sommer(JJA)\" \"Winter(DJF)\" ...\n $ Bern_Mitteltemperatur          : num  7.73 7.4 16.8 -2.73 7.53 ...\n $ Bern_Niederschlagssumme        : num  278 245 381 112 323 ...\n $ GrStBernhard_Mitteltemperatur  : num  -4 -0.8 6.3 -10.6 -3.63 ...\n $ GrStBernhard_Niederschlagssumme: num  495 521 285 356 448 ...",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Übung 1: Grundlagen R</span>"
    ]
  },
  {
    "objectID": "excercises/ex02.html",
    "href": "excercises/ex02.html",
    "title": "Appendix B — R-Übungen 2",
    "section": "",
    "text": "B.1 Grafik erstellen",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>R-Übungen 2</span>"
    ]
  },
  {
    "objectID": "excercises/ex02.html#grafik-erstellen",
    "href": "excercises/ex02.html#grafik-erstellen",
    "title": "Appendix B — R-Übungen 2",
    "section": "",
    "text": "B.1.1 Aufgabe\n\nExtrahiert aus den saisonalen Daten:\n\ndie Frühlingsdaten,\ndie Sommerdaten,\ndie Herbstdaten.\n\n\nBeispiel:\n\nfruehling &lt;- saison[saison[,2] == \"Fruehling(MAM)\", ]\n\n\nErstellt einen Plot, mit:\n\nden Jahren auf der x-Achse und\nder Temperatur in Genf auf der y-Achse.\n\nStellt dabei die Frühlings-, Sommer- und Herbsttemperaturen als Linien mit unterschiedlichen Farben dar.\n\n\nB.1.1.1 Schritte:\n\nZuerst:\n\n\nplot(x, y, col = \" \", xlab = \" \", ...)\n\n\nDann mit:\n\n\nlines(x, y, col = ...)\n\nweitere Saisons hinzufügen.\n\nÜberschrift und Achsen beschriften.\nLinien der beiden Mittelwerte hinzufügen:\n\n\nabline(h = ...)\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nJahreszeitentabelle &lt;- read.table(\"Data/meteodaten_saison.csv\",\n                     header = TRUE,\n                     sep = \",\")\n\nsaison_fruehling &lt;- Jahreszeitentabelle[Jahreszeitentabelle[, 2] == \"Fruehling(MAM)\", ]\nsaison_sommer &lt;- Jahreszeitentabelle[Jahreszeitentabelle$Saison == \"Sommer(JJA)\", ]\nsaison_herbst &lt;- Jahreszeitentabelle[Jahreszeitentabelle$Saison == \"Herbst(SON)\", ]\nsaison_winter &lt;- Jahreszeitentabelle[Jahreszeitentabelle$Saison == \"Winter(DJF)\", ]\n\nsaison_helper &lt;- data.frame(\n    saison_name = c(\"Fruehling\", \"Sommer\", \"Herbst\", \"Winter\"),\n    saison_farbe = c(\"green\", \"red\", \"orange\", \"blue\"))\n\nplot(saison_fruehling$Jahr,\n     saison_fruehling$Bern_Mitteltemperatur,\n     type = \"line\",\n\n     xlab = \"Jahr\",\n     ylab = \"Mitteltemperatur\",\n     main = \"Mitteltemperatur in Bern nach Jahreszeiten\",\n     ylim = c(-5, 25))\n\nWarning in plot.xy(xy, type, ...): plot type 'line' will be truncated to first\ncharacter\n\nlines(saison_sommer$Jahr,\n      saison_sommer$Bern_Mitteltemperatur,\n      col = saison_helper$saison_farbe[2]\n)\n\nlines(saison_herbst$Jahr,\n      saison_herbst$Bern_Mitteltemperatur,\n      col = saison_helper$saison_farbe[3]\n)\n\nlines(saison_winter$Jahr,\n      saison_winter$Bern_Mitteltemperatur,\n      col = saison_helper$saison_farbe[4]\n)\n\n\n\n\n\n\nlegend(\"topright\",\n       legend = saison_helper$saison_name,\n       col = saison_helper$saison_farbe,\n       lty = 1,\n       cex = 0.8\n       )\n\nabline(mean(saison_fruehling$Bern_Mitteltemperatur),\n       0,\n       col = \"green\")\n\n\n\n\n\n\n\n\n\nSchnittpunkt mit der y-Achse\nSteigung der Linie",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>R-Übungen 2</span>"
    ]
  },
  {
    "objectID": "excercises/ex02.html#jahresmittelwerte",
    "href": "excercises/ex02.html#jahresmittelwerte",
    "title": "Appendix B — R-Übungen 2",
    "section": "B.2 Jahresmittelwerte",
    "text": "B.2 Jahresmittelwerte\n\nB.2.1 Aufgabe\n\nErstellt mittels aggregate() die Jahresmittelwerte der Temperatur für Genf.\nStellt diese in einem Scatterplot mit Punkten dar:\n\n\nplot(x, y)\n\n\nBeschriftet die Achsen und vergebt einen Titel.\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nJahreswerte &lt;- aggregate(Jahreszeitentabelle$Bern_Mitteltemperatur,\n                         by = list(Jahreszeitentabelle$Jahr),\n                         FUN = mean)\n\n# Für bessere Lesbarkeit: umbenennen der generierten Spaltennamen\ncolnames(Jahreswerte) &lt;- c(\"Jahr\", \"Mitteltemperatur\")\n\nplot(Jahreswerte$Jahr,\n     Jahreswerte$Mitteltemperatur,\n     col = \"black\",\n     xlab = \"Jahr\",\n     ylab = \"Mitteltemperatur\",\n     ylim = c(7, 12),\n     main = \"Mitteltemperatur in Bern nach Jahren\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>R-Übungen 2</span>"
    ]
  },
  {
    "objectID": "excercises/ex02.html#boxplot",
    "href": "excercises/ex02.html#boxplot",
    "title": "Appendix B — R-Übungen 2",
    "section": "B.3 Boxplot",
    "text": "B.3 Boxplot\n\nWählt den Zeitraum 1981-2010, z.B.:\n\n\nzeit &lt;- saison[saison[,1] &gt;= 1981 & saison[,1] &lt;= 2010, ]\n\n\nStellt die Temperatur- und Niederschlagsverteilungen der Saisons in Genf und Gr. S. Bernhard für diesen Zeitraum in vier boxplot()-Plots dar.\n\n\nB.3.1 Hinweise:\n\nDas Grafikausgabefenster kann mit:\n\n\npar(mfrow = c(2, 2))\n\nin 2 Zeilen und 2 Spalten geteilt werden.\n\nBeschriftet die Achsen und vergebt Titel. Achtet darauf, für beide Stationen gleiche y-Achsen zu wählen, sodass die Plots visuell vergleichbar sind. Z.B. bei Niederschlag:\n\n\nylim = c(0, 1300)\n\n\nSetzt das Grafikausgabefenster zurück auf 1 Zeile und 1 Spalte:\n\n\npar(mfrow = c(1, 1))\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\n\nzeitraum &lt;- 1981:2010\n\n# Ausgabefenster in 2x2 aufteilen\npar(mfrow = c(2, 2))\n\nboxplot(Jahreszeitentabelle$Bern_Mitteltemperatur ~ Jahreszeitentabelle$Saison,\n        data = Jahreszeitentabelle[Jahreszeitentabelle$Jahr %in% zeitraum, ],\n        col = saison_helper$saison_farbe,\n        xlab = \"Jahreszeiten\",\n        ylab = \"Mitteltemperatur\",\n        main = \"Mitteltemperatur in Bern nach Saison (1981-2010)\",\n        ylim = c(-12, 22))\n\nboxplot(Jahreszeitentabelle$GrStBernhard_Mitteltemperatur ~ Jahreszeitentabelle$Saison,\n        data = Jahreszeitentabelle[Jahreszeitentabelle$Jahr %in% zeitraum, ],\n        col = saison_helper$saison_farbe,\n        xlab = \"Jahreszeiten\",\n        ylab = \"Mitteltemperatur\",\n        main = \"Mitteltemperatur auf dem GSB nach Saison (1981-2010)\",\n        ylim = c(-12, 22))\n\nboxplot(Jahreszeitentabelle$Bern_Niederschlagssumme ~ Jahreszeitentabelle$Saison,\n        data = Jahreszeitentabelle[Jahreszeitentabelle$Jahr %in% zeitraum, ],\n        col = saison_helper$saison_farbe,\n        xlab = \"Jahreszeiten\",\n        ylab = \"Niederschlagssumme\",\n        main = \"Niederschlagssumme in Bern nach Saison (1981-2010)\",\n        ylim = c(0, 1400))\n\nboxplot(Jahreszeitentabelle$GrStBernhard_Niederschlagssumme ~ Jahreszeitentabelle$Saison,\n        data = Jahreszeitentabelle[Jahreszeitentabelle$Jahr %in% zeitraum, ],\n        col = saison_helper$saison_farbe,\n        xlab = \"Jahreszeiten\",\n        ylab = \"Niederschlagssumme\",\n        main = \"Niederschlagssumme auf dem GSB nach Saison (1981-2010)\",\n        ylim = c(0, 1400))\n\n\n\n\n\n\n\n# Ausgabefenster zurücksetzen\npar(mfrow = c(1, 1))",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>R-Übungen 2</span>"
    ]
  },
  {
    "objectID": "excercises/ex03.html",
    "href": "excercises/ex03.html",
    "title": "Appendix C — R-Übungen 3",
    "section": "",
    "text": "C.1 Sommer (JJA) Temperaturanomalien",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>R-Übungen 3</span>"
    ]
  },
  {
    "objectID": "excercises/ex03.html#sommer-jja-temperaturanomalien",
    "href": "excercises/ex03.html#sommer-jja-temperaturanomalien",
    "title": "Appendix C — R-Übungen 3",
    "section": "",
    "text": "C.1.1 Aufgabe\n\nBerechnet die Sommer (JJA) Temperaturanomalien zur Referenzperiode 1961 bis 1990 in Bern mit Excel.\nSchreibt R-Code, um die gleiche Berechnung durchzuführen.\nHinweis: Chatbots können neuerdings Datenanalysen ohne Programmierkenntnisse durchführen. Siehe: Data Analysis with ChatGPT\nAufgabe: Überprüft den generierten Code und diskutiert die Vorteile, Nachteile und Risiken der drei Methoden.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>R-Übungen 3</span>"
    ]
  },
  {
    "objectID": "excercises/ex03.html#klimadiagramm",
    "href": "excercises/ex03.html#klimadiagramm",
    "title": "Appendix C — R-Übungen 3",
    "section": "C.2 Klimadiagramm",
    "text": "C.2 Klimadiagramm\n\nC.2.1 Schritte\n\nDatensatz laden:\n\nmeteodaten_tag.csv (nach Excel-Export in R):\n\n\n\ndata &lt;- read.csv(\"meteodaten_tag.csv\", na.strings = c(\"-\", \"NA\"))\n\n\nDatenstruktur überprüfen:\n\nstr(data)\n\nPrüfen, ob die Daten korrekt (z. B. numerisch) gelesen wurden.\nHistogramm erstellen:\n\nMit den Tagestemperaturen (z. B. mit feinen Abständen):\n\n\nhist(data$temp, breaks = 40)\n\nMonatsmittelwerte berechnen:\n\nTemperatur und Bewölkung über alle Jahre (z. B. Mittelwerte für jeden Monat).\nAchtung: Fehlwerte berücksichtigen.\n\nPlot erstellen:\n\nZwei Barplots (Temperatur und Bewölkung) übereinander:\n\n\npar(mfrow = c(2, 1))\nbarplot(temperature_means, main = \"Monatsmittelwerte Temperatur\")\nbarplot(cloud_cover_means, main = \"Monatsmittelwerte Bewölkung\")\n\n\nErwartet: Welche Trends zeigen die Ergebnisse?\n\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nComming soon!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>R-Übungen 3</span>"
    ]
  },
  {
    "objectID": "excercises/ex03.html#boxplots",
    "href": "excercises/ex03.html#boxplots",
    "title": "Appendix C — R-Übungen 3",
    "section": "C.3 Boxplots",
    "text": "C.3 Boxplots\n\nC.3.1 Aufgabe\n\nWählt den Zeitraum 2000-2001 in den täglichen Daten, z. B.:\n\nzeit &lt;- meteodaten_tag[meteodaten_tag[, 1] &gt;= 2000 & meteodaten_tag[, 1] &lt;= 2001, ]\n\nBoxplot erstellen:\n\nTemperaturen als Funktion der Bewölkung:\n\nboxplot(temp ~ cloud_cover, data = zeit)\n\nAchsen beschriften und Titel vergeben.\n\nAnalyse:\n\nUnter welchen Bewölkungsbedingungen ist die Spannweite/Varianz der Temperatur am größten?\nFindet den bewölkungsärmsten und bewölkungsreichsten Monat (im Mittel der zwei Jahre). Wie hoch ist die mittlere Bewölkung (in Oktas)?\n\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nComming soon!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>R-Übungen 3</span>"
    ]
  },
  {
    "objectID": "excercises/ex03.html#r-als-gis-ersatz",
    "href": "excercises/ex03.html#r-als-gis-ersatz",
    "title": "Appendix C — R-Übungen 3",
    "section": "C.4 R als GIS-Ersatz",
    "text": "C.4 R als GIS-Ersatz\n\nC.4.1 Schritte\n\nPaket installieren und laden:\n\n\ninstall.packages(\"maps\")\nlibrary(maps)\n\n\nKoordinaten herausfinden:\n\nFür Genf und Gr. S. Bernhard (ca.).\n\nEuropakarte erstellen:\n\nLeeren Plot erstellen:\n\n\n\nplot(x = c(-5, 30), y = c(35, 60), type = \"n\", xlab = \"lon\", ylab = \"lat\")\n\n\nWeltkarte hinzufügen:\n\n\nmap(\"world\", add = TRUE)\n\n\nStationen hinzufügen:\n\nStationen als Punkte plotten (mit unterschiedlichen Farben/Symbolen):\n\n\n\npoints(x_coord, y_coord, col = \"red\", pch = 19)\ntext(x_coord, y_coord, labels = station_names, pos = 4)\n\n\nHinweis: Für genaue Koordinaten verwendet Google!\n\n\n\n\n\n\n\nLösung\n\n\n\n\n\nComming soon!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>R-Übungen 3</span>"
    ]
  },
  {
    "objectID": "excercises/deskriptive-statistik.html",
    "href": "excercises/deskriptive-statistik.html",
    "title": "Appendix D — Übung 4: Deskriptive Statistik und Visualisierung",
    "section": "",
    "text": "D.1 Histogramm und Kennzahlen\nArbeite mit den saisonalen Meteodaten, die wir im R-Kurs eingelesen haben\nIhr möchtet die mittlere Jahresniederschlagssumme und die mittlere Jahrestemperatur sowie deren Varianz von Jahr zu Jahr bestimmen, um die Klimabedingungen in Bern zu beschreiben.\nUm die richtigen Kennzahlen (Mittelwert, Median, Modus, etc.) zu wählen, müssen wir die Verteilung der Daten kennen. Erstellt ein Histogramm der Jahresmitteltemperaturen und -niederschlag:\n# Jahresdaten erstellen\n# Berechne Jahresmittelwerte und -summen\njahresmitteltemp_bern &lt;- aggregate(meteodaten$Bern_Mitteltemperatur ~ meteodaten$Jahr,\n                                   FUN = mean,\n                                   na.rm = TRUE)\njahresmitteltemp_grstbernhard &lt;- aggregate(meteodaten$GrStBernhard_Mitteltemperatur ~ meteodaten$Jahr,\n                                           FUN = mean,\n                                           na.rm = TRUE)\njahresniederschlag_bern &lt;- aggregate(meteodaten$Bern_Niederschlagssumme ~ meteodaten$Jahr,\n                                     FUN = sum,\n                                     na.rm = TRUE)\njahresniederschlag_grstbernhard &lt;- aggregate(meteodaten$GrStBernhard_Niederschlagssumme ~ meteodaten$Jahr,\n                                             FUN = sum,\n                                             na.rm = TRUE)\n\n# Kombiniere die Ergebnisse in eine neue Tabelle\njahresdaten &lt;- data.frame(\n  Jahr = jahresmitteltemp_bern$`meteodaten$Jahr`,\n  Bern_Mitteltemperatur = jahresmitteltemp_bern$`meteodaten$Bern_Mitteltemperatur`,\n  GrStBernhard_Mitteltemperatur = jahresmitteltemp_grstbernhard$`meteodaten$GrStBernhard_Mitteltemperatur`,\n  Bern_Niederschlagssumme = jahresniederschlag_bern$`meteodaten$Bern_Niederschlagssumme`,\n  GrStBernhard_Niederschlagssumme = jahresniederschlag_grstbernhard$`meteodaten$GrStBernhard_Niederschlagssumme`\n)\n\n\nhist(jahresdaten$Bern_Mitteltemperatur,\n     main = 'Histogramm der Mitteltemperatur in Bern',\n     xlab = 'Mitteltemperatur (°C)',\n     ylab = 'Anzahl der Jahre',\n     xlim = c(6, 12),\n     ylim = c(0, 15),\n     breaks = 30)\n\n\n\n\n\n\n\nhist(jahresdaten$Bern_Niederschlagssumme,\n     main = 'Histogramm der Niederschlagssumme in Bern',\n     xlab = 'Niederschlagssumme (mm)',\n     ylab = 'Anzahl der Jahre',\n     xlim = c(500, 1500),\n     ylim = c(0, 10),\n     breaks = 30)\nBeide Verteilungen sind ungefähr symmetrisch, da die Daten in der Mitte des Bereichs konzentriert sind und die Verteilung nach links und rechts ungefähr gleich ist. Dies ist aber mit so “wenigen” Daten nicht wirklich aussagekräftig. Das macht aber bei der Art der Daten Sinn, da die Temperatur und Niederschlagssumme in der Regel normalverteilt sind.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Übung 4: Deskriptive Statistik und Visualisierung</span>"
    ]
  },
  {
    "objectID": "excercises/deskriptive-statistik.html#histogramm-und-kennzahlen",
    "href": "excercises/deskriptive-statistik.html#histogramm-und-kennzahlen",
    "title": "Appendix D — Übung 4: Deskriptive Statistik und Visualisierung",
    "section": "",
    "text": "D.1.1 Kennzahlen\nWelche Kennzahlen zur Beschreibung des Mittels und der Streuung kommen aufgrund den Verteilung und Skala der Daten in Frage?\nDa die Daten normalverteilt sind, können wir den Mittelwert und die Standardabweichung verwenden, um die zentrale Tendenz und die Streuung der Daten zu beschreiben. Der Median ist auch eine gute Kennzahl, um die zentrale Tendenz zu beschreiben, da er robust gegenüber Ausreissern ist.\n\nBerechne Mittelwert, Median, Spannweite und Standardabweichung des jährlichen Temperaturen in Bern und schaue dir zusätzlich die Ausgabe der summary() Funktion von R an.\n\n\nmean_temp_bern &lt;- mean(jahresdaten$Bern_Mitteltemperatur)\nprint(mean_temp_bern)\n\n[1] 8.715176\n\nmedian_temp_bern &lt;- median(jahresdaten$Bern_Mitteltemperatur)\nprint(median_temp_bern)\n\n[1] 8.666667\n\nrange_temp_bern &lt;- diff(range(jahresdaten$Bern_Mitteltemperatur))\nrange(jahresdaten$Bern_Mitteltemperatur)\n\n[1]  7.091667 11.050000\n\nprint(range_temp_bern)\n\n[1] 3.958333\n\nsd_temp_bern &lt;- sd(jahresdaten$Bern_Mitteltemperatur)\nprint(sd_temp_bern)\n\n[1] 0.8185026\n\nsummary(jahresdaten$Bern_Mitteltemperatur)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  7.092   8.104   8.667   8.715   9.221  11.050 \n\n\nDer Unterschied zwischen Mittelwert und Median (0.05°C) ist sehr klein, was darauf hindeutet, dass die Verteilung der Daten symmetrisch ist. Die Spannweite der Daten beträgt 3.96 °C, was darauf hindeutet, dass die Daten relativ eng um den Mittelwert verteilt sind. Die Standardabweichung beträgt 0.82°C, was darauf hindeutet, dass die Daten relativ homogen um den Mittelwert verteilt sind.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Übung 4: Deskriptive Statistik und Visualisierung</span>"
    ]
  },
  {
    "objectID": "excercises/deskriptive-statistik.html#kontingenztabelle",
    "href": "excercises/deskriptive-statistik.html#kontingenztabelle",
    "title": "Appendix D — Übung 4: Deskriptive Statistik und Visualisierung",
    "section": "D.2 Kontingenztabelle",
    "text": "D.2 Kontingenztabelle\n\nKonvertiere die Spalte mit den Niederschlagssummen in Bern in Klassen, die jeweils 100 mm umfassen (Tipp: z.B. mit round() Funktion für Klassen: &lt;50mm, 50-150mm, 150-250mm, …) Erstelle die Kontingenztabelle für die Anzahl der Regensummen in den 100 mm Klassen in den vier Jahreszeiten (table() Funktion)\n\n                  0 100 200 300 400 500 600\nFruehling(MAM)  ?  ?   ?   ?   ?   ?   ?\nHerbst(SON)     ?  ?   ?   ?   ?   ?   ?\nSommer(JJA)     ?  ?   ?   ?   ?   ?   ?\nWinter(DJF)     ?  ?   ?   ?   ?   ?   ?\nKontingenztabelle mit R erstellen\n\n# 1. Erstelle Klassen für Niederschlagssummen in Bern in 100-mm-Schritten\n# Die Spalte 'Saison' enthält die Jahreszeiten (z.B. 'Fruehling(MAM)', 'Sommer(JJA)')\n# Die Spalte 'Bern_Niederschlagssumme' enthält die Niederschlagssummen\n\n# Klassen für Niederschlagssummen in 100-mm-Schritten erstellen\nmeteodaten$Niederschlag_klassen &lt;- cut(\n  meteodaten$Bern_Niederschlagssumme,\n  breaks = seq(0, max(meteodaten$Bern_Niederschlagssumme, na.rm = TRUE), by = 100),\n  include.lowest = TRUE,\n  right = FALSE\n)\n\nDie Einteilung liesse sich auch einfacher machen, aber etwas weniger hübsch…\n\n# Klassen für Niederschlagssummen in 100-mm-Schritten erstellen\nmeteodaten$Niederschlag_klassen &lt;-\n            round(meteodaten$Bern_Niederschlagssumme/100)*100\n\nWir arbeiten weiter mit der ersten Lösung.\n\n# 2. Erstelle eine Kontingenztabelle\nkontingenz_tabelle &lt;- table(\n  meteodaten$Saison,\n  meteodaten$Niederschlag_klassen\n)\n\n# 3. Zeige die Kontingenztabelle an\nprint(kontingenz_tabelle)\n\n                \n                 [0,100) [100,200) [200,300) [300,400) [400,500) [500,600]\n  Fruehling(MAM)       0        36        60        25         1         1\n  Herbst(SON)          2        38        53        24         6         0\n  Sommer(JJA)          0         7        40        41        31         3\n  Winter(DJF)         14        59        45         5         0         0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Übung 4: Deskriptive Statistik und Visualisierung</span>"
    ]
  },
  {
    "objectID": "excercises/deskriptive-statistik.html#visualisierung-des-erwärmungstrends",
    "href": "excercises/deskriptive-statistik.html#visualisierung-des-erwärmungstrends",
    "title": "Appendix D — Übung 4: Deskriptive Statistik und Visualisierung",
    "section": "D.3 Visualisierung des Erwärmungstrends",
    "text": "D.3 Visualisierung des Erwärmungstrends\n\nVisualisiert die Erwärmungstrend der Station Bern mit einem Liniendiagramm, indem du die Jahresmitteltemperatur darstellst und die 31-jährige Gleitende Mittel (auch “running mean” genannt z.B. mit der Funktion runmean() aus der Bibliothek “caTools”) hinzufügst.\n\n\nD.3.1 Liniendiagramm mit Gleitendem Mittel\n\n# Bibliothek caTools laden\nlibrary(caTools)\n\n# Berechnung des 31-jährigen gleitenden Mittels (Running Mean)\n# Die Spalte für die Mitteltemperatur in Bern heisst 'Bern_Mitteltemperatur'\ngleitendes_mittel &lt;- runmean(jahresdaten$Bern_Mitteltemperatur, 31, align = \"center\", endrule = \"mean\")\n\n# Liniendiagramm erstellen\nplot(jahresdaten$Jahr, jahresdaten$Bern_Mitteltemperatur, type = \"l\", col = \"blue\",\n     xlab = \"Jahr\", ylab = \"Mitteltemperatur in Bern (°C)\",\n     main = \"Erwärmungstrend der Station Bern mit 31-jährigem Gleitendem Mittel\")\n\n# Hinzufügen der Gleitenden Mittel-Linie (31-jähriges Running Mean)\nlines(jahresdaten$Jahr, gleitendes_mittel, col = \"red\", lwd = 2)\n\n# Legende hinzufügen\nlegend(\"topright\", legend = c(\"Jahresmitteltemperatur\", \"31-jähriges Gleitendes Mittel\"),\n       col = c(\"blue\", \"red\"), lty = 1, lwd = 2)\n\n\n\n\n\n\n\n\nErstelle zusätzlich zwei Abbildungen der Temperaturanomalien wie hier und hier:\n\n\nD.3.2 Warming Stripes\n\n# Bibliothek\n# Bibliothek\nlibrary(ggplot2)\n\n# Berechnung der Abweichung der Mitteltemperatur von der Referenzperiode 1961-1990\njahresdaten$Abweichung &lt;- jahresdaten$Bern_Mitteltemperatur - mean(jahresdaten$Bern_Mitteltemperatur[jahresdaten$Jahr &gt;= 1961 & jahresdaten$Jahr &lt;= 1990])\n\n# Erstelle die \"Warming Stripes\" mit Legende\nggplot(jahresdaten, aes(x = Jahr, y = 1, fill = Abweichung)) +\n  geom_tile() +\n  scale_fill_gradientn(\n    colours = c(\"blue\", \"lightblue\", \"white\", \"orange\", \"red\", \"darkred\"),\n    name = \"Temperaturabweichung (°C)\"\n  ) +\n  theme_void() +  # Entfernt Achsen, Titel etc.\n  theme(legend.position = \"bottom\", legend.title = element_text(size = 10)) +\n  labs(title = \"Schweizer Temperatur seit 1864\")\n\n\n\n\nWarming Stripes im vergleich zur Referenzperiode 1961-1990. Nach R-Bloggers\n\n\n\n\n\n\nD.3.3 Barplot der Temperaturanomalien\n\n# Bibliotheken\nlibrary(ggplot2)\n\n# Erstelle einen Barplot, der die Abweichungen darstellt\nggplot(jahresdaten, aes(x = Jahr, y = Abweichung, fill = Abweichung &gt; 0)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) +\n  scale_fill_manual(values = c(\"blue\", \"red\")) +  # Farben: Blau für kälter, Rot für wärmer\n  theme_minimal() +\n  labs(title = \"Jahres-Temperatur Abweichungen – Bern\",\n       x = \"Jahr\",\n       y = \"Abweichung in °C\") +\n  theme(plot.title = element_text(hjust = 0.5))  # Zentriere den Titel\n\n\n\n\nBarplot der Temperaturanomalien zur Vergleichsperiode 1961–1990.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Übung 4: Deskriptive Statistik und Visualisierung</span>"
    ]
  },
  {
    "objectID": "excercises/verteilungen-und-tests.html",
    "href": "excercises/verteilungen-und-tests.html",
    "title": "Appendix E — Übung 5 & 6: Verteilungen und Tests",
    "section": "",
    "text": "E.1 Statistische Tests mit Psychologieexperiment\nErstellt mit R das Histogramm und die Verteilungsfunktion des Sommer-Niederschlags am Grossen St. Bernhard (Datei mit Saison-Mittelwerten). Tipp: In R kann die empirische Verteilungsfunktion mit der Funktion ecdf() erzeugt werden.\nLies aus dem Plot der Verteilungsfunktion ungefähr ab, welcher Niederschlag in 20% der Jahre überschritten wird?\nANTWORT: Die Niederschlagssumme, die in 20% der Jahre überschritten wird, beträgt 647.2 mm.\nDaten einlesen\npsychologieExperiment &lt;- read.table('Data/Psycho_Exp_Ergebnisse2_2024-10-28.csv',\n                                    sep = ',',\n                                    header = TRUE,\n                                    na.strings = '999')\n\n# Spalten im DataFrame umbenennen (kürzere und schönere Namen)\ncolnames(psychologieExperiment) &lt;- c(\n  \"Gefuehl_Vor_SelberGutesTun\",\n  \"Gefuehl_Nach_SelberGutesTun\",\n  \"Gefuehl_Vor_AnderenGutesTun\",\n  \"Gefuehl_Nach_AnderenGutesTun\")\nGehen wir mal davon aus, dass die Mittelwerte interpretierbar seien. (Nächste Woche lernen wir weitere Test für Verteilungen kennen und Test, wenn die Normalverteilung nicht gegeben ist.) Dann könnten wir untersuchen, wie stark sich die Mittelwerte vor und nach dem Experiment unterscheiden und ob die Unterschiede statistisch signifikant sind.\nStellt nun zunächst die Null- und Alternativhypothesen für beide Experimente (1. sich selber und 2. anderen etwas gutes tun) auf, ob etwas Gutes tun sich auf das Wohlbefinden auswirkt.\nH0: Die beiden Mittelwerte sind gleich\nHA: Jemandem oder uns selbst etwas Gutes zu tun, hat einen (positiven) Einfluss auf das Wohlbefinden\nErstelle einen Boxplot, um die Verteilung der Daten anzusehen und einen ersten Eindruck zu erhalten\nboxplot(psychologieExperiment,\n        main = \"Boxplot der Gefühle vor und nach dem Experiment\",\n        ylab = \"Gefühle\",\n        col = c(\"lightblue\", \"lightgreen\"),\n        names = c(\"Gefühl, vor dem man sich selbst etwas Gutes tut\",\n                  \"Gefühl, nach dem man sich selbst etwas Gutes tut\",\n                  \"Gefühl, vor dem man jemand anderem etwas Gutes tut\",\n                  \"Gefühl, nach dem man jemand anderem etwas Gutes tut\"),\n        las = 2)\nBerechne wie gross die Unterschiede der Mittelwerte sind?\nDiffSelberGutesTun &lt;- mean(psychologieExperiment$Gefuehl_Nach_SelberGutesTun,\n                           na.rm = TRUE) -\n                      mean(psychologieExperiment$Gefuehl_Vor_SelberGutesTun,\n                           na.rm = TRUE)\nprint(DiffSelberGutesTun)\n\n[1] 2.027027\n\nDiffAnderenGutesTun &lt;- mean(psychologieExperiment$Gefuehl_Nach_AnderenGutesTun,\n                            na.rm = TRUE) -\n                       mean(psychologieExperiment$Gefuehl_Vor_AnderenGutesTun,\n                            na.rm = TRUE)\nprint(DiffAnderenGutesTun)\n\n[1] 1.918919\nInterpretiere die Unterschiede!\nANTWORT: Beide Effekte sind positiv. Fast identisch grosse Effekte.\nFühre nun einen 2-Stichproben T-Test mit R durch. Schau dir mit der Hilfe unter ?t.test die Parameter der t.test Funktion in R an. Wähle entsprechend einen ein- oder zweiseitigen Test, das Konfidenzlevel mit 99% und gehe davon aus, dass die Varianzen beider Stichproben gleich sind. Recherchiere, ob es sich um abhängige oder unabhängige Stichproben handelt. Gibt den entsprechenden Parameter bei der Nutzung der t.test Funktion an.\nt.test(psychologieExperiment$Gefuehl_Nach_SelberGutesTun,\n       psychologieExperiment$Gefuehl_Vor_SelberGutesTun,\n       alternative = \"greater\",\n       paired = TRUE,\n       equal.var = TRUE,\n       conf.level = 0.99)\n\n\n    Paired t-test\n\ndata:  psychologieExperiment$Gefuehl_Nach_SelberGutesTun and psychologieExperiment$Gefuehl_Vor_SelberGutesTun\nt = 9.8012, df = 36, p-value = 5.298e-12\nalternative hypothesis: true mean difference is greater than 0\n99 percent confidence interval:\n 1.523537      Inf\nsample estimates:\nmean difference \n       2.027027 \n\nt.test(psychologieExperiment$Gefuehl_Nach_AnderenGutesTun,\n       psychologieExperiment$Gefuehl_Vor_AnderenGutesTun,\n       alternative = \"greater\",\n       paired = TRUE,\n       equal.var = TRUE,\n       conf.level = 0.99)\n\n\n    Paired t-test\n\ndata:  psychologieExperiment$Gefuehl_Nach_AnderenGutesTun and psychologieExperiment$Gefuehl_Vor_AnderenGutesTun\nt = 8.5716, df = 36, p-value = 1.61e-10\nalternative hypothesis: true mean difference is greater than 0\n99 percent confidence interval:\n 1.373909      Inf\nsample estimates:\nmean difference \n       1.918919\nInterpretiere die Ausgabe des Tests (Konfidenzintervalle besprechen wir nächste Woche).\nANTWORT:\nIhr könntest mit einem gleichen t-Test auch testen, ob der Effekt beider Experimente gleich gross ist, d.h. wirkt es sich gleich oder unterschiedlich auf das Befinden aus, ob man sich selbst oder anderen etwas Gutes tut. Wie würdet ihr hierfür vorgehen?\nH0: Ob ich anderen oder mir selber tue underscheidet sich nicht in der Auswirkung auf mein Wohlbefinden.\nHA: Ob ich anderen oder mir selber tue unterscheidet sich in der Auswirkung auf mein Wohlbefinden.\nDiffAuswirkung &lt;- DiffSelberGutesTun - DiffAnderenGutesTun\nprint(DiffAuswirkung)\n\n[1] 0.1081081\nt.test(psychologieExperiment$Gefuehl_Nach_SelberGutesTun -\n         psychologieExperiment$Gefuehl_Vor_SelberGutesTun,\n       psychologieExperiment$Gefuehl_Nach_AnderenGutesTun -\n         psychologieExperiment$Gefuehl_Vor_AnderenGutesTun,\n       alternative = \"greater\",\n       paired = TRUE,\n       equal.var = TRUE,\n       conf.level = 0.99)\n\n\n    Paired t-test\n\ndata:  psychologieExperiment$Gefuehl_Nach_SelberGutesTun - psychologieExperiment$Gefuehl_Vor_SelberGutesTun and psychologieExperiment$Gefuehl_Nach_AnderenGutesTun - psychologieExperiment$Gefuehl_Vor_AnderenGutesTun\nt = 0.48727, df = 36, p-value = 0.3145\nalternative hypothesis: true mean difference is greater than 0\n99 percent confidence interval:\n -0.4320254        Inf\nsample estimates:\nmean difference \n      0.1081081\nANTWORT:\nMit dem t-Wert von ca. 0.5 und einem p-Wert von ca. 0.3 können wir die Nullhypothese nicht ablehnen. Es gibt keinen statistisch signifikanten Unterschied zwischen den beiden Experimenten.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Übung 5 & 6: Verteilungen und Tests</span>"
    ]
  },
  {
    "objectID": "excercises/verteilungen-und-tests.html#statistische-tests-mit-psychologieexperiment",
    "href": "excercises/verteilungen-und-tests.html#statistische-tests-mit-psychologieexperiment",
    "title": "Appendix E — Übung 5 & 6: Verteilungen und Tests",
    "section": "",
    "text": "Hypothesen aufstellen\n\n\n\n\nBerechnungen\n\n\n\nt-Test\n\n\n\nInterpretation aller ausgegebenen Ergebnisse der R t-test() Funktion.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Übung 5 & 6: Verteilungen und Tests</span>"
    ]
  },
  {
    "objectID": "excercises/verteilungen-und-tests.html#chi2-verteilungstest-zu-würfelexperiment",
    "href": "excercises/verteilungen-und-tests.html#chi2-verteilungstest-zu-würfelexperiment",
    "title": "Appendix E — Übung 5 & 6: Verteilungen und Tests",
    "section": "E.2 Chi^2 Verteilungstest zu Würfelexperiment",
    "text": "E.2 Chi^2 Verteilungstest zu Würfelexperiment\n\n# Daten einlesen\nwuerfel_daten &lt;- read.table(\n  \"Data/alle_wuerfel.csv\",\n  sep = \",\",\n  header = TRUE\n)\n\n# Initialisierung eines Vektors für p-Werte\np_werte &lt;- rep(NA, 4)\n\n# Schleife durch die relevanten Spalten (Spalten 5 bis 32)\nfor (spalte in 5:32) {\n  # Durchführung des Chi-Quadrat-Tests\n  chi_quadrat_test &lt;- chisq.test(x = wuerfel_daten[, spalte],\n                                 p = wuerfel_daten[, 3])  # Erwartete Wahrscheinlichkeiten in Spalte 3\n\n  # p-Wert speichern\n  p_werte &lt;- c(p_werte, chi_quadrat_test$p.value)\n\n  # Barplot für absolute Häufigkeiten\n  barplot(\n    wuerfel_daten[, 4],\n    names.arg = wuerfel_daten[, 1],\n    col = \"grey\",\n    ylim = c(0, 15),\n    main = colnames(wuerfel_daten)[spalte]\n  )\n\n  # Neuen Plot für die Vergleichsdaten auf derselben Grafik\n  par(new = TRUE)\n  barplot(\n    wuerfel_daten[, spalte],\n    col = rgb(1, 0, 0, 0.5, maxColorValue = 1),\n    ylim = c(0, 15)\n  )\n\n  # Legende hinzufügen, die den p-Wert anzeigt\n  legend(\n    \"topleft\",\n    paste(\"p-Wert:\", round(chi_quadrat_test$p.value, 2),\n          ifelse(chi_quadrat_test$p.value &lt; 0.05, \"erfunden\",\n                 ifelse(\n                   chi_quadrat_test$p.value &gt; 0.9,\n                      \"wahrscheinlich erfunden\",\n                      \"gewürfelt\")\n                 )\n          )\n  )\n\n  # Anhalten für visuelle Überprüfung\n  # cat(\"Drücke Enter, um fortzufahren...\")\n  # readline()\n}\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n\nWarning in chisq.test(x = wuerfel_daten[, spalte], p = wuerfel_daten[, 3]):\nChi-squared approximation may be incorrect\n\n\n\n\n\n\n\n\n# Ergebniszeilen für die Interpretation der p-Werte hinzufügen\ninterpretation &lt;- ifelse(\n  p_werte &lt; 0.05 | p_werte &gt; 0.9,\n  \"erfunden\", \"gewürfelt\"\n)\n\n# p-Werte und die neue Interpretation an den DataFrame anhängen\nwuerfel_daten &lt;- rbind(\n  wuerfel_daten,\n  p_werte,\n  interpretation\n)\n\n# Überarbeiteter DataFrame mit neuen Zeilen für p-Werte und ihre Interpretation",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Übung 5 & 6: Verteilungen und Tests</span>"
    ]
  },
  {
    "objectID": "excercises/verteilungen-und-tests.html#psychologie-experiment",
    "href": "excercises/verteilungen-und-tests.html#psychologie-experiment",
    "title": "Appendix E — Übung 5 & 6: Verteilungen und Tests",
    "section": "E.3 Psychologie Experiment",
    "text": "E.3 Psychologie Experiment\nDaten einlesen\n\npsychologieExperiment &lt;- read.table(\n  'Data/Psycho_Exp_Ergebnisse2_2024-10-28.csv',\n  sep = ',',\n  header = TRUE,\n  na.strings = '999'\n)\n\n# Spalten im DataFrame umbenennen\ncolnames(psychologieExperiment) &lt;- c(\n  \"Gefuehl_Vor_SelberGutesTun\",\n  \"Gefuehl_Nach_SelberGutesTun\",\n  \"Gefuehl_Vor_AnderenGutesTun\",\n  \"Gefuehl_Nach_AnderenGutesTun\"\n)\n\nErstelle sogenannte QQ Plots und führe den Shapiro Test auf Normalverteilung durch\n\n# QQ-Plots für die vier Spalten erstellen\n\nqqnorm(psychologieExperiment$Gefuehl_Vor_SelberGutesTun)\nqqline(psychologieExperiment$Gefuehl_Vor_SelberGutesTun)\n\n\n\n\n\n\n\nqqnorm(psychologieExperiment$Gefuehl_Nach_SelberGutesTun)\nqqline(psychologieExperiment$Gefuehl_Nach_SelberGutesTun)\n\n\n\n\n\n\n\nqqnorm(psychologieExperiment$Gefuehl_Vor_AnderenGutesTun)\nqqline(psychologieExperiment$Gefuehl_Vor_AnderenGutesTun)\n\n\n\n\n\n\n\nqqnorm(psychologieExperiment$Gefuehl_Nach_AnderenGutesTun)\nqqline(psychologieExperiment$Gefuehl_Nach_AnderenGutesTun)\n\n\n\n\n\n\n\n# Shapiro-Wilk-Test für die Normalverteilung durchführen\nshapiro_test_vor_selber &lt;- shapiro.test(psychologieExperiment$Gefuehl_Vor_SelberGutesTun)\nshapiro_test_nach_selber &lt;- shapiro.test(psychologieExperiment$Gefuehl_Nach_SelberGutesTun)\nshapiro_test_vor_anderen &lt;- shapiro.test(psychologieExperiment$Gefuehl_Vor_AnderenGutesTun)\nshapiro_test_nach_anderen &lt;- shapiro.test(psychologieExperiment$Gefuehl_Nach_AnderenGutesTun)\n\n# Shapiro-Wilk-Testergebnisse ausgeben\nshapiro_test_vor_selber\n\n\n    Shapiro-Wilk normality test\n\ndata:  psychologieExperiment$Gefuehl_Vor_SelberGutesTun\nW = 0.94075, p-value = 0.04874\n\nshapiro_test_nach_selber\n\n\n    Shapiro-Wilk normality test\n\ndata:  psychologieExperiment$Gefuehl_Nach_SelberGutesTun\nW = 0.88499, p-value = 0.001157\n\nshapiro_test_vor_anderen\n\n\n    Shapiro-Wilk normality test\n\ndata:  psychologieExperiment$Gefuehl_Vor_AnderenGutesTun\nW = 0.93818, p-value = 0.0404\n\nshapiro_test_nach_anderen\n\n\n    Shapiro-Wilk normality test\n\ndata:  psychologieExperiment$Gefuehl_Nach_AnderenGutesTun\nW = 0.8824, p-value = 0.0009906\n\n\nSind alle Daten normalverteilt?\nDie Ergebnisse des Shapiro-Wilk-Tests zeigen, dass der p-Wert für alle vier Variablen unter 0.05 liegt:\n\nGefühl Vor Selber Gutes Tun: p-Wert = 0.0487385\nGefühl Nach Selber Gutes Tun: p-Wert = 0.0011574\nGefühl Vor Anderen Gutes Tun: p-Wert = 0.0404011\nGefühl Nach Anderen Gutes Tun: p-Wert = 9.9057037^{-4}\n\nDa alle p-Werte unter 0.05 liegen, können wir die Nullhypothese der Normalverteilung für alle Variablen ablehnen.\nAntwort: Nein, die Daten sind nicht normalverteilt.\nBeim t-Test hatten wir Gleichheit der Varianzen angenommen. Testet hier, ob diese Annahme korrekt war?\n\nlibrary(car)  # Für den Levene-Test\n\nLoading required package: carData\n\n# Levene-Test für Gleichheit der Varianzen\nlevene_test_result &lt;- leveneTest(\n  c(psychologieExperiment$Gefuehl_Nach_SelberGutesTun, psychologieExperiment$Gefuehl_Vor_SelberGutesTun),\n  group = rep(c(\"Nach\", \"Vor\"), each = nrow(psychologieExperiment))\n)\n\nWarning in\nleveneTest.default(c(psychologieExperiment$Gefuehl_Nach_SelberGutesTun, :\nrep(c(\"Nach\", \"Vor\"), each = nrow(psychologieExperiment)) coerced to factor.\n\n# Ausgabe des Testergebnisses\n\nprint(levene_test_result)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  0.0147 0.9039\n      72               \n\npWertLevene &lt;- levene_test_result$`Pr(&gt;F)`[1]\n\nANTWORT:\nDie Ausgabe des Levene-Tests zeigt Folgendes:\n\np-Wert des Levene-Tests: 0.9039338\n\nInterpretation:\nDer p-Wert ist deutlich grösser als 0.05, was darauf hindeutet, dass die Nullhypothese der Gleichheit der Varianzen nicht abgelehnt wird. Das bedeutet, dass es keinen statistisch signifikanten Unterschied in den Varianzen der Gruppen gibt.\nAntwort: Ja, die Annahme der Gleichheit der Varianzen beim t-Test war korrekt.\nWie stark verändert sich der Median in den beiden Experimenten von Bevor zu Danach?\n\n# Berechnung der Mediane für die Bedingungen\nmedian_vor_selber &lt;- median(psychologieExperiment$Gefuehl_Vor_SelberGutesTun, na.rm = TRUE)\nmedian_nach_selber &lt;- median(psychologieExperiment$Gefuehl_Nach_SelberGutesTun, na.rm = TRUE)\nmedian_vor_anderen &lt;- median(psychologieExperiment$Gefuehl_Vor_AnderenGutesTun, na.rm = TRUE)\nmedian_nach_anderen &lt;- median(psychologieExperiment$Gefuehl_Nach_AnderenGutesTun, na.rm = TRUE)\n\n# Berechnung der Veränderungen der Mediane\ndiff_median_selber &lt;- median_nach_selber - median_vor_selber\ndiff_median_anderen &lt;- median_nach_anderen - median_vor_anderen\n\n# Ausgabe der Ergebnisse\ncat(\"Veränderung des Medians für Selber Gutes Tun:\", diff_median_selber, \"\\n\")\n\nVeränderung des Medians für Selber Gutes Tun: 2 \n\ncat(\"Veränderung des Medians für Anderen Gutes Tun:\", diff_median_anderen, \"\\n\")\n\nVeränderung des Medians für Anderen Gutes Tun: 2 \n\n\nANTWORT:\nSuche mit Entscheidungsbäumen, Chatbots, Internetsuche, etc. welcher statistische Test sich zum Vergleich der zentralen Tendenz dieser Daten eignet? ANTWORT: Wilcoxon-Vorzeichen-Rang-Test. Dieser vergleicht die Mediane von zwei abhängigen Stichproben.\nParameterfreier Tests, d.h. unabhängig von Verteilung der Daten\n\n# Wilcoxon-Vorzeichen-Rang-Tests für beide Experimente\nwilcox_test_selber &lt;- wilcox.test(\n  psychologieExperiment$Gefuehl_Vor_SelberGutesTun,\n  psychologieExperiment$Gefuehl_Nach_SelberGutesTun,\n  paired = TRUE\n)\n\nWarning in\nwilcox.test.default(psychologieExperiment$Gefuehl_Vor_SelberGutesTun, : cannot\ncompute exact p-value with ties\n\n\nWarning in\nwilcox.test.default(psychologieExperiment$Gefuehl_Vor_SelberGutesTun, : cannot\ncompute exact p-value with zeroes\n\nwilcox_test_anderen &lt;- wilcox.test(\n  psychologieExperiment$Gefuehl_Vor_AnderenGutesTun,\n  psychologieExperiment$Gefuehl_Nach_AnderenGutesTun,\n  paired = TRUE\n)\n\nWarning in\nwilcox.test.default(psychologieExperiment$Gefuehl_Vor_AnderenGutesTun, : cannot\ncompute exact p-value with ties\n\n\nWarning in\nwilcox.test.default(psychologieExperiment$Gefuehl_Vor_AnderenGutesTun, : cannot\ncompute exact p-value with zeroes\n\n# Ausgabe der Testergebnisse\nwilcox_test_selber\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  psychologieExperiment$Gefuehl_Vor_SelberGutesTun and psychologieExperiment$Gefuehl_Nach_SelberGutesTun\nV = 0, p-value = 2.768e-07\nalternative hypothesis: true location shift is not equal to 0\n\nwilcox_test_anderen\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  psychologieExperiment$Gefuehl_Vor_AnderenGutesTun and psychologieExperiment$Gefuehl_Nach_AnderenGutesTun\nV = 28, p-value = 2.051e-06\nalternative hypothesis: true location shift is not equal to 0\n\n\nWie interpretierst du die Tests?\n\nE.3.1 Interpretation der Warnungen und Testergebnisse:\n\nE.3.1.1 Testergebnisse:\n\nWilcoxon-Test für Selber Gutes Tun:\n\nV-Wert: 0\np-Wert: 2.7683169^{-7}\n\n\nDer p-Wert ist viel kleiner als 0.05, was bedeutet, dass die Veränderung der Mediane statistisch signifikant ist. Die Nullhypothese (kein Unterschied der zentralen Tendenz) kann abgelehnt werden, was darauf hinweist, dass das Experiment „Selber Gutes Tun“ eine signifikante Veränderung im Median bewirkt hat.\n\nWilcoxon-Test für Anderen Gutes Tun:\n\nV-Wert: 28\np-Wert: 2.0506755^{-6}\n\n\nAuch hier ist der p-Wert viel kleiner als 0.05. Die Nullhypothese kann abgelehnt werden, was zeigt, dass auch das Experiment „Anderen Gutes Tun“ eine signifikante Veränderung im Median bewirkt hat.\n\n\n\nE.3.2 Gesamtfazit:\nBeide Experimente zeigen eine signifikante Veränderung der Mediane von „Bevor“ zu „Danach“. Die zentralen Tendenzen sind in beiden Fällen signifikant unterschiedlich. Dies unterstützt die Schlussfolgerung, dass die Handlung, sich selbst oder anderen etwas Gutes zu tun, eine positive Wirkung auf die Bewertung des Gefühls hat.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Übung 5 & 6: Verteilungen und Tests</span>"
    ]
  }
]